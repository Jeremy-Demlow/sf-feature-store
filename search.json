[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Snowflake Feature Store",
    "section": "",
    "text": "A comprehensive toolkit for building and managing ML features in Snowflake, with focus on LTV prediction.\n\n\n\nThis library provides a production-ready interface to Snowflake’s Feature Store, enabling you to:\n\nCreate and manage feature transformations with validation\nHandle temporal features and windowed aggregations\nGenerate point-in-time correct training datasets\nMonitor feature quality and detect drift\nMaintain feature documentation and versioning\n\n\n\n\nThe Snowflake Feature Store library is designed with a modular architecture to provide a comprehensive feature engineering workflow:\n┌─────────────────────────────────────────────────────────────────┐\n│                     Snowflake Feature Store                     │\n└─────────────────────────────────────────────────────────────────┘\n                                  │\n               ┌─────────────────┴─────────────────┐\n               ▼                                   ▼\n┌────────────────────────────┐        ┌─────────────────────────┐\n│    Connection Management   │        │    Feature Definitions  │\n│  (Snowflake Session Setup) │        │   (Configs, Validation) │\n└────────────────────────────┘        └─────────────────────────┘\n               │                                   │\n               └─────────────────┬─────────────────┘\n                                 ▼\n                  ┌─────────────────────────────┐\n                  │     Feature Engineering     │\n                  │  (Transforms, Aggregations) │\n                  └─────────────────────────────┘\n                                 │\n                ┌───────────────┴────────────────┐\n                ▼                                ▼\n┌─────────────────────────────┐      ┌────────────────────────────┐\n│    Feature Views & Store    │      │     Feature Monitoring     │\n│ (Registration, Management)  │      │  (Drift Detection, Stats)  │\n└─────────────────────────────┘      └────────────────────────────┘\n                │                                 │\n                └───────────────┬─────────────────┘\n                                ▼\n                  ┌─────────────────────────────┐\n                  │    Training Data Generation │\n                  │  (Point-in-time Correctness)│\n                  └─────────────────────────────┘\n\n\n\nBefore using this library, you’ll need:\n\nSnowflake Account\n\nAccess to a Snowflake account with sufficient privileges\nAbility to create databases and schemas\n\nRequired Snowflake Privileges\n\nCREATE DATABASE or access to an existing database\nCREATE SCHEMA on target database\nCREATE TABLE, CREATE VIEW privileges in target schema\nUSAGE on a warehouse with adequate compute resources\n\nPython Environment\n\nPython 3.7 or higher\nAdministrative access to install Python packages\n\nSnowflake Configurations\n\nConnection parameters (account, username, password)\nOptionally, Snowflake key pair authentication setup\nEnvironment variables or configuration file for credentials\n\nData Access\n\nAccess to source data in Snowflake tables or external sources\nPermissions to read from source data locations\n\n\n\n\n\n\n\n\n\n\nFeature\nThis Library\n\n\n\n\nLearning Curve\nSimplified, higher-level abstractions\n\n\nFeature Validation\nBuilt-in validation rules, drift detection\n\n\nTransformations\nPre-built, composable transformation library\n\n\nPoint-in-time Training\nSimplified API with safeguards\n\n\nMonitoring\nBuilt-in drift detection and metrics\n\n\nDocumentation\nAuto-generated feature documentation\n\n\n\n\n\n\n\npip install snowflake-feature-store\n\n\n\n\n\nHere’s a full example of setting up a feature store for LTV prediction:\nfrom snowflake_feature_store.connection import get_connection\nfrom snowflake_feature_store.manager import feature_store_session \nfrom snowflake_feature_store.config import (\n    FeatureViewConfig, FeatureConfig, FeatureValidationConfig\n)\n\n# Connect to Snowflake\nconn = get_connection()\n\n# Create feature store session\nwith feature_store_session(conn, cleanup=False) as manager:\n    # 1. Create Customer Entity\n    manager.add_entity(\n        name=\"CUSTOMER\", \n        join_keys=[\"CUSTOMER_ID\"],\n        description=\"Customer entity for LTV prediction\"\n    )\n\n    # 2. Configure Features\n    feature_configs = {\n        \"LIFE_TIME_VALUE\": FeatureConfig(\n            name=\"LIFE_TIME_VALUE\",\n            description=\"Current customer value\",\n            validation=FeatureValidationConfig(\n                null_threshold=0.1,\n                range_check=True,\n                min_value=0\n            )\n        ),\n        \"SESSION_LENGTH\": FeatureConfig(\n            name=\"SESSION_LENGTH\",\n            description=\"Session duration in minutes\",\n            validation=FeatureValidationConfig(\n                null_threshold=0.3,\n                range_check=True,\n                min_value=0\n            )\n        )\n    }\n\n    # 3. Create Feature View\n    config = FeatureViewConfig(\n        name=\"customer_behavior\",\n        domain=\"RETAIL\",\n        entity=\"CUSTOMER\",\n        feature_type=\"BEHAVIOR\",\n        features=feature_configs,\n        refresh=RefreshConfig(frequency=\"1 day\")\n    )\n\n    # 4. Add Transformations\n    transforms = [\n        fill_na(['SESSION_LENGTH'], 0),\n        moving_agg(\n            cols=['TRANSACTIONS'],\n            window_sizes=[7, 30],\n            agg_funcs=['SUM', 'AVG'],\n            partition_by=['CUSTOMER_ID'],\n            order_by=['DATE']\n        )\n    ]\n\n    # 5. Create Feature View\n    feature_view = manager.add_feature_view(\n        config=config,\n        df=source_df,\n        entity_name=\"CUSTOMER\",\n        transforms=transforms\n    )\n\n    # 6. Generate Training Data\n    training_data = generate_ltv_training_data(\n        manager=manager,\n        feature_view=feature_view,\n        training_start_date='2024-01-01',\n        training_end_date='2024-03-01',\n        prediction_window=90\n    )\n\n\n\n\nFeature Configuration\n\nValidation rules\nData quality checks\nDocumentation\nDependencies\n\nTransformations\n\nMissing value handling\nWindow aggregations\nCustom transforms\nFeature combination\n\nMonitoring\n\nFeature statistics\nDrift detection\nQuality metrics\nAlert configuration\n\nTraining Data\n\nPoint-in-time correctness\nLabel generation\nFeature selection\nData validation\n\n\n\n\n\n\nThe library includes detailed documentation for each component:\n\nConnection Management: Snowflake connection setup and management\nFeature Transforms: Feature engineering and validation\nFeature Views: Feature organization and versioning\nFeature Store: Feature store operations and monitoring\nEnd-to-End Example: Complete LTV prediction workflow\n\n\n\n\n\nFeature Monitoring\n\n# Monitor feature drift\nmonitor = LTVMonitor(manager, feature_view.name) \nmonitor.set_baseline(feature_view.feature_df) \ndrift_metrics = monitor.check_feature_health(new_data)\n\nCustom Transformations\n\n# Create custom transform\n@transform_config(name=\"engagement_score\")\ndef calculate_engagement(df): \n    return df.with_column(\n        'ENGAGEMENT_SCORE', \n        (F.col('SESSION_LENGTH') + F.col('TIME_ON_APP')) / 2.0\n    )\n\nPoint-in-Time Training\n\n# Generate training data\ntraining_data = generate_ltv_training_data(\n    manager=manager,\n    feature_view=feature_view,\n    training_start_date='2024-01-01',\n    prediction_window=90\n)\n\n\n\n\n\n\nConnection problems\n\nCheck Snowflake credentials and account information\nEnsure network connectivity to Snowflake\nVerify proper role and warehouse permissions\n\nMissing dependencies\n\nEnsure all required libraries are installed: pip install -r requirements.txt\nCheck for version conflicts, especially with Pydantic\n\nSchema creation failures\n\nVerify you have CREATE SCHEMA privileges\nCheck for schema name conflicts\n\nPerformance issues\n\nConsider using a larger warehouse for heavy transformations\nOptimize window functions and partition keys\nReview data volumes and aggregation periods\n\n\n\n\n\n\nContributions are welcome! Please see our Contributing Guide for more information.\n\n\n\nApache License 2.0 - See LICENSE for details.\n\n\n\nInstall latest from the GitHub repository:\n$ pip install git+https://github.com/Jeremy-Demlow/sf-feature-store.git\nor from pypi\n$ pip install sf_feature_store",
    "crumbs": [
      "Snowflake Feature Store"
    ]
  },
  {
    "objectID": "index.html#what-is-this",
    "href": "index.html#what-is-this",
    "title": "Snowflake Feature Store",
    "section": "",
    "text": "This library provides a production-ready interface to Snowflake’s Feature Store, enabling you to:\n\nCreate and manage feature transformations with validation\nHandle temporal features and windowed aggregations\nGenerate point-in-time correct training datasets\nMonitor feature quality and detect drift\nMaintain feature documentation and versioning",
    "crumbs": [
      "Snowflake Feature Store"
    ]
  },
  {
    "objectID": "index.html#architecture",
    "href": "index.html#architecture",
    "title": "Snowflake Feature Store",
    "section": "",
    "text": "The Snowflake Feature Store library is designed with a modular architecture to provide a comprehensive feature engineering workflow:\n┌─────────────────────────────────────────────────────────────────┐\n│                     Snowflake Feature Store                     │\n└─────────────────────────────────────────────────────────────────┘\n                                  │\n               ┌─────────────────┴─────────────────┐\n               ▼                                   ▼\n┌────────────────────────────┐        ┌─────────────────────────┐\n│    Connection Management   │        │    Feature Definitions  │\n│  (Snowflake Session Setup) │        │   (Configs, Validation) │\n└────────────────────────────┘        └─────────────────────────┘\n               │                                   │\n               └─────────────────┬─────────────────┘\n                                 ▼\n                  ┌─────────────────────────────┐\n                  │     Feature Engineering     │\n                  │  (Transforms, Aggregations) │\n                  └─────────────────────────────┘\n                                 │\n                ┌───────────────┴────────────────┐\n                ▼                                ▼\n┌─────────────────────────────┐      ┌────────────────────────────┐\n│    Feature Views & Store    │      │     Feature Monitoring     │\n│ (Registration, Management)  │      │  (Drift Detection, Stats)  │\n└─────────────────────────────┘      └────────────────────────────┘\n                │                                 │\n                └───────────────┬─────────────────┘\n                                ▼\n                  ┌─────────────────────────────┐\n                  │    Training Data Generation │\n                  │  (Point-in-time Correctness)│\n                  └─────────────────────────────┘",
    "crumbs": [
      "Snowflake Feature Store"
    ]
  },
  {
    "objectID": "index.html#prerequisites",
    "href": "index.html#prerequisites",
    "title": "Snowflake Feature Store",
    "section": "",
    "text": "Before using this library, you’ll need:\n\nSnowflake Account\n\nAccess to a Snowflake account with sufficient privileges\nAbility to create databases and schemas\n\nRequired Snowflake Privileges\n\nCREATE DATABASE or access to an existing database\nCREATE SCHEMA on target database\nCREATE TABLE, CREATE VIEW privileges in target schema\nUSAGE on a warehouse with adequate compute resources\n\nPython Environment\n\nPython 3.7 or higher\nAdministrative access to install Python packages\n\nSnowflake Configurations\n\nConnection parameters (account, username, password)\nOptionally, Snowflake key pair authentication setup\nEnvironment variables or configuration file for credentials\n\nData Access\n\nAccess to source data in Snowflake tables or external sources\nPermissions to read from source data locations",
    "crumbs": [
      "Snowflake Feature Store"
    ]
  },
  {
    "objectID": "index.html#comparison-with-alternatives",
    "href": "index.html#comparison-with-alternatives",
    "title": "Snowflake Feature Store",
    "section": "",
    "text": "Feature\nThis Library\n\n\n\n\nLearning Curve\nSimplified, higher-level abstractions\n\n\nFeature Validation\nBuilt-in validation rules, drift detection\n\n\nTransformations\nPre-built, composable transformation library\n\n\nPoint-in-time Training\nSimplified API with safeguards\n\n\nMonitoring\nBuilt-in drift detection and metrics\n\n\nDocumentation\nAuto-generated feature documentation",
    "crumbs": [
      "Snowflake Feature Store"
    ]
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "Snowflake Feature Store",
    "section": "",
    "text": "pip install snowflake-feature-store",
    "crumbs": [
      "Snowflake Feature Store"
    ]
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "Snowflake Feature Store",
    "section": "",
    "text": "Here’s a full example of setting up a feature store for LTV prediction:\nfrom snowflake_feature_store.connection import get_connection\nfrom snowflake_feature_store.manager import feature_store_session \nfrom snowflake_feature_store.config import (\n    FeatureViewConfig, FeatureConfig, FeatureValidationConfig\n)\n\n# Connect to Snowflake\nconn = get_connection()\n\n# Create feature store session\nwith feature_store_session(conn, cleanup=False) as manager:\n    # 1. Create Customer Entity\n    manager.add_entity(\n        name=\"CUSTOMER\", \n        join_keys=[\"CUSTOMER_ID\"],\n        description=\"Customer entity for LTV prediction\"\n    )\n\n    # 2. Configure Features\n    feature_configs = {\n        \"LIFE_TIME_VALUE\": FeatureConfig(\n            name=\"LIFE_TIME_VALUE\",\n            description=\"Current customer value\",\n            validation=FeatureValidationConfig(\n                null_threshold=0.1,\n                range_check=True,\n                min_value=0\n            )\n        ),\n        \"SESSION_LENGTH\": FeatureConfig(\n            name=\"SESSION_LENGTH\",\n            description=\"Session duration in minutes\",\n            validation=FeatureValidationConfig(\n                null_threshold=0.3,\n                range_check=True,\n                min_value=0\n            )\n        )\n    }\n\n    # 3. Create Feature View\n    config = FeatureViewConfig(\n        name=\"customer_behavior\",\n        domain=\"RETAIL\",\n        entity=\"CUSTOMER\",\n        feature_type=\"BEHAVIOR\",\n        features=feature_configs,\n        refresh=RefreshConfig(frequency=\"1 day\")\n    )\n\n    # 4. Add Transformations\n    transforms = [\n        fill_na(['SESSION_LENGTH'], 0),\n        moving_agg(\n            cols=['TRANSACTIONS'],\n            window_sizes=[7, 30],\n            agg_funcs=['SUM', 'AVG'],\n            partition_by=['CUSTOMER_ID'],\n            order_by=['DATE']\n        )\n    ]\n\n    # 5. Create Feature View\n    feature_view = manager.add_feature_view(\n        config=config,\n        df=source_df,\n        entity_name=\"CUSTOMER\",\n        transforms=transforms\n    )\n\n    # 6. Generate Training Data\n    training_data = generate_ltv_training_data(\n        manager=manager,\n        feature_view=feature_view,\n        training_start_date='2024-01-01',\n        training_end_date='2024-03-01',\n        prediction_window=90\n    )\n\n\n\n\nFeature Configuration\n\nValidation rules\nData quality checks\nDocumentation\nDependencies\n\nTransformations\n\nMissing value handling\nWindow aggregations\nCustom transforms\nFeature combination\n\nMonitoring\n\nFeature statistics\nDrift detection\nQuality metrics\nAlert configuration\n\nTraining Data\n\nPoint-in-time correctness\nLabel generation\nFeature selection\nData validation",
    "crumbs": [
      "Snowflake Feature Store"
    ]
  },
  {
    "objectID": "index.html#documentation",
    "href": "index.html#documentation",
    "title": "Snowflake Feature Store",
    "section": "",
    "text": "The library includes detailed documentation for each component:\n\nConnection Management: Snowflake connection setup and management\nFeature Transforms: Feature engineering and validation\nFeature Views: Feature organization and versioning\nFeature Store: Feature store operations and monitoring\nEnd-to-End Example: Complete LTV prediction workflow",
    "crumbs": [
      "Snowflake Feature Store"
    ]
  },
  {
    "objectID": "index.html#advanced-features",
    "href": "index.html#advanced-features",
    "title": "Snowflake Feature Store",
    "section": "",
    "text": "Feature Monitoring\n\n# Monitor feature drift\nmonitor = LTVMonitor(manager, feature_view.name) \nmonitor.set_baseline(feature_view.feature_df) \ndrift_metrics = monitor.check_feature_health(new_data)\n\nCustom Transformations\n\n# Create custom transform\n@transform_config(name=\"engagement_score\")\ndef calculate_engagement(df): \n    return df.with_column(\n        'ENGAGEMENT_SCORE', \n        (F.col('SESSION_LENGTH') + F.col('TIME_ON_APP')) / 2.0\n    )\n\nPoint-in-Time Training\n\n# Generate training data\ntraining_data = generate_ltv_training_data(\n    manager=manager,\n    feature_view=feature_view,\n    training_start_date='2024-01-01',\n    prediction_window=90\n)",
    "crumbs": [
      "Snowflake Feature Store"
    ]
  },
  {
    "objectID": "index.html#troubleshooting",
    "href": "index.html#troubleshooting",
    "title": "Snowflake Feature Store",
    "section": "",
    "text": "Connection problems\n\nCheck Snowflake credentials and account information\nEnsure network connectivity to Snowflake\nVerify proper role and warehouse permissions\n\nMissing dependencies\n\nEnsure all required libraries are installed: pip install -r requirements.txt\nCheck for version conflicts, especially with Pydantic\n\nSchema creation failures\n\nVerify you have CREATE SCHEMA privileges\nCheck for schema name conflicts\n\nPerformance issues\n\nConsider using a larger warehouse for heavy transformations\nOptimize window functions and partition keys\nReview data volumes and aggregation periods",
    "crumbs": [
      "Snowflake Feature Store"
    ]
  },
  {
    "objectID": "index.html#contributing",
    "href": "index.html#contributing",
    "title": "Snowflake Feature Store",
    "section": "",
    "text": "Contributions are welcome! Please see our Contributing Guide for more information.",
    "crumbs": [
      "Snowflake Feature Store"
    ]
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "Snowflake Feature Store",
    "section": "",
    "text": "Apache License 2.0 - See LICENSE for details.",
    "crumbs": [
      "Snowflake Feature Store"
    ]
  },
  {
    "objectID": "index.html#installation",
    "href": "index.html#installation",
    "title": "Snowflake Feature Store",
    "section": "",
    "text": "Install latest from the GitHub repository:\n$ pip install git+https://github.com/Jeremy-Demlow/sf-feature-store.git\nor from pypi\n$ pip install sf_feature_store",
    "crumbs": [
      "Snowflake Feature Store"
    ]
  },
  {
    "objectID": "core.html",
    "href": "core.html",
    "title": "Core Functionality",
    "section": "",
    "text": "# ! pip install snowflake-ml-python\n# ! pip install snowflake-snowpark-python\n\n\nsource\n\nFeatureStoreDefaults\n\n FeatureStoreDefaults (version_prefix:str='V', refresh_frequency:str='1\n                       day', feature_view_prefix:str='FV',\n                       creation_mode:str='CREATE_IF_NOT_EXIST',\n                       refresh_mode:str='FULL')\n\nDefault settings for feature store operations\n\nsource\n\n\ncreate_version\n\n create_version (major:int, minor:int=0)\n\nCreate a standardized version string (e.g., ‘V1_0’ for major=1, minor=0)\n\nsource\n\n\ncreate_feature_view_name\n\n create_feature_view_name (domain:str, entity:str, feature_type:str)\n\n*Create standardized feature view name\nArgs: domain: Business domain (e.g., ‘RETAIL’, ‘FINANCE’) entity: Main entity name (e.g., ‘CUSTOMER’, ‘PRODUCT’) feature_type: Type of features (e.g., ‘BEHAVIOR’, ‘PROFILE’)\nReturns: Formatted name like ‘FV_RETAIL_CUSTOMER_BEHAVIOR’\nExample: &gt;&gt;&gt; create_feature_view_name(‘RETAIL’, ‘CUSTOMER’, ‘BEHAVIOR’) ‘FV_RETAIL_CUSTOMER_BEHAVIOR’*\n\nsource\n\n\nSQLFormatter\n\n SQLFormatter ()\n\nUtilities for SQL query formatting and analysis\n\n# Example usage of core functionality\nfrom snowflake_feature_store.core import *\n\n# Create feature view name\nfv_name = create_feature_view_name('RETAIL', 'CUSTOMER', 'BEHAVIOR')\nprint(f\"Feature view name: {fv_name}\")\n\n# Create version string\nversion = create_version(1, 2)\nprint(f\"Version: {version}\")\n\n# Format SQL\nsql = \"\"\"\nSELECT a, b \nFROM (SELECT * FROM table1) \nJOIN table2\n\"\"\"\nformatted = SQLFormatter.format_sql(sql, True)\nprint(\"Formatted SQL:\")\nprint(formatted)\n\ntables = SQLFormatter.extract_table_names(sql)\nprint(f\"Tables used: {tables}\")\n\nFeature view name: FV_RETAIL_CUSTOMER_BEHAVIOR\nVersion: V1_2\nFormatted SQL:\nWITH cte AS (\n  SELECT\n    *\n  FROM table1\n)\nSELECT\n  a,\n  b\nFROM cte AS cte, table2\nTables used: ['table2', 'table1']",
    "crumbs": [
      "Core Functionality"
    ]
  },
  {
    "objectID": "transforms.html",
    "href": "transforms.html",
    "title": "Feature Transforms",
    "section": "",
    "text": "source\n\nTransformConfig\n\n TransformConfig (name:str, description:Optional[str]=None,\n                  validate_output:bool=True, null_threshold:typing.Annotat\n                  ed[float,Ge(ge=0),Le(le=1)]=0.1,\n                  cardinality_threshold:Optional[int]=None,\n                  expected_types:Optional[List[str]]=None)\n\nConfiguration for data transformations\n\nsource\n\n\nTransform\n\n Transform (*args, **kwargs)\n\nProtocol for feature transformations\n\nsource\n\n\nValidationMixin\n\n ValidationMixin ()\n\nMixin class providing validation methods for transforms\n\nsource\n\n\nCustomTransform\n\n CustomTransform (transform_func:Callable[[snowflake.snowpark.dataframe.Da\n                  taFrame],snowflake.snowpark.dataframe.DataFrame],\n                  config:__main__.TransformConfig)\n\nWrapper for custom transformations\n\nsource\n\n\nWindowSpec\n\n WindowSpec (partition_by:Union[str,List[str],NoneType]=None,\n             order_by:Union[str,List[str],NoneType]=None,\n             window_size:Optional[int]=None)\n\nConfiguration for window-based transformations\n\nsource\n\n\nWindowTransform\n\n WindowTransform (agg_cols:Dict[str,List[str]],\n                  window_spec:__main__.WindowSpec,\n                  config:Optional[__main__.TransformConfig]=None)\n\nWindow-based aggregation transform\n\nsource\n\n\nwindow_agg\n\n window_agg (agg_cols:Dict[str,List[str]],\n             window_spec:__main__.WindowSpec,\n             config:Optional[__main__.TransformConfig]=None)\n\n*Create window-based aggregation transform\nArgs: agg_cols: Dictionary mapping columns to aggregation functions window_spec: Window specification for the aggregation config: Optional transform configuration\nReturns: Transform function\nExample: &gt;&gt;&gt; config = TransformConfig( … name=“customer_metrics”, … null_threshold=0.05, … expected_type=“DOUBLE” … ) &gt;&gt;&gt; spec = WindowSpec(partition_by=‘customer_id’, order_by=‘date’) &gt;&gt;&gt; transform = window_agg({‘amount’: [‘SUM’, ‘AVG’]}, spec, config)*\n\nsource\n\n\nFillNATransform\n\n FillNATransform (cols:Union[str,List[str]],\n                  fill_value:Union[int,float,str]=0,\n                  config:Optional[__main__.TransformConfig]=None)\n\nMixin class providing validation methods for transforms\n\nsource\n\n\nfill_na\n\n fill_na (cols:Union[str,List[str]], fill_value:Union[int,float,str]=0,\n          config:Optional[__main__.TransformConfig]=None)\n\n*Create transform to fill NA values\nArgs: cols: Column(s) to fill NA values in fill_value: Value to use for filling NAs config: Optional transform configuration\nReturns: Transform function\nExample: &gt;&gt;&gt; config = TransformConfig( … name=“fill_scores”, … null_threshold=1.0 … ) &gt;&gt;&gt; transform = fill_na([‘score’], fill_value=0, config=config)*\n\nsource\n\n\nDateDiffTransform\n\n DateDiffTransform (col:str, new_col:str,\n                    reference_date:Optional[str]=None,\n                    date_part:str='day',\n                    config:Optional[__main__.TransformConfig]=None)\n\nCalculate date difference between column and reference\n\nsource\n\n\ndate_diff\n\n date_diff (col:str, new_col:str, reference_date:Optional[str]=None,\n            date_part:str='day',\n            config:Optional[__main__.TransformConfig]=None)\n\n*Create date difference transform\nArgs: col: Date column to calculate difference from new_col: Name for the new difference column reference_date: Reference date (default: current_date) date_part: Part to calculate difference in (‘day’, ‘month’, etc.) config: Optional transform configuration\nReturns: Transform function\nExample: &gt;&gt;&gt; config = TransformConfig( … name=“membership_length”, … null_threshold=0.0 # No nulls allowed … ) &gt;&gt;&gt; transform = date_diff(‘join_date’, ‘days_member’, config=config)*\n\nsource\n\n\nMovingAggTransform\n\n MovingAggTransform (cols:Union[str,List[str]], window_sizes:List[int],\n                     agg_funcs:List[str]=['SUM', 'AVG'],\n                     partition_by:Optional[List[str]]=None,\n                     order_by:Optional[List[str]]=None,\n                     config:Optional[__main__.TransformConfig]=None)\n\nCalculate moving window aggregations\n\nsource\n\n\nmoving_agg\n\n moving_agg (cols:Union[str,List[str]], window_sizes:List[int],\n             agg_funcs:List[str]=['SUM', 'AVG'],\n             partition_by:Optional[List[str]]=None,\n             order_by:Optional[List[str]]=None,\n             config:Optional[__main__.TransformConfig]=None)\n\n*Create moving aggregation transform\nArgs: cols: Columns to aggregate window_sizes: List of window sizes agg_funcs: List of aggregation functions partition_by: Columns to partition by order_by: Columns to order by config: Optional transform configuration\nReturns: Transform function\nExample: &gt;&gt;&gt; config = TransformConfig( … name=“rolling_metrics”, … expected_type=“DOUBLE” … ) &gt;&gt;&gt; transform = moving_agg( … ‘amount’, … [7, 30], … [‘SUM’], … [‘customer_id’], … [‘date’], … config … )*\n\nsource\n\n\nCumulativeAggTransform\n\n CumulativeAggTransform (cols:Union[str,List[str]],\n                         agg_funcs:List[str]=['SUM'],\n                         partition_by:Optional[List[str]]=None,\n                         order_by:Optional[List[str]]=None,\n                         config:Optional[__main__.TransformConfig]=None)\n\nCalculate cumulative aggregations\n\nsource\n\n\ncumulative_agg\n\n cumulative_agg (cols:Union[str,List[str]], agg_funcs:List[str]=['SUM'],\n                 partition_by:Optional[List[str]]=None,\n                 order_by:Optional[List[str]]=None,\n                 config:Optional[__main__.TransformConfig]=None)\n\n*Create cumulative aggregation transform\nArgs: cols: Columns to aggregate agg_funcs: List of aggregation functions partition_by: Columns to partition by order_by: Columns to order by config: Optional transform configuration\nReturns: Transform function\nExample: &gt;&gt;&gt; config = TransformConfig( … name=“running_totals”, … expected_type=“DOUBLE” … ) &gt;&gt;&gt; transform = cumulative_agg( … ‘amount’, … [‘SUM’], … [‘customer_id’], … [‘date’], … config … )*\n\nsource\n\n\napply_transforms\n\n apply_transforms (df:snowflake.snowpark.dataframe.DataFrame,\n                   transforms:List[__main__.Transform])\n\n*Apply a list of transformations to a DataFrame\nArgs: df: Input DataFrame transforms: List of transform functions to apply\nReturns: Transformed DataFrame\nExample: &gt;&gt;&gt; transforms = [ … fill_na([‘score’], config=TransformConfig(name=‘fill_scores’)), … date_diff(‘date’, ‘days_ago’, config=TransformConfig(name=‘time_features’)) … ] &gt;&gt;&gt; df = apply_transforms(df, transforms)*\n\n# Example usage\nfrom snowflake_feature_store.connection import get_connection, ConnectionConfig\nfrom snowflake_feature_store.transforms import *\n\n# Get connection automatically\nconn = get_connection()\nsession = conn.session\n\n# Create example DataFrame - Note the UPPERCASE column names for Snowflake\ndf = session.create_dataframe([\n    ['C1', '2024-01-01', 100, None],\n    ['C1', '2024-01-02', 200, 3.5],\n    ['C2', '2024-01-01', 150, None]\n], ['CUSTOMER_ID', 'DATE', 'AMOUNT', 'SCORE'])\n\n# Configuration with appropriate type validation\nagg_config = TransformConfig(\n    name=\"customer_metrics\",\n    null_threshold=0.05,\n    expected_types=['LONG', 'DOUBLE']  # Accept either type for numeric aggregations\n)\n\ndate_config = TransformConfig(\n    name=\"date_features\",\n    null_threshold=0.0,\n    expected_types=['INT', 'LONG']  # Date diffs can be either type\n)\n\n# Create transforms\ntransforms = [\n    # Fill NA values in score column\n    fill_na(['SCORE'], fill_value=0),\n    \n    # Calculate moving aggregations on amount\n    moving_agg(\n        cols='AMOUNT',\n        window_sizes=[2],\n        agg_funcs=['SUM', 'AVG'],  # Now we can use both\n        partition_by=['CUSTOMER_ID'],\n        order_by=['DATE'],\n        config=agg_config\n    ),\n    \n    # Calculate date differences\n    date_diff(\n        col='DATE',\n        new_col='DAYS_AGO',\n        reference_date='2024-01-03',\n        config=date_config\n    )\n]\n\n# Apply transforms\nresult_df = apply_transforms(df, transforms)\n\n# Show results\nprint(\"\\nTransformed DataFrame:\")\nresult_df.show()\n\n# Show schema\nprint(\"\\nResult Schema:\")\nfor field in result_df.schema.fields:\n    print(f\"{field.name}: {field.datatype}\")\n\n2025-02-26 19:34:47,083 - snowflake_feature_store - INFO - No active session found, creating new connection from environment\n2025-02-26 19:34:47,762 - snowflake_feature_store - INFO - Initialized connection to \"CONTAINER_DEMO_DB\".\"PUBLIC\"\n2025-02-26 19:34:47,763 - snowflake_feature_store - INFO - Using role: \"ACCOUNTADMIN\", warehouse: \"CONTAINER_DEMO_WH\", database: \"CONTAINER_DEMO_DB\", schema: \"PUBLIC\"\n\nTransformed DataFrame:\n--------------------------------------------------------------------------------------------------\n|\"CUSTOMER_ID\"  |\"DATE\"      |\"AMOUNT\"  |\"SCORE\"  |\"SUM_AMOUNT_2\"  |\"AVG_AMOUNT_2\"  |\"DAYS_AGO\"  |\n--------------------------------------------------------------------------------------------------\n|C2             |2024-01-01  |150       |0.0      |150.0           |150.0           |2           |\n|C1             |2024-01-01  |100       |0.0      |100.0           |100.0           |2           |\n|C1             |2024-01-02  |200       |3.5      |300.0           |150.0           |1           |\n--------------------------------------------------------------------------------------------------\n\n\nResult Schema:\nCUSTOMER_ID: StringType()\nDATE: StringType()\nAMOUNT: LongType()\nSCORE: DoubleType()\nSUM_AMOUNT_2: DoubleType()\nAVG_AMOUNT_2: DoubleType()\nDAYS_AGO: LongType()",
    "crumbs": [
      "Feature Transforms"
    ]
  },
  {
    "objectID": "config.html",
    "href": "config.html",
    "title": "Configuration",
    "section": "",
    "text": "source\n\nRefreshConfig\n\n RefreshConfig (frequency:str='1 day', mode:str='FULL')\n\nConfiguration for feature refresh settings\n\nsource\n\n\nFeatureValidationConfig\n\n FeatureValidationConfig (null_check:bool=True, null_threshold:float=0.1,\n                          range_check:bool=False,\n                          min_value:Optional[float]=None,\n                          max_value:Optional[float]=None,\n                          unique_check:bool=False,\n                          unique_threshold:float=0.9)\n\nConfiguration for feature validation rules\n\nsource\n\n\nFeatureConfig\n\n FeatureConfig (name:str, description:str,\n                validation:Optional[__main__.FeatureValidationConfig]=&lt;fac\n                tory&gt;, dependencies:List[str]=&lt;factory&gt;)\n\nConfiguration for individual features\n\nsource\n\n\nFeatureViewConfig\n\n FeatureViewConfig (name:str, domain:str='', entity:str='CUSTOMER',\n                    feature_type:str='BASE',\n                    major_version:typing.Annotated[int,Ge(ge=1)]=1,\n                    minor_version:typing.Annotated[int,Ge(ge=0)]=0,\n                    refresh:__main__.RefreshConfig=&lt;factory&gt;,\n                    timestamp_col:Optional[str]=None,\n                    description:Optional[str]=None,\n                    features:Dict[str,__main__.FeatureConfig]=&lt;factory&gt;,\n                    tags:Dict[str,str]=&lt;factory&gt;)\n\nEnhanced configuration for feature views",
    "crumbs": [
      "Configuration"
    ]
  },
  {
    "objectID": "manager.html",
    "href": "manager.html",
    "title": "Manager (Features)",
    "section": "",
    "text": "source\n\nFeatureStoreCallback\n\n FeatureStoreCallback ()\n\nProtocol for feature store callbacks\n\nsource\n\n\nMetricsCallback\n\n MetricsCallback (metrics_path:Optional[pathlib.Path]=None)\n\nCallback that logs metrics and statistics\n\nsource\n\n\nFeatureStoreManager\n\n FeatureStoreManager\n                      (connection:snowflake_feature_store.connection.Snowf\n                      lakeConnection, callbacks:Optional[List[__main__.Fea\n                      tureStoreCallback]]=None,\n                      metrics_path:Union[str,pathlib.Path,NoneType]=None,\n                      overwrite:bool=False)\n\nManages feature store operations with monitoring and dependency tracking\n\nsource\n\n\nfeature_store_session\n\n feature_store_session\n                        (connection:snowflake_feature_store.connection.Sno\n                        wflakeConnection, schema_name:Optional[str]=None, \n                        metrics_path:Union[str,pathlib.Path,NoneType]=None\n                        , cleanup:bool=True)\n\n*Context manager for feature store operations\nArgs: connection: Snowflake connection schema_name: Optional schema name (keyword only) metrics_path: Optional path to save metrics (keyword only) cleanup: Whether to cleanup schema after use (keyword only)*\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nconnection\nSnowflakeConnection\n\n\n\n\nschema_name\nOptional\nNone\nForce keyword arguments\n\n\nmetrics_path\nUnion\nNone\nChanged type hint\n\n\ncleanup\nbool\nTrue\n\n\n\n\n\nfrom snowflake_feature_store.connection import get_connection\nfrom snowflake_feature_store.config import (\n    FeatureViewConfig, FeatureConfig, FeatureValidationConfig, RefreshConfig\n)\nfrom snowflake_feature_store.transforms import TransformConfig, moving_agg, fill_na\nfrom snowflake_feature_store.manager import feature_store_session\nimport snowflake.snowpark.functions as F\nfrom datetime import datetime\nimport tempfile\nfrom pathlib import Path\n\n# Create a temporary directory for metrics\nmetrics_dir = Path(tempfile.mkdtemp()) / \"feature_store_metrics\"\n\n# Get connection\nconn = get_connection()\n\n# Use the feature store session context manager\nwith feature_store_session(conn, metrics_path=str(metrics_dir)) as manager:\n    # Create sample data\n    # First, create a regular table to store our data\n    conn.session.sql(\"\"\"\n        CREATE OR REPLACE TABLE TEMP_CUSTOMER_DATA (\n            CUSTOMER_ID STRING,\n            DATE DATE,\n            AMOUNT FLOAT,\n            TRANSACTIONS INT,\n            SESSION_LENGTH FLOAT\n        )\n    \"\"\").collect()\n    \n    # Insert the data using SQL\n    conn.session.sql(\"\"\"\n        INSERT INTO TEMP_CUSTOMER_DATA VALUES\n        ('C1', '2024-01-01', 100.0, 2, NULL),\n        ('C1', '2024-01-02', 150.0, 3, 30.5),\n        ('C2', '2024-01-01', 75.0, 1, NULL),\n        ('C2', '2024-01-02', 200.0, 4, 45.2)\n    \"\"\").collect()\n    \n    # Create feature DataFrame from the table\n    df = conn.session.table(\"TEMP_CUSTOMER_DATA\")\n    \n    print(\"\\nInitial DataFrame Schema:\")\n    for field in df.schema.fields:\n        print(f\"{field.name}: {field.datatype}\")\n    \n    print(\"\\nSample Data:\")\n    df.show()\n    \n    # 1. Add Customer Entity\n    manager.add_entity(\n        name=\"CUSTOMER\",\n        join_keys=[\"CUSTOMER_ID\"],\n        description=\"Customer entity for retail domain\"\n    )\n    \n    # 2. Create Feature Configurations with dependencies\n    feature_configs = {\n        \"AMOUNT\": FeatureConfig(\n            name=\"AMOUNT\",\n            description=\"Transaction amount\",\n            validation=FeatureValidationConfig(\n                null_threshold=0.1,\n                range_check=True,\n                min_value=0\n            ),\n            dependencies=[]  # Base feature, no dependencies\n        ),\n        \"TRANSACTIONS\": FeatureConfig(\n            name=\"TRANSACTIONS\",\n            description=\"Number of transactions\",\n            validation=FeatureValidationConfig(\n                null_threshold=0.05,\n                range_check=True,\n                min_value=0\n            ),\n            dependencies=[]  # Base feature, no dependencies\n        ),\n        \"SESSION_LENGTH\": FeatureConfig(\n            name=\"SESSION_LENGTH\",\n            description=\"Session length in minutes\",\n            validation=FeatureValidationConfig(\n                null_threshold=0.3,\n                range_check=True,\n                min_value=0\n            ),\n            dependencies=[]  # Base feature, no dependencies\n        ),\n        \"SUM_AMOUNT_2\": FeatureConfig(\n            name=\"SUM_AMOUNT_2\",\n            description=\"2-day rolling sum of amount\",\n            validation=FeatureValidationConfig(\n                null_threshold=0.05,\n                range_check=True,\n                min_value=0\n            ),\n            dependencies=[\"AMOUNT\"]  # Depends on AMOUNT\n        ),\n        \"AVG_AMOUNT_2\": FeatureConfig(\n            name=\"AVG_AMOUNT_2\",\n            description=\"2-day rolling average of amount\",\n            validation=FeatureValidationConfig(\n                null_threshold=0.05,\n                range_check=True,\n                min_value=0\n            ),\n            dependencies=[\"AMOUNT\"]  # Depends on AMOUNT\n        )\n    }\n\n    \n    # 3. Create Feature View Config\n    config = FeatureViewConfig(\n        name=\"customer_behavior\",\n        domain=\"RETAIL\",\n        entity=\"CUSTOMER\",\n        feature_type=\"BEHAVIOR\",\n        refresh=RefreshConfig(frequency=\"1 day\"),\n        features=feature_configs,\n        description=\"Customer behavior features\",\n        timestamp_col=\"DATE\"\n    )\n    \n    # 4. Create transforms\n    transform_config = TransformConfig(\n        name=\"amount_metrics\",\n        null_threshold=0.05,\n        expected_types=['DECIMAL', 'DOUBLE', 'NUMBER']\n    )\n    \n    transforms = [\n        # Fill NA values in session length\n        fill_na(['SESSION_LENGTH'], fill_value=0),\n        \n        # Calculate moving aggregations for amount\n        moving_agg(\n            cols='AMOUNT',\n            window_sizes=[2],  # 2-day window\n            agg_funcs=['SUM', 'AVG'],\n            partition_by=['CUSTOMER_ID'],\n            order_by=['DATE'],\n            config=transform_config\n        )\n    ]\n    \n    # 5. Create Feature View\n    feature_view = manager.add_feature_view(\n        config=config,\n        df=df,\n        entity_name=\"CUSTOMER\",\n        transforms=transforms,\n        collect_stats=True\n    )\n    \n    # 6. Check for feature drift with new data\n    # Create new table for drift detection\n    conn.session.sql(\"\"\"\n        CREATE OR REPLACE TABLE TEMP_NEW_DATA (\n            CUSTOMER_ID STRING,\n            DATE DATE,\n            AMOUNT FLOAT,\n            TRANSACTIONS INT,\n            SESSION_LENGTH FLOAT\n        )\n    \"\"\").collect()\n    \n    conn.session.sql(\"\"\"\n        INSERT INTO TEMP_NEW_DATA VALUES\n        ('C1', '2024-01-03', 300.0, 5, 60.0),\n        ('C2', '2024-01-03', 80.0, 1, 15.5)\n    \"\"\").collect()\n    \n    new_df = conn.session.table(\"TEMP_NEW_DATA\")\n    \n    drift_results = manager.check_feature_drift(config.name, new_df)\n    print(\"\\nDrift Detection Results:\")\n    for feature, metrics in drift_results.items():\n        print(f\"\\n{feature}:\")\n        for metric, value in metrics.items():\n            print(f\"  {metric}: {value:.3f}\")\n    \n    # 7. Get Feature Dependencies\n    deps = manager.get_feature_dependencies(config.name)\n    print(\"\\nFeature Dependencies:\", deps)\n    \n    # 8. Generate Training Dataset\n    print(\"\\nEntity join keys:\")\n    for entity in feature_view.entities:\n        print(f\"Entity {entity.name}: {entity.join_keys}\")\n\n    # Create spine DataFrame with explicit quoting\n    spine_df = df.select([\n        F.col('CUSTOMER_ID').alias('\"CUSTOMER_ID\"'),\n        F.col('DATE').alias('\"DATE\"')\n    ])\n\n    print(\"\\nSpine DataFrame columns:\")\n    print(spine_df.columns)\n    print(\"\\nSpine DataFrame schema:\")\n    for field in spine_df.schema.fields:\n        print(f\"{field.name}: {field.datatype}\")\n\n    training_data = manager.get_features(\n        spine_df=spine_df,\n        feature_views=[config],\n        label_cols=[\"TRANSACTIONS\"],\n        spine_timestamp_col=\"DATE\"\n    )\n    print(\"\\nTraining Data Sample:\")\n    training_data.show(2)\n\n    # After creating the feature view:\n    print(\"\\nFeature Statistics:\")\n    for feature_name, stats in manager.feature_stats[config.name].items():\n        print(f\"\\n{feature_name}:\")\n        print(stats)\n    \n    print(\"\\nTraining Data Schema:\")\n    for field in training_data.schema.fields:\n        print(f\"{field.name}: {field.datatype}\")\n    \n    # Cleanup temporary tables\n    conn.session.sql(\"DROP TABLE IF EXISTS TEMP_CUSTOMER_DATA\").collect()\n    conn.session.sql(\"DROP TABLE IF EXISTS TEMP_NEW_DATA\").collect()\n\n2025-02-17 21:05:11,278 - snowflake_feature_store - INFO - Using active Snowflake session\n2025-02-17 21:05:11,279 - snowflake_feature_store - INFO - Initialized connection to \"DATASCIENCE\".\"FEATURE_STORE_DEMO\"\n2025-02-17 21:05:14,673 - snowflake_feature_store - INFO - FeatureStoreManager initialized\n\nInitial DataFrame Schema:\nCUSTOMER_ID: StringType()\nDATE: DateType()\nAMOUNT: DoubleType()\nTRANSACTIONS: LongType()\nSESSION_LENGTH: DoubleType()\n\nSample Data:\n-----------------------------------------------------------------------------\n|\"CUSTOMER_ID\"  |\"DATE\"      |\"AMOUNT\"  |\"TRANSACTIONS\"  |\"SESSION_LENGTH\"  |\n-----------------------------------------------------------------------------\n|C1             |2024-01-01  |100.0     |2               |NULL              |\n|C1             |2024-01-02  |150.0     |3               |30.5              |\n|C2             |2024-01-01  |75.0      |1               |NULL              |\n|C2             |2024-01-02  |200.0     |4               |45.2              |\n-----------------------------------------------------------------------------\n\n2025-02-17 21:05:18,241 - snowflake_feature_store - INFO - Created entity: CUSTOMER with keys: ['CUSTOMER_ID']\n2025-02-17 21:05:22,346 - snowflake_feature_store - INFO - Validated feature AMOUNT (stats: {'timestamp': '2025-02-18T05:05:21.322713', 'row_count': 4, 'null_count': 0, 'null_ratio': 0.0, 'unique_count': 4, 'min_value': 75.0, 'max_value': 200.0, 'mean_value': 131.25, 'std_value': 55.433894565208625})\n2025-02-17 21:05:23,939 - snowflake_feature_store - INFO - Validated feature TRANSACTIONS (stats: {'timestamp': '2025-02-18T05:05:22.847648', 'row_count': 4, 'null_count': 0, 'null_ratio': 0.0, 'unique_count': 4, 'min_value': 1.0, 'max_value': 4.0, 'mean_value': 2.5, 'std_value': 1.290994577835244})\n2025-02-17 21:05:25,600 - snowflake_feature_store - INFO - Validated feature SESSION_LENGTH (stats: {'timestamp': '2025-02-18T05:05:24.371173', 'row_count': 4, 'null_count': 0, 'null_ratio': 0.0, 'unique_count': 3, 'min_value': 0.0, 'max_value': 45.2, 'mean_value': 18.925, 'std_value': 22.661770304487096})\n2025-02-17 21:05:27,676 - snowflake_feature_store - INFO - Validated feature SUM_AMOUNT_2 (stats: {'timestamp': '2025-02-18T05:05:26.006356', 'row_count': 4, 'null_count': 0, 'null_ratio': 0.0, 'unique_count': 4, 'min_value': 75.0, 'max_value': 275.0, 'mean_value': 175.0, 'std_value': 102.06207261596575})\n2025-02-17 21:05:29,551 - snowflake_feature_store - INFO - Validated feature AVG_AMOUNT_2 (stats: {'timestamp': '2025-02-18T05:05:28.391694', 'row_count': 4, 'null_count': 0, 'null_ratio': 0.0, 'unique_count': 4, 'min_value': 75.0, 'max_value': 137.5, 'mean_value': 109.375, 'std_value': 27.716947282604313})\n2025-02-17 21:05:42,824 - snowflake_feature_store - INFO - Created feature view: customer_behavior with 7 features\n2025-02-17 21:05:43,755 - snowflake_feature_store - INFO - Applying 2 transforms to new data\n2025-02-17 21:05:47,739 - snowflake_feature_store - WARNING - Drift detected in customer_behavior.AMOUNT: {'null_ratio_change': 0.0, 'mean_shift': 58.75, 'std_ratio': 2.806288338230435}\n2025-02-17 21:05:49,132 - snowflake_feature_store - WARNING - Drift detected in customer_behavior.TRANSACTIONS: {'null_ratio_change': 0.0, 'mean_shift': 0.5, 'std_ratio': 2.1908900109316742}\n2025-02-17 21:05:50,541 - snowflake_feature_store - WARNING - Drift detected in customer_behavior.SESSION_LENGTH: {'null_ratio_change': 0.0, 'mean_shift': 18.825, 'std_ratio': 1.3885169313789645}\n2025-02-17 21:05:52,132 - snowflake_feature_store - WARNING - Drift detected in customer_behavior.SUM_AMOUNT_2: {'null_ratio_change': 0.0, 'mean_shift': 15.0, 'std_ratio': 1.5242047106606122}\n2025-02-17 21:05:53,858 - snowflake_feature_store - WARNING - Drift detected in customer_behavior.AVG_AMOUNT_2: {'null_ratio_change': 0.0, 'mean_shift': 80.625, 'std_ratio': 5.61257667646087}\n\nDrift Detection Results:\n\nAMOUNT:\n  null_ratio_change: 0.000\n  mean_shift: 58.750\n  std_ratio: 2.806\n\nTRANSACTIONS:\n  null_ratio_change: 0.000\n  mean_shift: 0.500\n  std_ratio: 2.191\n\nSESSION_LENGTH:\n  null_ratio_change: 0.000\n  mean_shift: 18.825\n  std_ratio: 1.389\n\nSUM_AMOUNT_2:\n  null_ratio_change: 0.000\n  mean_shift: 15.000\n  std_ratio: 1.524\n\nAVG_AMOUNT_2:\n  null_ratio_change: 0.000\n  mean_shift: 80.625\n  std_ratio: 5.613\n2025-02-17 21:05:53,861 - snowflake_feature_store - INFO - Dependencies for customer_behavior: {'customer_behavior'}\n\nFeature Dependencies: {'customer_behavior'}\n\nEntity join keys:\nEntity CUSTOMER: ['CUSTOMER_ID']\n\nSpine DataFrame columns:\n['CUSTOMER_ID', 'DATE']\n\nSpine DataFrame schema:\nCUSTOMER_ID: StringType()\nDATE: DateType()\n2025-02-17 21:05:54,051 - snowflake_feature_store - INFO - Spine DataFrame columns: ['CUSTOMER_ID', 'DATE']\n2025-02-17 21:05:54,052 - snowflake_feature_store - INFO - Spine DataFrame schema: StructType([StructField('CUSTOMER_ID', StringType(), nullable=True), StructField('DATE', DateType(), nullable=True)])\n2025-02-17 21:05:56,507 - snowflake_feature_store - INFO - Generating dataset with name: DATASET_20250218_050556_eea51bbf\n2025-02-17 21:05:56,508 - snowflake_feature_store - INFO - Label columns: ['\"TRANSACTIONS\"']\n2025-02-17 21:05:56,509 - snowflake_feature_store - INFO - Timestamp column: \"DATE\"\n\nTraining Data Sample:\n----------------------------------------------------------------------------------------------------------------\n|\"CUSTOMER_ID\"  |\"DATE\"      |\"AMOUNT\"  |\"TRANSACTIONS\"  |\"SESSION_LENGTH\"   |\"SUM_AMOUNT_2\"  |\"AVG_AMOUNT_2\"  |\n----------------------------------------------------------------------------------------------------------------\n|C2             |2024-01-01  |75.0      |1               |0.0                |75.0            |75.0            |\n|C2             |2024-01-02  |200.0     |4               |45.20000076293945  |275.0           |137.5           |\n----------------------------------------------------------------------------------------------------------------\n\n\nFeature Statistics:\n\nAMOUNT:\nTimestamp: 2025-02-18T05:05:34.852313\nRow count: 4\nNull count: 0 (0.0%)\nUnique values: 4\nMin value: 75.00\nMax value: 200.00\nMean value: 131.25\nStd dev: 55.43\n\nTRANSACTIONS:\nTimestamp: 2025-02-18T05:05:36.576349\nRow count: 4\nNull count: 0 (0.0%)\nUnique values: 4\nMin value: 1.00\nMax value: 4.00\nMean value: 2.50\nStd dev: 1.29\n\nSESSION_LENGTH:\nTimestamp: 2025-02-18T05:05:38.475332\nRow count: 4\nNull count: 0 (0.0%)\nUnique values: 3\nMin value: 0.00\nMax value: 45.20\nMean value: 18.93\nStd dev: 22.66\n\nSUM_AMOUNT_2:\nTimestamp: 2025-02-18T05:05:39.955529\nRow count: 4\nNull count: 0 (0.0%)\nUnique values: 4\nMin value: 75.00\nMax value: 275.00\nMean value: 175.00\nStd dev: 102.06\n\nAVG_AMOUNT_2:\nTimestamp: 2025-02-18T05:05:41.388139\nRow count: 4\nNull count: 0 (0.0%)\nUnique values: 4\nMin value: 75.00\nMax value: 137.50\nMean value: 109.38\nStd dev: 27.72\n\nTraining Data Schema:\nCUSTOMER_ID: StringType()\nDATE: DateType()\nAMOUNT: DoubleType()\nTRANSACTIONS: LongType()\nSESSION_LENGTH: DoubleType()\nSUM_AMOUNT_2: DoubleType()\nAVG_AMOUNT_2: DoubleType()\n2025-02-17 21:06:02,055 - snowflake_feature_store - INFO - Cleaned up schema FEATURE_STORE_20250218_050511_c9c725ea",
    "crumbs": [
      "Manager (Features)"
    ]
  },
  {
    "objectID": "kaggle_example.html",
    "href": "kaggle_example.html",
    "title": "Kaggle Competition Example",
    "section": "",
    "text": "import os\nfrom typing import Optional\nfrom pathlib import Path\nimport logging\nfrom datetime import datetime, timedelta\n\nimport snowflake.snowpark.functions as F\nfrom snowflake.snowpark import Window\n\nfrom snowflake_feature_store.connection import get_connection\nfrom snowflake_feature_store.manager import FeatureStoreManager\nfrom snowflake_feature_store.config import (\n    FeatureViewConfig, FeatureConfig, RefreshConfig, \n    FeatureValidationConfig\n)\nfrom snowflake_feature_store.transforms import (\n    Transform, TransformConfig, moving_agg, \n    fill_na, date_diff, CustomTransform\n)\nfrom snowflake_feature_store.examples import (\n    get_example_data, create_feature_configs\n)\nfrom snowflake_feature_store.logging import logger\n\n\n# Connect to Snowflake and create feature store\nconn = get_connection(\n    database=\"DATASCIENCE\",\n    schema=\"FEATURE_STORE\",\n    create_objects=True\n)\n\n2025-03-03 17:22:26,166 - snowflake_feature_store - INFO - No active session found, creating new connection from environment\n2025-03-03 17:22:26,878 - snowflake_feature_store - INFO - Initialized connection to \"DATASCIENCE\".\"FEATURE_STORE\"\n2025-03-03 17:22:27,778 - snowflake_feature_store - INFO - Using role: \"ACCOUNTADMIN\", warehouse: \"CONTAINER_DEMO_WH\", database: DATASCIENCE, schema: FEATURE_STORE\n\n\n\n# Create a feature store manager\nmanager = FeatureStoreManager(\n    connection=conn,\n    overwrite=True\n)\n\n2025-03-03 17:22:30,633 - snowflake_feature_store - INFO - FeatureStoreManager initialized\n\n\n\n1. Define Entities\n\n# User entity\nmanager.add_entity(\n    name=\"USER\",\n    join_keys=[\"USER_ID\"],\n    description=\"Instacart users who place orders\"\n)\n\n# Product entity\nmanager.add_entity(\n    name=\"PRODUCT\",\n    join_keys=[\"PRODUCT_ID\"],\n    description=\"Products available in Instacart\"\n)\n\n# User-Product entity\nmanager.add_entity(\n    name=\"USER_PRODUCT\",\n    join_keys=[\"USER_ID\", \"PRODUCT_ID\"],\n    description=\"Interactions between users and products\"\n)\n\n/Users/jdemlow/miniconda3/envs/feature-store/lib/python3.11/site-packages/snowflake/ml/feature_store/feature_store.py:197: UserWarning: Entity USER already exists. Skip registration.\n  return f(self, *args, **kargs)\n/Users/jdemlow/miniconda3/envs/feature-store/lib/python3.11/site-packages/snowflake/ml/feature_store/feature_store.py:197: UserWarning: Entity PRODUCT already exists. Skip registration.\n  return f(self, *args, **kargs)\n/Users/jdemlow/miniconda3/envs/feature-store/lib/python3.11/site-packages/snowflake/ml/feature_store/feature_store.py:197: UserWarning: Entity USER_PRODUCT already exists. Skip registration.\n  return f(self, *args, **kargs)\n\n\n&lt;snowflake_feature_store.manager.FeatureStoreManager at 0x143615090&gt;\n\n\n\n\n2. Create base DataFrames for features\n\nUser features\n\nuser_features_df = conn.session.sql(\"\"\"\nSELECT \n    USER_ID,\n    COUNT(DISTINCT ORDER_ID) AS USER_TOTAL_ORDERS,\n    AVG(DAYS_SINCE_PRIOR_ORDER) AS AVG_DAYS_BETWEEN_ORDERS,\n    PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY ORDER_HOUR_OF_DAY) AS TYPICAL_ORDER_HOUR,\n    MODE(ORDER_DOW) AS PREFERRED_ORDER_DAY,\n    AVG(BASKET_SIZE) AS AVG_BASKET_SIZE,\n    COUNT(DISTINCT PRODUCT_ID) AS DISTINCT_PRODUCTS_COUNT,\n    SUM(REORDERED) / NULLIF(COUNT(REORDERED), 0) AS USER_REORDER_RATE\nFROM (\n    SELECT \n        o.USER_ID, \n        o.ORDER_ID,\n        o.ORDER_DOW,\n        o.ORDER_HOUR_OF_DAY,\n        o.DAYS_SINCE_PRIOR_ORDER,\n        COUNT(op.PRODUCT_ID) AS BASKET_SIZE,\n        op.PRODUCT_ID,\n        op.REORDERED,\n        DATEADD('day', o.order_dow, TO_DATE('2023-01-01')) AS synthetic_date\n    FROM INSTACART_RAW.ORDERS o\n    JOIN INSTACART_RAW.ORDER_PRODUCTS op ON o.ORDER_ID = op.ORDER_ID\n    WHERE o.EVAL_SET = 'prior'\n    GROUP BY 1, 2, 3, 4, 5, 7, 8\n) user_orders\nGROUP BY USER_ID\n\"\"\")\n\n\n\nProduct features\n\nproduct_features_df = conn.session.sql(\"\"\"\nSELECT \n    p.PRODUCT_ID,\n    p.AISLE_ID,\n    p.DEPARTMENT_ID,\n    COUNT(DISTINCT op.ORDER_ID) AS PRODUCT_ORDERS,\n    SUM(op.REORDERED) AS PRODUCT_REORDERS,\n    SUM(op.REORDERED) / NULLIF(COUNT(CASE WHEN op.REORDERED = 1 THEN 1 END), 0) AS PRODUCT_REORDER_RATE,\n    AVG(op.ADD_TO_CART_ORDER) AS PRODUCT_AVG_CART_POSITION\nFROM INSTACART_RAW.PRODUCTS p\nJOIN INSTACART_RAW.ORDER_PRODUCTS op ON p.PRODUCT_ID = op.PRODUCT_ID\nJOIN INSTACART_RAW.ORDERS o ON op.ORDER_ID = o.ORDER_ID\nWHERE o.EVAL_SET = 'prior'\nGROUP BY p.PRODUCT_ID, p.AISLE_ID, p.DEPARTMENT_ID\n\"\"\")\n\n\n\n\nUser-Product features\n\nuser_product_features_df = conn.session.sql(\"\"\"\nWITH user_product_history AS (\n    SELECT \n        o.USER_ID,\n        op.PRODUCT_ID,\n        o.ORDER_ID,\n        o.ORDER_NUMBER,\n        o.ORDER_DOW,\n        o.ORDER_HOUR_OF_DAY,\n        op.REORDERED,\n        op.ADD_TO_CART_ORDER,\n        ROW_NUMBER() OVER (PARTITION BY o.USER_ID, op.PRODUCT_ID ORDER BY o.ORDER_NUMBER DESC) AS rn,\n        COUNT(*) OVER (PARTITION BY o.USER_ID, op.PRODUCT_ID) AS UP_ORDERS,\n        AVG(op.ADD_TO_CART_ORDER) OVER (PARTITION BY o.USER_ID, op.PRODUCT_ID) AS UP_AVG_CART_POSITION\n    FROM INSTACART_RAW.ORDERS o\n    JOIN INSTACART_RAW.ORDER_PRODUCTS op ON o.ORDER_ID = op.ORDER_ID\n    WHERE o.EVAL_SET = 'prior'\n),\nuser_last_order AS (\n    SELECT \n        USER_ID, \n        MAX(ORDER_NUMBER) AS LAST_ORDER_NUMBER\n    FROM INSTACART_RAW.ORDERS\n    WHERE EVAL_SET = 'prior'\n    GROUP BY USER_ID\n)\nSELECT \n    h.USER_ID,\n    h.PRODUCT_ID,\n    h.ORDER_ID,\n    h.ORDER_NUMBER,\n    DATEADD('day', h.order_dow, TO_DATE('2023-01-01')) AS synthetic_date,\n    h.ORDER_DOW,\n    h.ORDER_HOUR_OF_DAY,\n    h.REORDERED,\n    h.UP_ORDERS,\n    h.UP_AVG_CART_POSITION,\n    l.LAST_ORDER_NUMBER - h.ORDER_NUMBER AS ORDERS_SINCE_LAST_PURCHASE\nFROM user_product_history h\nJOIN user_last_order l ON h.USER_ID = l.USER_ID\nWHERE h.rn = 1\n\"\"\")\n\n\n\n3. Define Feature Configurations\n\n# User feature configs\nuser_feature_configs = {\n    \"USER_TOTAL_ORDERS\": FeatureConfig(\n        name=\"USER_TOTAL_ORDERS\",\n        description=\"Total number of orders placed by user\",\n        validation=FeatureValidationConfig(null_threshold=0.0, range_check=True, min_value=1)\n    ),\n    \"AVG_DAYS_BETWEEN_ORDERS\": FeatureConfig(\n        name=\"AVG_DAYS_BETWEEN_ORDERS\",\n        description=\"Average days between orders\",\n        validation=FeatureValidationConfig(null_threshold=0.1, range_check=True, min_value=0)\n    ),\n    \"TYPICAL_ORDER_HOUR\": FeatureConfig(\n        name=\"TYPICAL_ORDER_HOUR\",\n        description=\"Median hour of day when user places orders\",\n        validation=FeatureValidationConfig(null_threshold=0.0, range_check=True, min_value=0, max_value=23)\n    ),\n    \"PREFERRED_ORDER_DAY\": FeatureConfig(\n        name=\"PREFERRED_ORDER_DAY\",\n        description=\"Most common day of week for orders (0=Sunday)\",\n        validation=FeatureValidationConfig(null_threshold=0.0, range_check=True, min_value=0, max_value=6)\n    ),\n    \"AVG_BASKET_SIZE\": FeatureConfig(\n        name=\"AVG_BASKET_SIZE\",\n        description=\"Average number of products per order\",\n        validation=FeatureValidationConfig(null_threshold=0.0, range_check=True, min_value=1)\n    ),\n    \"DISTINCT_PRODUCTS_COUNT\": FeatureConfig(\n        name=\"DISTINCT_PRODUCTS_COUNT\",\n        description=\"Number of unique products ordered\",\n        validation=FeatureValidationConfig(null_threshold=0.0, range_check=True, min_value=1)\n    ),\n    \"USER_REORDER_RATE\": FeatureConfig(\n        name=\"USER_REORDER_RATE\",\n        description=\"Proportion of products that are reordered\",\n        validation=FeatureValidationConfig(null_threshold=0.0, range_check=True, min_value=0, max_value=1)\n    ),\n    \"DOMINANT_DAY_PART\": FeatureConfig(\n        name=\"DOMINANT_DAY_PART\",\n        description=\"Most common time of day for orders\",\n        validation=FeatureValidationConfig(null_threshold=0.1)\n    )\n}\n\n\n# Product feature configs\nproduct_feature_configs = {\n    \"AISLE_ID\": FeatureConfig(\n        name=\"AISLE_ID\",\n        description=\"Aisle ID for the product\",\n        validation=FeatureValidationConfig(null_threshold=0.0)\n    ),\n    \"DEPARTMENT_ID\": FeatureConfig(\n        name=\"DEPARTMENT_ID\",\n        description=\"Department ID for the product\",\n        validation=FeatureValidationConfig(null_threshold=0.0)\n    ),\n    \"PRODUCT_ORDERS\": FeatureConfig(\n        name=\"PRODUCT_ORDERS\",\n        description=\"Number of orders containing this product\",\n        validation=FeatureValidationConfig(null_threshold=0.0, range_check=True, min_value=1)\n    ),\n    \"PRODUCT_REORDERS\": FeatureConfig(\n        name=\"PRODUCT_REORDERS\",\n        description=\"Number of times this product was reordered\",\n        validation=FeatureValidationConfig(null_threshold=0.0, range_check=True, min_value=0)\n    ),\n    \"PRODUCT_REORDER_RATE\": FeatureConfig(\n        name=\"PRODUCT_REORDER_RATE\",\n        description=\"Proportion of orders that are reorders\",\n        validation=FeatureValidationConfig(null_threshold=0.1, range_check=True, min_value=0, max_value=1)\n    ),\n    \"PRODUCT_AVG_CART_POSITION\": FeatureConfig(\n        name=\"PRODUCT_AVG_CART_POSITION\",\n        description=\"Average position in cart\",\n        validation=FeatureValidationConfig(null_threshold=0.0, range_check=True, min_value=1)\n    ),\n    \"DEPARTMENT_POPULARITY_RANK\": FeatureConfig(\n        name=\"DEPARTMENT_POPULARITY_RANK\",\n        description=\"Popularity rank within department\",\n        validation=FeatureValidationConfig(null_threshold=0.1, range_check=True, min_value=1)\n    ),\n    \"AISLE_POPULARITY_RANK\": FeatureConfig(\n        name=\"AISLE_POPULARITY_RANK\",\n        description=\"Popularity rank within aisle\",\n        validation=FeatureValidationConfig(null_threshold=0.1, range_check=True, min_value=1)\n    )\n}\n\n\n# User-Product feature configs\nuser_product_feature_configs = {\n    \"ORDER_DOW\": FeatureConfig(\n        name=\"ORDER_DOW\",\n        description=\"Day of week for the order\"\n    ),\n    \"SYNTHETIC_DATE\": FeatureConfig(\n        name=\"SYNTHETIC_DATE\",\n        description=\"Created Date For Feature Store\"\n    ),\n    \"ORDER_HOUR_OF_DAY\": FeatureConfig(\n        name=\"ORDER_HOUR_OF_DAY\",\n        description=\"Hour of day for the order\"\n    ),\n    \"REORDERED\": FeatureConfig(\n        name=\"REORDERED\",\n        description=\"Whether the product was reordered\"\n    ),\n    \"UP_ORDERS\": FeatureConfig(\n        name=\"UP_ORDERS\",\n        description=\"Number of times user ordered this product\",\n        validation=FeatureValidationConfig(null_threshold=0.0, range_check=True, min_value=1)\n    ),\n    \"UP_AVG_CART_POSITION\": FeatureConfig(\n        name=\"UP_AVG_CART_POSITION\",\n        description=\"Average cart position for this user-product\",\n        validation=FeatureValidationConfig(null_threshold=0.0, range_check=True, min_value=1)\n    ),\n    \"ORDERS_SINCE_LAST_PURCHASE\": FeatureConfig(\n        name=\"ORDERS_SINCE_LAST_PURCHASE\",\n        description=\"Number of orders since user last purchased this product\",\n        validation=FeatureValidationConfig(null_threshold=0.1, range_check=True, min_value=0)\n    ),\n    \"UP_ORDERS_RATIO\": FeatureConfig(\n        name=\"UP_ORDERS_RATIO\",\n        description=\"Ratio of orders containing this product to total user orders\",\n        validation=FeatureValidationConfig(null_threshold=0.1, range_check=True, min_value=0, max_value=1)\n    ),\n    \"PURCHASE_RECENCY_BUCKET\": FeatureConfig(\n        name=\"PURCHASE_RECENCY_BUCKET\",\n        description=\"Recency category of last purchase (recent, medium, old)\",\n        validation=FeatureValidationConfig(null_threshold=0.1)\n    )\n}\n\n\n\n4. Define Feature View Configs\n\n# User feature view config\nuser_config = FeatureViewConfig(\n    name=\"user_features\",\n    domain=\"INSTACART\",\n    entity=\"USER\",\n    feature_type=\"BEHAVIOR\",\n    refresh=RefreshConfig(frequency=\"1 day\", mode=\"INCREMENTAL\"),\n    features=user_feature_configs,\n    description=\"User behavior features for Instacart market basket prediction\"\n)\n\n\n# Product feature view config\nproduct_config = FeatureViewConfig(\n    name=\"product_features\",\n    domain=\"INSTACART\",\n    entity=\"PRODUCT\",\n    feature_type=\"ATTRIBUTE\",\n    refresh=RefreshConfig(frequency=\"1 day\", mode=\"INCREMENTAL\"),\n    features=product_feature_configs,\n    description=\"Product features for Instacart market basket prediction\"\n)\n\n\n# User-Product feature view config\nuser_product_config = FeatureViewConfig(\n    name=\"user_product_features\",\n    domain=\"INSTACART\",\n    entity=\"USER_PRODUCT\",\n    feature_type=\"INTERACTION\",\n    refresh=RefreshConfig(frequency=\"1 day\", mode=\"INCREMENTAL\"),\n    features=user_product_feature_configs,  # Updated feature configs\n    description=\"User-product interaction features for Instacart market basket prediction\"\n)\n\n\n\n5. Define Transformations\n\nUser transformations\n\nuser_transform_config = TransformConfig(\n    name=\"user_transforms\",\n    null_threshold=0.1,\n    expected_types=['DECIMAL', 'DOUBLE', 'NUMBER', 'INT', 'STRING']\n)\n\nuser_transforms = [\n    CustomTransform(\n        transform_func=lambda df: df.with_column(\n            'DOMINANT_DAY_PART',\n            F.when(F.col(\"TYPICAL_ORDER_HOUR\") &lt; 6, F.lit(\"night\"))\n             .when(F.col(\"TYPICAL_ORDER_HOUR\") &lt; 12, F.lit(\"morning\"))\n             .when(F.col(\"TYPICAL_ORDER_HOUR\") &lt; 18, F.lit(\"midday\"))\n             .otherwise(F.lit(\"evening\"))\n        ),\n        config=user_transform_config\n    ),\n    CustomTransform(\n        transform_func=lambda df: df.with_column(\n            'DOMINANT_DOW',\n            F.col(\"PREFERRED_ORDER_DAY\")\n        ),\n        config=user_transform_config\n    ),\n    \n    # Fill nulls in numeric columns\n    fill_na(\n        cols=[\"AVG_DAYS_BETWEEN_ORDERS\", \"AVG_BASKET_SIZE\"],\n        fill_value=0.0,\n        config=user_transform_config\n    )\n]\n\n\n\nProduct transformations\n\nproduct_transform_config = TransformConfig(\n    name=\"product_transforms\",\n    null_threshold=0.1,\n    expected_types=['DECIMAL', 'DOUBLE', 'NUMBER', 'INT']\n)\n\n# Corrected product transforms with proper Window syntax\nproduct_transforms = [\n    # Add department popularity rank\n    CustomTransform(\n        transform_func=lambda df: df.with_column(\n            'DEPARTMENT_POPULARITY_RANK',\n            F.dense_rank().over(\n                Window.partition_by(\"DEPARTMENT_ID\").order_by(F.col(\"PRODUCT_ORDERS\").desc())\n            )\n        ),\n        config=product_transform_config\n    ),\n    \n    # Add aisle popularity rank\n    CustomTransform(\n        transform_func=lambda df: df.with_column(\n            'AISLE_POPULARITY_RANK',\n            F.dense_rank().over(\n                Window.partition_by(\"AISLE_ID\").order_by(F.col(\"PRODUCT_ORDERS\").desc())\n            )\n        ),\n        config=product_transform_config\n    )\n]\n\n\n\nUser-Product transformations\n\nprint(\"Available columns in user_product_features_df:\")\nprint(user_product_features_df.columns)\n\n# Now let's create a simplified version of the transforms that will work\nuser_product_transform_config = TransformConfig(\n    name=\"user_product_transforms\",\n    null_threshold=0.1,\n    expected_types=['DECIMAL', 'DOUBLE', 'NUMBER', 'INT', 'STRING']\n)\n\nuser_product_transforms = [\n    # Just add UP_ORDERS_RATIO column\n    CustomTransform(\n        transform_func=lambda df: df.with_column(\n            \"UP_ORDERS_RATIO\", (F.col(\"UP_ORDERS\") / F.lit(1.0))\n        ),\n        config=user_product_transform_config\n    ),\n    \n    # Add PURCHASE_RECENCY_BUCKET column\n    CustomTransform(\n        transform_func=lambda df: df.with_column(\n            \"PURCHASE_RECENCY_BUCKET\", \n            F.when(F.col(\"ORDERS_SINCE_LAST_PURCHASE\") == 0, F.lit(\"recent\"))\n             .when(F.col(\"ORDERS_SINCE_LAST_PURCHASE\") &lt;= 3, F.lit(\"medium\"))\n             .otherwise(F.lit(\"old\"))\n        ),\n        config=user_product_transform_config\n    )\n]\n\nAvailable columns in user_product_features_df:\n['USER_ID', 'PRODUCT_ID', 'ORDER_ID', 'ORDER_NUMBER', 'SYNTHETIC_DATE', 'ORDER_DOW', 'ORDER_HOUR_OF_DAY', 'REORDERED', 'UP_ORDERS', 'UP_AVG_CART_POSITION', 'ORDERS_SINCE_LAST_PURCHASE']\n\n\n\n\n\n6. Create Feature Views\n\nuser_feature_view = manager.add_feature_view(\n    config=user_config,\n    df=user_features_df,\n    entity_name=\"USER\",\n    transforms=user_transforms,\n    collect_stats=True\n)\n\nInput value type doesn't match the target column data type, this replacement was skipped. Column Name: \"AVG_BASKET_SIZE\", Type: DecimalType(36, 6), Input Value: 0.0, Type: &lt;class 'str'&gt;\n\n\n2025-03-03 17:31:53,939 - snowflake_feature_store - INFO - Validated feature USER_TOTAL_ORDERS (stats: {'timestamp': '2025-03-04T01:31:43.864602', 'row_count': 206209, 'null_count': 0, 'null_ratio': 0.0, 'unique_count': 97, 'min_value': 3.0, 'max_value': 99.0, 'mean_value': 15.590367, 'std_value': 16.65477348990373})\n2025-03-03 17:32:03,939 - snowflake_feature_store - INFO - Validated feature AVG_DAYS_BETWEEN_ORDERS (stats: {'timestamp': '2025-03-04T01:31:54.383766', 'row_count': 206209, 'null_count': 0, 'null_ratio': 0.0, 'unique_count': 99570, 'min_value': 0.0, 'max_value': 30.0, 'mean_value': 15.469669692770578, 'std_value': 7.207435949657587})\n2025-03-03 17:32:21,807 - snowflake_feature_store - INFO - Validated feature TYPICAL_ORDER_HOUR (stats: {'timestamp': '2025-03-04T01:32:08.452892', 'row_count': 206209, 'null_count': 0, 'null_ratio': 0.0, 'unique_count': 47, 'min_value': 0.0, 'max_value': 23.0, 'mean_value': 13.52991625, 'std_value': 2.849790670923568})\n2025-03-03 17:32:36,393 - snowflake_feature_store - INFO - Validated feature PREFERRED_ORDER_DAY (stats: {'timestamp': '2025-03-04T01:32:25.326038', 'row_count': 206209, 'null_count': 0, 'null_ratio': 0.0, 'unique_count': 7, 'min_value': 0.0, 'max_value': 6.0, 'mean_value': 2.583282, 'std_value': 2.1554667707946695})\n2025-03-03 17:32:51,321 - snowflake_feature_store - INFO - Validated feature AVG_BASKET_SIZE (stats: {'timestamp': '2025-03-04T01:32:36.693819', 'row_count': 206209, 'null_count': 0, 'null_ratio': 0.0, 'unique_count': 1, 'min_value': 1.0, 'max_value': 1.0, 'mean_value': 1.0, 'std_value': 0.0})\n2025-03-03 17:33:05,243 - snowflake_feature_store - INFO - Validated feature DISTINCT_PRODUCTS_COUNT (stats: {'timestamp': '2025-03-04T01:32:54.842348', 'row_count': 206209, 'null_count': 0, 'null_ratio': 0.0, 'unique_count': 493, 'min_value': 1.0, 'max_value': 726.0, 'mean_value': 64.536238, 'std_value': 56.59233864225793})\n2025-03-03 17:33:18,875 - snowflake_feature_store - INFO - Validated feature USER_REORDER_RATE (stats: {'timestamp': '2025-03-04T01:33:08.412607', 'row_count': 206209, 'null_count': 0, 'null_ratio': 0.0, 'unique_count': 31274, 'min_value': 0.0, 'max_value': 0.989529, 'mean_value': 0.432249265439, 'std_value': 0.21214404798862493})\n2025-03-03 17:33:27,818 - snowflake_feature_store - INFO - Validated feature DOMINANT_DAY_PART (stats: {'timestamp': '2025-03-04T01:33:23.369257', 'row_count': 206209, 'null_count': 0, 'null_ratio': 0.0, 'unique_count': 4, 'min_value': None, 'max_value': None, 'mean_value': None, 'std_value': None})\n\n\n/Users/jdemlow/miniconda3/envs/feature-store/lib/python3.11/site-packages/snowflake/ml/feature_store/feature_store.py:1877: UserWarning: Your pipeline won't be incrementally refreshed due to: \"Change tracking is not supported on queries containing the function 'PERCENTILE_CONT'.\".\n  self._check_dynamic_table_refresh_mode(feature_view_name)\n\n\n\nproduct_feature_view = manager.add_feature_view(\n    config=product_config,\n    df=product_features_df,\n    entity_name=\"PRODUCT\",\n    transforms=product_transforms,\n    collect_stats=True\n)\n\n2025-03-03 17:33:59,434 - snowflake_feature_store - INFO - Validated feature AISLE_ID (stats: {'timestamp': '2025-03-04T01:33:54.478943', 'row_count': 49677, 'null_count': 0, 'null_ratio': 0.0, 'unique_count': 134, 'min_value': 1.0, 'max_value': 134.0, 'mean_value': 67.769189, 'std_value': 38.317847251639805})\n2025-03-03 17:34:04,542 - snowflake_feature_store - INFO - Validated feature DEPARTMENT_ID (stats: {'timestamp': '2025-03-04T01:33:59.753146', 'row_count': 49677, 'null_count': 0, 'null_ratio': 0.0, 'unique_count': 21, 'min_value': 1.0, 'max_value': 21.0, 'mean_value': 11.727802, 'std_value': 5.8503070004915125})\n2025-03-03 17:34:14,571 - snowflake_feature_store - INFO - Validated feature PRODUCT_ORDERS (stats: {'timestamp': '2025-03-04T01:34:07.033387', 'row_count': 49677, 'null_count': 0, 'null_ratio': 0.0, 'unique_count': 4161, 'min_value': 1.0, 'max_value': 472565.0, 'mean_value': 652.907563, 'std_value': 4792.114415774002})\n2025-03-03 17:34:18,909 - snowflake_feature_store - INFO - Validated feature PRODUCT_REORDERS (stats: {'timestamp': '2025-03-04T01:34:15.722067', 'row_count': 49677, 'null_count': 0, 'null_ratio': 0.0, 'unique_count': 3134, 'min_value': 0.0, 'max_value': 398609.0, 'mean_value': 385.017936, 'std_value': 3601.7136460579427})\n2025-03-03 17:34:24,061 - snowflake_feature_store - INFO - Validated feature PRODUCT_REORDER_RATE (stats: {'timestamp': '2025-03-04T01:34:20.028942', 'row_count': 49677, 'null_count': 4372, 'null_ratio': 0.08800853513698492, 'unique_count': 2, 'min_value': 1.0, 'max_value': 1.0, 'mean_value': 1.0, 'std_value': 0.0})\n2025-03-03 17:34:28,391 - snowflake_feature_store - INFO - Validated feature PRODUCT_AVG_CART_POSITION (stats: {'timestamp': '2025-03-04T01:34:25.198282', 'row_count': 49677, 'null_count': 0, 'null_ratio': 0.0, 'unique_count': 26275, 'min_value': 1.0, 'max_value': 53.0, 'mean_value': 9.097568089015, 'std_value': 2.5512671640202638})\n2025-03-03 17:34:38,194 - snowflake_feature_store - INFO - Validated feature DEPARTMENT_POPULARITY_RANK (stats: {'timestamp': '2025-03-04T01:34:30.810365', 'row_count': 49677, 'null_count': 0, 'null_ratio': 0.0, 'unique_count': 1462, 'min_value': 1.0, 'max_value': 1462.0, 'mean_value': 692.939992, 'std_value': 393.0882567681207})\n2025-03-03 17:34:48,228 - snowflake_feature_store - INFO - Validated feature AISLE_POPULARITY_RANK (stats: {'timestamp': '2025-03-04T01:34:40.891883', 'row_count': 49677, 'null_count': 0, 'null_ratio': 0.0, 'unique_count': 646, 'min_value': 1.0, 'max_value': 646.0, 'mean_value': 180.192604, 'std_value': 127.69884932136233})\n\n\n/Users/jdemlow/miniconda3/envs/feature-store/lib/python3.11/site-packages/snowflake/ml/feature_store/feature_store.py:1877: UserWarning: Your pipeline won't be incrementally refreshed due to: \"Change tracking is not supported on queries with window functions that have disjoint partition keys.\".\n  self._check_dynamic_table_refresh_mode(feature_view_name)\n\n\n\n# Now try creating the feature view again\nuser_product_feature_view = manager.add_feature_view(\n    config=user_product_config,\n    df=user_product_features_df,\n    entity_name=\"USER_PRODUCT\",\n    transforms=user_product_transforms,\n    collect_stats=True\n)\n\n2025-03-03 17:35:35,926 - snowflake_feature_store - INFO - Validated feature ORDER_DOW (stats: {'timestamp': '2025-03-04T01:35:12.678529', 'row_count': 13307953, 'null_count': 0, 'null_ratio': 0.0, 'unique_count': 7, 'min_value': 0.0, 'max_value': 6.0, 'mean_value': 2.753886, 'std_value': 2.0996980735334305})\n2025-03-03 17:35:52,093 - snowflake_feature_store - INFO - Validated feature SYNTHETIC_DATE (stats: {'timestamp': '2025-03-04T01:35:44.005099', 'row_count': 13307953, 'null_count': 0, 'null_ratio': 0.0, 'unique_count': 7, 'min_value': None, 'max_value': None, 'mean_value': None, 'std_value': None})\n2025-03-03 17:36:16,029 - snowflake_feature_store - INFO - Validated feature ORDER_HOUR_OF_DAY (stats: {'timestamp': '2025-03-04T01:35:52.431130', 'row_count': 13307953, 'null_count': 0, 'null_ratio': 0.0, 'unique_count': 24, 'min_value': 0.0, 'max_value': 23.0, 'mean_value': 13.514454, 'std_value': 4.226126358735621})\n2025-03-03 17:36:38,680 - snowflake_feature_store - INFO - Validated feature REORDERED (stats: {'timestamp': '2025-03-04T01:36:16.408837', 'row_count': 13307953, 'null_count': 0, 'null_ratio': 0.0, 'unique_count': 2, 'min_value': 0.0, 'max_value': 1.0, 'mean_value': 0.400156, 'std_value': 0.4899295867775287})\n2025-03-03 17:37:12,197 - snowflake_feature_store - INFO - Validated feature UP_ORDERS (stats: {'timestamp': '2025-03-04T01:36:47.477476', 'row_count': 13307953, 'null_count': 0, 'null_ratio': 0.0, 'unique_count': 99, 'min_value': 1.0, 'max_value': 99.0, 'mean_value': 2.437226, 'std_value': 3.5545275354117036})\n2025-03-03 17:37:48,296 - snowflake_feature_store - INFO - Validated feature UP_AVG_CART_POSITION (stats: {'timestamp': '2025-03-04T01:37:20.925876', 'row_count': 13307953, 'null_count': 0, 'null_ratio': 0.0, 'unique_count': 12379, 'min_value': 1.0, 'max_value': 145.0, 'mean_value': 9.217372202, 'std_value': 6.984274209769116})\n2025-03-03 17:38:18,408 - snowflake_feature_store - INFO - Validated feature ORDERS_SINCE_LAST_PURCHASE (stats: {'timestamp': '2025-03-04T01:37:55.863574', 'row_count': 13307953, 'null_count': 0, 'null_ratio': 0.0, 'unique_count': 99, 'min_value': 0.0, 'max_value': 98.0, 'mean_value': 9.512767, 'std_value': 13.416951404846035})\n2025-03-03 17:38:51,105 - snowflake_feature_store - INFO - Validated feature UP_ORDERS_RATIO (stats: {'timestamp': '2025-03-04T01:38:26.493033', 'row_count': 13307953, 'null_count': 0, 'null_ratio': 0.0, 'unique_count': 99, 'min_value': 1.0, 'max_value': 99.0, 'mean_value': 2.437225995613, 'std_value': 3.554527520515631})\n2025-03-03 17:39:06,630 - snowflake_feature_store - INFO - Validated feature PURCHASE_RECENCY_BUCKET (stats: {'timestamp': '2025-03-04T01:38:59.048885', 'row_count': 13307953, 'null_count': 0, 'null_ratio': 0.0, 'unique_count': 3, 'min_value': None, 'max_value': None, 'mean_value': None, 'std_value': None})\n\n\n/Users/jdemlow/miniconda3/envs/feature-store/lib/python3.11/site-packages/snowflake/ml/feature_store/feature_store.py:1877: UserWarning: Your pipeline won't be incrementally refreshed due to: \"This dynamic table contains a complex query. Refresh mode has been set to FULL. If you wish to override this automatic choice, please re-create the dynamic table and specify REFRESH_MODE=INCREMENTAL. For best results, we recommend reading https://docs.snowflake.com/user-guide/dynamic-table-performance-guide before setting the refresh mode to INCREMENTAL.\".\n  self._check_dynamic_table_refresh_mode(feature_view_name)\n\n\n\n\n7. Generate Training Dataset\n\n# Create spine for training data\nspine_df = conn.session.sql(\"\"\"\nSELECT \n    o.USER_ID,\n    op.PRODUCT_ID,\n    o.ORDER_ID,\n    o.ORDER_NUMBER,\n    o.ORDER_DOW,\n    o.ORDER_HOUR_OF_DAY,\n    op.REORDERED\nFROM INSTACART_RAW.ORDERS o\nJOIN INSTACART_RAW.ORDER_PRODUCTS op ON o.ORDER_ID = op.ORDER_ID\nWHERE o.EVAL_SET = 'train'\n\"\"\")\n\nspine_df.show(5)\n\n------------------------------------------------------------------------------------------------------------\n|\"USER_ID\"  |\"PRODUCT_ID\"  |\"ORDER_ID\"  |\"ORDER_NUMBER\"  |\"ORDER_DOW\"  |\"ORDER_HOUR_OF_DAY\"  |\"REORDERED\"  |\n------------------------------------------------------------------------------------------------------------\n|112108     |49302         |1           |4               |4            |10                   |1            |\n|112108     |11109         |1           |4               |4            |10                   |1            |\n|112108     |10246         |1           |4               |4            |10                   |0            |\n|112108     |49683         |1           |4               |4            |10                   |0            |\n|112108     |43633         |1           |4               |4            |10                   |1            |\n------------------------------------------------------------------------------------------------------------\n\n\n\n\n# Generate training dataset with all features\ntraining_data = manager.get_features(\n    spine_df=spine_df,\n    feature_views=[user_config, product_config, user_product_config],\n    label_cols=[\"REORDERED\"],\n    spine_timestamp_col=None  # No timestamp needed for this dataset\n)\ntraining_data.show(5)\n\n2025-02-28 17:46:13,023 - snowflake_feature_store - INFO - Spine DataFrame columns: ['USER_ID', 'PRODUCT_ID', 'ORDER_ID', 'ORDER_NUMBER', 'ORDER_DOW', 'ORDER_HOUR_OF_DAY', 'REORDERED']\n2025-02-28 17:46:13,024 - snowflake_feature_store - INFO - Spine DataFrame schema: StructType([StructField('USER_ID', LongType(), nullable=True), StructField('PRODUCT_ID', LongType(), nullable=True), StructField('ORDER_ID', LongType(), nullable=True), StructField('ORDER_NUMBER', LongType(), nullable=True), StructField('ORDER_DOW', LongType(), nullable=True), StructField('ORDER_HOUR_OF_DAY', LongType(), nullable=True), StructField('REORDERED', LongType(), nullable=True)])\n2025-02-28 17:46:19,091 - snowflake_feature_store - INFO - Generating dataset with name: DATASET_20250301_014619_d1812ae8\n2025-02-28 17:46:19,092 - snowflake_feature_store - INFO - Label columns: ['\"REORDERED\"']\n2025-02-28 17:46:19,093 - snowflake_feature_store - INFO - Timestamp column: None\n\n\n\n---------------------------------------------------------------------------\nSnowparkSQLException                      Traceback (most recent call last)\nFile ~/miniconda3/envs/feature-store/lib/python3.11/site-packages/snowflake/ml/feature_store/feature_store.py:1665, in FeatureStore.generate_dataset(self, name, spine_df, features, version, spine_timestamp_col, spine_label_cols, exclude_columns, include_feature_view_timestamp_col, desc, output_type)\n   1664 # TODO: Add feature store tag once Dataset (version) supports tags\n-&gt; 1665 ds: dataset.Dataset = dataset.create_from_dataframe(\n   1666     self._session,\n   1667     name,\n   1668     version,\n   1669     input_dataframe=result_df,\n   1670     exclude_cols=[spine_timestamp_col] if spine_timestamp_col is not None else [],\n   1671     label_cols=spine_label_cols,\n   1672     properties=fs_meta,\n   1673     comment=desc,\n   1674 )\n   1675 return ds\n\nFile ~/miniconda3/envs/feature-store/lib/python3.11/site-packages/snowflake/ml/_internal/telemetry.py:542, in send_api_usage_telemetry.&lt;locals&gt;.decorator.&lt;locals&gt;.wrap(*args, **kwargs)\n    541 try:\n--&gt; 542     return ctx.run(execute_func_with_statement_params)\n    543 except Exception as e:\n\nFile ~/miniconda3/envs/feature-store/lib/python3.11/site-packages/snowflake/ml/_internal/telemetry.py:503, in send_api_usage_telemetry.&lt;locals&gt;.decorator.&lt;locals&gt;.wrap.&lt;locals&gt;.execute_func_with_statement_params()\n    502 _patch_manager.set_statement_params(statement_params)\n--&gt; 503 result = func(*args, **kwargs)\n    504 return update_stmt_params_if_snowpark_df(result, statement_params)\n\nFile ~/miniconda3/envs/feature-store/lib/python3.11/site-packages/snowflake/ml/dataset/dataset_factory.py:33, in create_from_dataframe(session, name, version, input_dataframe, **version_kwargs)\n     32 ds: dataset.Dataset = dataset.Dataset.create(session, name, exist_ok=True)\n---&gt; 33 ds.create_version(version, input_dataframe=input_dataframe, **version_kwargs)\n     34 ds = ds.select_version(version)  # select_version returns a new copy\n\nFile ~/miniconda3/envs/feature-store/lib/python3.11/site-packages/snowflake/ml/_internal/telemetry.py:542, in send_api_usage_telemetry.&lt;locals&gt;.decorator.&lt;locals&gt;.wrap(*args, **kwargs)\n    541 try:\n--&gt; 542     return ctx.run(execute_func_with_statement_params)\n    543 except Exception as e:\n\nFile ~/miniconda3/envs/feature-store/lib/python3.11/site-packages/snowflake/ml/_internal/telemetry.py:503, in send_api_usage_telemetry.&lt;locals&gt;.decorator.&lt;locals&gt;.wrap.&lt;locals&gt;.execute_func_with_statement_params()\n    502 _patch_manager.set_statement_params(statement_params)\n--&gt; 503 result = func(*args, **kwargs)\n    504 return update_stmt_params_if_snowpark_df(result, statement_params)\n\nFile ~/miniconda3/envs/feature-store/lib/python3.11/site-packages/snowflake/ml/dataset/dataset.py:368, in Dataset.create_version(self, version, input_dataframe, shuffle, exclude_cols, label_cols, properties, partition_by, comment)\n    367 sql_command += f\" METADATA=$${metadata.to_json()}$$\"\n--&gt; 368 self._session.sql(sql_command).collect(statement_params=_TELEMETRY_STATEMENT_PARAMS)\n    370 return Dataset(self._session, self._db, self._schema, self._name, version)\n\nFile ~/miniconda3/envs/feature-store/lib/python3.11/site-packages/snowflake/snowpark/_internal/telemetry.py:174, in df_collect_api_telemetry.&lt;locals&gt;.wrap(*args, **kwargs)\n    173 with args[0]._session.query_history() as query_history:\n--&gt; 174     result = func(*args, **kwargs)\n    175 plan = args[0]._select_statement or args[0]._plan\n\nFile ~/miniconda3/envs/feature-store/lib/python3.11/site-packages/snowflake/snowpark/_internal/utils.py:1029, in publicapi.&lt;locals&gt;.call_wrapper(*args, **kwargs)\n   1027 # TODO: Could modify internal docstring to display that users should not modify the _emit_ast parameter.\n-&gt; 1029 return func(*args, **kwargs)\n\nFile ~/miniconda3/envs/feature-store/lib/python3.11/site-packages/snowflake/snowpark/dataframe.py:713, in DataFrame.collect(self, statement_params, block, log_on_exception, case_sensitive, _emit_ast)\n    712 with open_telemetry_context_manager(self.collect, self):\n--&gt; 713     return self._internal_collect_with_tag_no_telemetry(\n    714         statement_params=statement_params,\n    715         block=block,\n    716         log_on_exception=log_on_exception,\n    717         case_sensitive=case_sensitive,\n    718         **kwargs,\n    719     )\n\nFile ~/miniconda3/envs/feature-store/lib/python3.11/site-packages/snowflake/snowpark/dataframe.py:784, in DataFrame._internal_collect_with_tag_no_telemetry(self, statement_params, block, data_type, log_on_exception, case_sensitive, **kwargs)\n    771 def _internal_collect_with_tag_no_telemetry(\n    772     self,\n    773     *,\n   (...)\n    782     # we should always call this method instead of collect(), to make sure the\n    783     # query tag is set properly.\n--&gt; 784     return self._session._conn.execute(\n    785         self._plan,\n    786         block=block,\n    787         data_type=data_type,\n    788         _statement_params=create_or_update_statement_params_with_query_tag(\n    789             statement_params or self._statement_params,\n    790             self._session.query_tag,\n    791             SKIP_LEVELS_THREE,\n    792         ),\n    793         log_on_exception=log_on_exception,\n    794         case_sensitive=case_sensitive,\n    795         **kwargs,\n    796     )\n\nFile ~/miniconda3/envs/feature-store/lib/python3.11/site-packages/snowflake/snowpark/_internal/server_connection.py:609, in ServerConnection.execute(self, plan, to_pandas, to_iter, block, data_type, log_on_exception, case_sensitive, **kwargs)\n    606     raise NotImplementedError(\n    607         \"Async query is not supported in stored procedure yet\"\n    608     )\n--&gt; 609 result_set, result_meta = self.get_result_set(\n    610     plan,\n    611     to_pandas,\n    612     to_iter,\n    613     **kwargs,\n    614     block=block,\n    615     data_type=data_type,\n    616     log_on_exception=log_on_exception,\n    617     case_sensitive=case_sensitive,\n    618 )\n    619 if not block:\n\nFile ~/miniconda3/envs/feature-store/lib/python3.11/site-packages/snowflake/snowpark/_internal/analyzer/snowflake_plan.py:205, in SnowflakePlan.Decorator.wrap_exception.&lt;locals&gt;.wrap(*args, **kwargs)\n    202 ne = SnowparkClientExceptionMessages.SQL_EXCEPTION_FROM_PROGRAMMING_ERROR(\n    203     e\n    204 )\n--&gt; 205 raise ne.with_traceback(tb) from None\n\nFile ~/miniconda3/envs/feature-store/lib/python3.11/site-packages/snowflake/snowpark/_internal/analyzer/snowflake_plan.py:136, in SnowflakePlan.Decorator.wrap_exception.&lt;locals&gt;.wrap(*args, **kwargs)\n    135 try:\n--&gt; 136     return func(*args, **kwargs)\n    137 except snowflake.connector.errors.ProgrammingError as e:\n\nFile ~/miniconda3/envs/feature-store/lib/python3.11/site-packages/snowflake/snowpark/_internal/server_connection.py:727, in ServerConnection.get_result_set(self, plan, to_pandas, to_iter, block, data_type, log_on_exception, case_sensitive, ignore_results, **kwargs)\n    726     kwargs[DATAFRAME_AST_PARAMETER] = dataframe_ast\n--&gt; 727 result = self.run_query(\n    728     final_query,\n    729     to_pandas,\n    730     to_iter and (i == len(main_queries) - 1),\n    731     is_ddl_on_temp_object=query.is_ddl_on_temp_object,\n    732     block=not is_last,\n    733     data_type=data_type,\n    734     async_job_plan=plan,\n    735     log_on_exception=log_on_exception,\n    736     case_sensitive=case_sensitive,\n    737     params=query.params,\n    738     ignore_results=ignore_results,\n    739     async_post_actions=post_actions,\n    740     **kwargs,\n    741 )\n    742 placeholders[query.query_id_place_holder] = (\n    743     result[\"sfqid\"] if not is_last else result.query_id\n    744 )\n\nFile ~/miniconda3/envs/feature-store/lib/python3.11/site-packages/snowflake/snowpark/_internal/server_connection.py:130, in ServerConnection._Decorator.wrap_exception.&lt;locals&gt;.wrap(*args, **kwargs)\n    129 except Exception as ex:\n--&gt; 130     raise ex\n\nFile ~/miniconda3/envs/feature-store/lib/python3.11/site-packages/snowflake/snowpark/_internal/server_connection.py:124, in ServerConnection._Decorator.wrap_exception.&lt;locals&gt;.wrap(*args, **kwargs)\n    123 try:\n--&gt; 124     return func(*args, **kwargs)\n    125 except ReauthenticationRequest as ex:\n\nFile ~/miniconda3/envs/feature-store/lib/python3.11/site-packages/snowflake/snowpark/_internal/server_connection.py:516, in ServerConnection.run_query(self, query, to_pandas, to_iter, is_ddl_on_temp_object, block, data_type, async_job_plan, log_on_exception, case_sensitive, params, num_statements, ignore_results, async_post_actions, **kwargs)\n    515         logger.error(f\"Failed to execute query{query_id_log} {query}\\n{ex}\")\n--&gt; 516     raise ex\n    518 # fetch_pandas_all/batches() only works for SELECT statements\n    519 # We call fetchall() if fetch_pandas_all/batches() fails,\n    520 # because when the query plan has multiple queries, it will\n    521 # have non-select statements, and it shouldn't fail if the user\n    522 # calls to_pandas() to execute the query.\n\nFile ~/miniconda3/envs/feature-store/lib/python3.11/site-packages/snowflake/snowpark/_internal/server_connection.py:501, in ServerConnection.run_query(self, query, to_pandas, to_iter, is_ddl_on_temp_object, block, data_type, async_job_plan, log_on_exception, case_sensitive, params, num_statements, ignore_results, async_post_actions, **kwargs)\n    500 if block:\n--&gt; 501     results_cursor = self.execute_and_notify_query_listener(\n    502         query, params=params, **kwargs\n    503     )\n    504     logger.debug(f\"Execute query [queryID: {results_cursor.sfqid}] {query}\")\n\nFile ~/miniconda3/envs/feature-store/lib/python3.11/site-packages/snowflake/ml/_internal/telemetry.py:183, in _StatementParamsPatchManager._patch_with_statement_params.&lt;locals&gt;.wrapper(*args, **kwargs)\n    182 try:\n--&gt; 183     return func(*args, **kwargs)\n    184 except TypeError as e:\n\nFile ~/miniconda3/envs/feature-store/lib/python3.11/site-packages/snowflake/snowpark/_internal/server_connection.py:443, in ServerConnection.execute_and_notify_query_listener(self, query, **kwargs)\n    440     self.notify_query_listeners(\n    441         QueryRecord(sfqid, err_query, False), is_error=True, **notify_kwargs\n    442     )\n--&gt; 443     raise ex\n    445 notify_kwargs[\"requestId\"] = str(results_cursor._request_id)\n\nFile ~/miniconda3/envs/feature-store/lib/python3.11/site-packages/snowflake/snowpark/_internal/server_connection.py:434, in ServerConnection.execute_and_notify_query_listener(self, query, **kwargs)\n    433 try:\n--&gt; 434     results_cursor = self._cursor.execute(query, **kwargs)\n    435 except Exception as ex:\n\nFile ~/miniconda3/envs/feature-store/lib/python3.11/site-packages/snowflake/connector/cursor.py:1103, in SnowflakeCursor.execute(self, command, params, _bind_stage, timeout, _exec_async, _no_retry, _do_reset, _put_callback, _put_azure_callback, _put_callback_output_stream, _get_callback, _get_azure_callback, _get_callback_output_stream, _show_progress_bar, _statement_params, _is_internal, _describe_only, _no_results, _is_put_get, _raise_put_get_error, _force_put_overwrite, _skip_upload_on_content_match, file_stream, num_statements, _force_qmark_paramstyle, _dataframe_ast)\n   1102     error_class = IntegrityError if is_integrity_error else ProgrammingError\n-&gt; 1103     Error.errorhandler_wrapper(self.connection, self, error_class, errvalue)\n   1104 return self\n\nFile ~/miniconda3/envs/feature-store/lib/python3.11/site-packages/snowflake/connector/errors.py:283, in Error.errorhandler_wrapper(connection, cursor, error_class, error_value)\n    267 \"\"\"Error handler wrapper that calls the errorhandler method.\n    268 \n    269 Args:\n   (...)\n    280     exception to the first handler in that order.\n    281 \"\"\"\n--&gt; 283 handed_over = Error.hand_to_other_handler(\n    284     connection,\n    285     cursor,\n    286     error_class,\n    287     error_value,\n    288 )\n    289 if not handed_over:\n\nFile ~/miniconda3/envs/feature-store/lib/python3.11/site-packages/snowflake/connector/errors.py:338, in Error.hand_to_other_handler(connection, cursor, error_class, error_value)\n    337 cursor.messages.append((error_class, error_value))\n--&gt; 338 cursor.errorhandler(connection, cursor, error_class, error_value)\n    339 return True\n\nFile ~/miniconda3/envs/feature-store/lib/python3.11/site-packages/snowflake/connector/errors.py:214, in Error.default_errorhandler(connection, cursor, error_class, error_value)\n    213 done_format_msg = error_value.get(\"done_format_msg\")\n--&gt; 214 raise error_class(\n    215     msg=error_value.get(\"msg\"),\n    216     errno=None if errno is None else int(errno),\n    217     sqlstate=error_value.get(\"sqlstate\"),\n    218     sfqid=error_value.get(\"sfqid\"),\n    219     query=error_value.get(\"query\"),\n    220     done_format_msg=(\n    221         None if done_format_msg is None else bool(done_format_msg)\n    222     ),\n    223     connection=connection,\n    224     cursor=cursor,\n    225 )\n\nSnowparkSQLException: (1304): 01bab4ea-0004-a5f9-004d-de07052ae67a: 002028 (42601): SQL compilation error:\nambiguous column name 'ORDER_ID'\n\nThe above exception was the direct cause of the following exception:\n\nSnowflakeMLException                      Traceback (most recent call last)\nFile ~/miniconda3/envs/feature-store/lib/python3.11/site-packages/snowflake/ml/_internal/telemetry.py:542, in send_api_usage_telemetry.&lt;locals&gt;.decorator.&lt;locals&gt;.wrap(*args, **kwargs)\n    541 try:\n--&gt; 542     return ctx.run(execute_func_with_statement_params)\n    543 except Exception as e:\n\nFile ~/miniconda3/envs/feature-store/lib/python3.11/site-packages/snowflake/ml/_internal/telemetry.py:503, in send_api_usage_telemetry.&lt;locals&gt;.decorator.&lt;locals&gt;.wrap.&lt;locals&gt;.execute_func_with_statement_params()\n    502 _patch_manager.set_statement_params(statement_params)\n--&gt; 503 result = func(*args, **kwargs)\n    504 return update_stmt_params_if_snowpark_df(result, statement_params)\n\nFile ~/miniconda3/envs/feature-store/lib/python3.11/site-packages/snowflake/ml/feature_store/feature_store.py:178, in switch_warehouse.&lt;locals&gt;.wrapper(self, *args, **kargs)\n    177         warehouse_updated = True\n--&gt; 178     return f(self, *args, **kargs)\n    179 finally:\n\nFile ~/miniconda3/envs/feature-store/lib/python3.11/site-packages/snowflake/ml/feature_store/feature_store.py:197, in dispatch_decorator.&lt;locals&gt;.decorator.&lt;locals&gt;.wrap(self, *args, **kargs)\n    193 @telemetry.send_api_usage_telemetry(project=_PROJECT)\n    194 @switch_warehouse\n    195 @functools.wraps(f)\n    196 def wrap(self: FeatureStore, /, *args: _Args.args, **kargs: _Args.kwargs) -&gt; _RT:\n--&gt; 197     return f(self, *args, **kargs)\n\nFile ~/miniconda3/envs/feature-store/lib/python3.11/site-packages/snowflake/ml/feature_store/feature_store.py:1683, in FeatureStore.generate_dataset(self, name, spine_df, features, version, spine_timestamp_col, spine_label_cols, exclude_columns, include_feature_view_timestamp_col, desc, output_type)\n   1682 except SnowparkSQLException as e:\n-&gt; 1683     raise snowml_exceptions.SnowflakeMLException(\n   1684         error_code=error_codes.INTERNAL_SNOWPARK_ERROR,\n   1685         original_exception=RuntimeError(f\"An error occurred during dataset generation: {e}.\"),\n   1686     ) from e\n\nSnowflakeMLException: RuntimeError(\"(1300) An error occurred during dataset generation: (1304): 01bab4ea-0004-a5f9-004d-de07052ae67a: 002028 (42601): SQL compilation error:\\nambiguous column name 'ORDER_ID'.\")\n\nThe above exception was the direct cause of the following exception:\n\nRuntimeError                              Traceback (most recent call last)\nFile ~/github/sf-feature-store/nbs/snowflake_feature_store/manager.py:477, in FeatureStoreManager.get_features(self, spine_df, feature_views, label_cols, dataset_name, spine_timestamp_col, **kwargs)\n    475 logger.info(f\"Timestamp column: {spine_timestamp_col}\")\n--&gt; 477 dataset = self.feature_store.generate_dataset(\n    478     name=dataset_name,\n    479     spine_df=spine_df,\n    480     features=views,\n    481     spine_label_cols=label_cols,\n    482     spine_timestamp_col=spine_timestamp_col,\n    483     **kwargs\n    484 )\n    486 return dataset.read.to_snowpark_dataframe()\n\nFile ~/miniconda3/envs/feature-store/lib/python3.11/site-packages/snowflake/ml/_internal/telemetry.py:566, in send_api_usage_telemetry.&lt;locals&gt;.decorator.&lt;locals&gt;.wrap(*args, **kwargs)\n    565     else:\n--&gt; 566         raise me.original_exception from e\n    567 finally:\n\nRuntimeError: (1300) An error occurred during dataset generation: (1304): 01bab4ea-0004-a5f9-004d-de07052ae67a: 002028 (42601): SQL compilation error:\nambiguous column name 'ORDER_ID'.\n\nDuring handling of the above exception, another exception occurred:\n\nFeatureStoreException                     Traceback (most recent call last)\nCell In[57], line 2\n      1 # Generate training dataset with all features\n----&gt; 2 training_data = manager.get_features(\n      3     spine_df=spine_df,\n      4     feature_views=[user_config, product_config, user_product_config],\n      5     label_cols=[\"REORDERED\"],\n      6     spine_timestamp_col=None  # No timestamp needed for this dataset\n      7 )\n      8 training_data.show(5)\n\nFile ~/github/sf-feature-store/nbs/snowflake_feature_store/manager.py:492, in FeatureStoreManager.get_features(self, spine_df, feature_views, label_cols, dataset_name, spine_timestamp_col, **kwargs)\n    490 for cb in self.callbacks:\n    491     cb.on_error(error_msg)\n--&gt; 492 raise FeatureStoreException(error_msg)\n\nFeatureStoreException: Error generating dataset: (1300) An error occurred during dataset generation: (1304): 01bab4ea-0004-a5f9-004d-de07052ae67a: 002028 (42601): SQL compilation error:\nambiguous column name 'ORDER_ID'.\n\n\n\n\n# 1. First, let's check the column names in each feature view DataFrame\nprint(\"User feature columns:\", user_features_df.columns)\nprint(\"Product feature columns:\", product_features_df.columns)\nprint(\"User-Product feature columns:\", user_product_features_df.columns)\n\nUser feature columns: ['USER_ID', 'USER_TOTAL_ORDERS', 'AVG_DAYS_BETWEEN_ORDERS', 'TYPICAL_ORDER_HOUR', 'PREFERRED_ORDER_DAY', 'AVG_BASKET_SIZE', 'DISTINCT_PRODUCTS_COUNT', 'USER_REORDER_RATE']\nProduct feature columns: ['PRODUCT_ID', 'AISLE_ID', 'DEPARTMENT_ID', 'PRODUCT_ORDERS', 'PRODUCT_REORDERS', 'PRODUCT_REORDER_RATE', 'PRODUCT_AVG_CART_POSITION']\nUser-Product feature columns: ['USER_ID', 'PRODUCT_ID', 'ORDER_ID', 'ORDER_NUMBER', 'ORDER_DOW', 'ORDER_HOUR_OF_DAY', 'REORDERED', 'UP_ORDERS', 'UP_AVG_CART_POSITION', 'ORDERS_SINCE_LAST_PURCHASE']\n\n\n\n# Add additional basket-level features\ntraining_data = training_data.with_columns([\n    F.count(\"*\").over(F.Window.partition_by(\"ORDER_ID\")).alias(\"BASKET_SIZE\"),\n    F.sum(F.col(\"REORDERED\")).over(F.Window.partition_by(\"USER_ID\")) / \n    F.count(\"*\").over(F.Window.partition_by(\"USER_ID\")).alias(\"REORDER_RATIO\"),\n    F.count_distinct(\"AISLE_ID\").over(F.Window.partition_by(\"ORDER_ID\")).alias(\"UNIQUE_AISLES\"),\n    F.count_distinct(\"DEPARTMENT_ID\").over(F.Window.partition_by(\"ORDER_ID\")).alias(\"UNIQUE_DEPARTMENTS\"),\n    F.when(F.col(\"ORDER_HOUR_OF_DAY\") &lt; 6, F.lit(\"night\"))\n     .when(F.col(\"ORDER_HOUR_OF_DAY\") &lt; 12, F.lit(\"morning\"))\n     .when(F.col(\"ORDER_HOUR_OF_DAY\") &lt; 18, F.lit(\"midday\"))\n     .otherwise(F.lit(\"evening\")).alias(\"DAY_PART\")\n])\n\n\n\n8. Save the training dataset\n\ntraining_data.write.mode(\"overwrite\").save_as_table(\"INSTACART_FEATURES.TRAINING_DATA\")\n\n\n\n9. (Optional) Monitor Feature Drift\n\n# Create a simple monitoring function\ndef check_feature_drift(manager, feature_view_name, new_data):\n    \"\"\"Check for feature drift in new data\"\"\"\n    drift_results = manager.check_feature_drift(\n        feature_view_name=feature_view_name,\n        new_data=new_data\n    )\n    \n    if drift_results:\n        logger.warning(f\"Drift detected in {feature_view_name}:\")\n        for feature, metrics in drift_results.items():\n            logger.warning(f\"  {feature}: {metrics}\")\n    else:\n        logger.info(f\"No significant drift detected in {feature_view_name}\")\n    \n    return drift_results\n\n\n# Example usage for monitoring\n# check_feature_drift(manager, \"user_features\", new_user_data)\n\n\n\n10. Define prediction function\n\ndef predict_next_order(user_id, model_path=None):\n    \"\"\"Predict items for a user's next order\"\"\"\n    # Get user features\n    user_features = manager.get_features(\n        spine_df=conn.session.create_dataframe([[user_id]], schema=[\"USER_ID\"]),\n        feature_views=[user_config]\n    )\n    \n    # Get user-product features for this user\n    user_products = conn.session.sql(f\"\"\"\n    SELECT USER_ID, PRODUCT_ID \n    FROM INSTACART_RAW.ORDER_PRODUCTS op\n    JOIN INSTACART_RAW.ORDERS o ON op.ORDER_ID = o.ORDER_ID\n    WHERE USER_ID = {user_id}\n    \"\"\")\n    \n    # Get full features for user-products\n    user_product_features = manager.get_features(\n        spine_df=user_products,\n        feature_views=[user_config, product_config, user_product_config]\n    )\n    \n    # Load model and predict (placeholder)\n    if model_path:\n        # In a real implementation, you would load your trained model\n        # and make predictions here\n        pass\n    \n    # For demo, just return products with highest reorder rate\n    top_products = user_product_features.sort(F.col(\"PRODUCT_REORDER_RATE\").desc()).limit(10)\n    \n    return top_products",
    "crumbs": [
      "Kaggle Competition Example"
    ]
  },
  {
    "objectID": "feature_view.html",
    "href": "feature_view.html",
    "title": "Feature Views",
    "section": "",
    "text": "source\n\nFeatureStats\n\n FeatureStats (timestamp:datetime.datetime, row_count:int, null_count:int,\n               null_ratio:float, unique_count:Optional[int]=None,\n               min_value:Optional[float]=None,\n               max_value:Optional[float]=None,\n               mean_value:Optional[float]=None,\n               std_value:Optional[float]=None)\n\nStatistics for feature monitoring\n\nsource\n\n\nFeatureMonitor\n\n FeatureMonitor\n                 (feature_config:snowflake_feature_store.config.FeatureCon\n                 fig, collect_detailed_stats:bool=True)\n\nMonitor feature statistics and detect drift\n\nsource\n\n\nFeatureViewBuilder\n\n FeatureViewBuilder\n                     (config:snowflake_feature_store.config.FeatureViewCon\n                     fig,\n                     feature_df:snowflake.snowpark.dataframe.DataFrame, en\n                     tities:Union[snowflake.ml.feature_store.entity.Entity\n                     ,List[snowflake.ml.feature_store.entity.Entity]],\n                     collect_stats:bool=True)\n\nBuilder for creating feature views with monitoring\n\nsource\n\n\ncreate_feature_view\n\n create_feature_view\n                      (config:snowflake_feature_store.config.FeatureViewCo\n                      nfig,\n                      feature_df:snowflake.snowpark.dataframe.DataFrame, e\n                      ntities:Union[snowflake.ml.feature_store.entity.Enti\n                      ty,List[snowflake.ml.feature_store.entity.Entity]],\n                      collect_stats:bool=True)\n\n*Create a feature view with validation and monitoring\nArgs: config: Feature view configuration feature_df: DataFrame containing feature transformations entities: Entity or list of entities collect_stats: Whether to collect detailed statistics\nReturns: Configured FeatureView object\nExample: &gt;&gt;&gt; config = FeatureViewConfig( … name=“customer_behavior”, … domain=“RETAIL”, … features={ … “session_length”: FeatureConfig( … name=“session_length”, … description=“Session length in minutes”, … validation=FeatureValidationConfig( … null_threshold=0.1, … range_check=True, … min_value=0 … ) … ) … } … ) &gt;&gt;&gt; entity = Entity(“CUSTOMER”, [“customer_id”]) &gt;&gt;&gt; feature_view = create_feature_view(config, df, entity)*\n\nfrom snowflake_feature_store.feature_view import *\nfrom snowflake_feature_store.config import *\nfrom snowflake_feature_store.connection import get_connection\nfrom snowflake_feature_store.transforms import *\n\nfrom snowflake.ml.feature_store import Entity\nimport snowflake.snowpark.functions as F\n\n\n# Get connection\nconn = get_connection()\nsession = conn.session\n\n# Create sample data\ndata = [\n    ['C1', '2024-01-01', 30, 2],\n    ['C1', '2024-01-02', 45, 3],\n    ['C2', '2024-01-01', 15, 1],\n    ['C2', '2024-01-02', 60, 4]\n]\n\n# Create feature DataFrame\nfeature_df = session.create_dataframe(\n    data,\n    schema=['CUSTOMER_ID', 'DATE', 'AVG_SESSION_LENGTH', 'TOTAL_PURCHASES']\n)\n# Create customer entity\ncustomer_entity = Entity(\n    name=\"CUSTOMER\",\n    join_keys=[\"CUSTOMER_ID\"],\n    desc=\"Customer entity for retail domain\"\n)\n\n# Create feature configurations\nfeature_configs = {\n    \"AVG_SESSION_LENGTH\": FeatureConfig(\n        name=\"AVG_SESSION_LENGTH\",\n        description=\"Average session length in minutes\",\n        validation=FeatureValidationConfig(\n            null_threshold=0.1,\n            range_check=True,\n            min_value=0\n        )\n    ),\n    \"TOTAL_PURCHASES\": FeatureConfig(\n        name=\"TOTAL_PURCHASES\",\n        description=\"Total number of purchases\",\n        validation=FeatureValidationConfig(\n            null_threshold=0.05,\n            range_check=True,\n            min_value=0\n        )\n    )\n}\n\n# Create feature view config\nconfig = FeatureViewConfig(\n    name=\"customer_behavior\",\n    domain=\"RETAIL\",\n    entity=\"CUSTOMER\",\n    feature_type=\"BEHAVIOR\",\n    refresh=RefreshConfig(frequency=\"1 day\"),\n    features=feature_configs,\n    description=\"Customer behavior features\"\n)\n\n# Create feature view with monitoring\nfeature_view = create_feature_view(\n    config, \n    feature_df, \n    customer_entity,\n    collect_stats=True\n)\n\n# Show the feature view data\nprint(\"\\nFeature View Data:\")\nfeature_view.feature_df.show()\n\n# Access feature monitors\nbuilder = FeatureViewBuilder(config, feature_df, customer_entity)\nfor feature_name, monitor in builder.monitors.items():\n    stats = monitor.compute_stats(feature_df, feature_name)\n    print(f\"\\nStats for {feature_name}:\")\n    print(json.dumps(stats.model_dump(), indent=2))\n\n# Create transform configurations\ntransform_config = TransformConfig(\n    name=\"session_metrics\",\n    null_threshold=0.05,\n    expected_types=['DECIMAL', 'DOUBLE', 'NUMBER']  # Accept any numeric type\n)\n\n# Apply transforms to create features\ntransforms = [\n    moving_agg(\n        cols='AVG_SESSION_LENGTH',\n        window_sizes=[7],  # 7-day window\n        agg_funcs=['AVG', 'MAX'],\n        partition_by=['CUSTOMER_ID'],\n        order_by=['DATE'],\n        config=transform_config\n    )\n]\n\n# Apply transforms to create feature DataFrame\nfeature_df = apply_transforms(feature_df, transforms)\n\n2025-02-16 20:56:04,651 - snowflake_feature_store - INFO - No active session found, creating new connection from environment\n\n\n/Users/jdemlow/miniconda3/envs/feature-store/lib/python3.11/site-packages/pydantic/_internal/_fields.py:192: UserWarning: Field name \"schema\" in \"ConnectionConfig\" shadows an attribute in parent \"BaseModel\"\n  warnings.warn(\n\n\n2025-02-16 20:56:05,094 - snowflake_feature_store - INFO - Initialized connection to \"CONTAINER_DEMO_DB\".\"PUBLIC\"\n-------------------------------------------------------------------------\n|\"CUSTOMER_ID\"  |\"DATE\"      |\"AVG_SESSION_LENGTH\"  |\"TOTAL_PURCHASES\"  |\n-------------------------------------------------------------------------\n|C1             |2024-01-01  |30                    |2                  |\n|C1             |2024-01-02  |45                    |3                  |\n|C2             |2024-01-01  |15                    |1                  |\n-------------------------------------------------------------------------\n\n2025-02-16 20:56:06,339 - snowflake_feature_store - INFO - Validated feature AVG_SESSION_LENGTH (stats: {'timestamp': '2025-02-17T04:56:05.679493', 'row_count': 4, 'null_count': 0, 'null_ratio': 0.0, 'unique_count': 4, 'min_value': 15.0, 'max_value': 60.0, 'mean_value': 37.5, 'std_value': 19.364916731037084})\n2025-02-16 20:56:07,502 - snowflake_feature_store - INFO - Validated feature TOTAL_PURCHASES (stats: {'timestamp': '2025-02-17T04:56:06.580706', 'row_count': 4, 'null_count': 0, 'null_ratio': 0.0, 'unique_count': 4, 'min_value': 1.0, 'max_value': 4.0, 'mean_value': 2.5, 'std_value': 1.290994577835244})\n\nFeature View Data:\n-------------------------------------------------------------------------\n|\"CUSTOMER_ID\"  |\"DATE\"      |\"AVG_SESSION_LENGTH\"  |\"TOTAL_PURCHASES\"  |\n-------------------------------------------------------------------------\n|C1             |2024-01-01  |30                    |2                  |\n|C1             |2024-01-02  |45                    |3                  |\n|C2             |2024-01-01  |15                    |1                  |\n|C2             |2024-01-02  |60                    |4                  |\n-------------------------------------------------------------------------\n\n\nStats for AVG_SESSION_LENGTH:\n{\n  \"timestamp\": \"2025-02-17T04:56:07.827584\",\n  \"row_count\": 4,\n  \"null_count\": 0,\n  \"null_ratio\": 0.0,\n  \"unique_count\": 4,\n  \"min_value\": 15.0,\n  \"max_value\": 60.0,\n  \"mean_value\": 37.5,\n  \"std_value\": 19.364916731037084\n}\n\nStats for TOTAL_PURCHASES:\n{\n  \"timestamp\": \"2025-02-17T04:56:08.595868\",\n  \"row_count\": 4,\n  \"null_count\": 0,\n  \"null_ratio\": 0.0,\n  \"unique_count\": 4,\n  \"min_value\": 1.0,\n  \"max_value\": 4.0,\n  \"mean_value\": 2.5,\n  \"std_value\": 1.290994577835244\n}",
    "crumbs": [
      "Feature Views"
    ]
  },
  {
    "objectID": "simple_example.html",
    "href": "simple_example.html",
    "title": "End-to-End Feature Store Example",
    "section": "",
    "text": "First, we’ll create some example customer data:\n\nimport os\nfrom typing import Optional\nfrom pathlib import Path\nimport logging\nfrom datetime import datetime, timedelta\n\nimport snowflake.snowpark.functions as F\n\nfrom snowflake_feature_store.connection import get_connection\nfrom snowflake_feature_store.manager import FeatureStoreManager\nfrom snowflake_feature_store.config import (\n    FeatureViewConfig, FeatureConfig, RefreshConfig, \n    FeatureValidationConfig\n)\nfrom snowflake_feature_store.transforms import (\n    Transform, TransformConfig, moving_agg, \n    fill_na, date_diff, CustomTransform\n)\nfrom snowflake_feature_store.examples import (\n    get_example_data, create_feature_configs\n)\nfrom snowflake_feature_store.logging import logger",
    "crumbs": [
      "End-to-End Feature Store Example"
    ]
  },
  {
    "objectID": "simple_example.html#data-generation",
    "href": "simple_example.html#data-generation",
    "title": "End-to-End Feature Store Example",
    "section": "",
    "text": "First, we’ll create some example customer data:\n\nimport os\nfrom typing import Optional\nfrom pathlib import Path\nimport logging\nfrom datetime import datetime, timedelta\n\nimport snowflake.snowpark.functions as F\n\nfrom snowflake_feature_store.connection import get_connection\nfrom snowflake_feature_store.manager import FeatureStoreManager\nfrom snowflake_feature_store.config import (\n    FeatureViewConfig, FeatureConfig, RefreshConfig, \n    FeatureValidationConfig\n)\nfrom snowflake_feature_store.transforms import (\n    Transform, TransformConfig, moving_agg, \n    fill_na, date_diff, CustomTransform\n)\nfrom snowflake_feature_store.examples import (\n    get_example_data, create_feature_configs\n)\nfrom snowflake_feature_store.logging import logger",
    "crumbs": [
      "End-to-End Feature Store Example"
    ]
  },
  {
    "objectID": "simple_example.html#step-1-setting-up-the-feature-store",
    "href": "simple_example.html#step-1-setting-up-the-feature-store",
    "title": "End-to-End Feature Store Example",
    "section": "Step 1: Setting Up the Feature Store",
    "text": "Step 1: Setting Up the Feature Store\nFirst, we’ll create a permanent feature store in Snowflake. This differs from our previous examples which used temporary schemas.\n\nRequired Permissions\nTo create a permanent feature store, you need: - CREATE DATABASE if creating a new database - CREATE SCHEMA in the target database - USAGE on the warehouse - CREATE TABLE in the target schema\n\n\nWhy Permanent vs Temporary?\nPermanent feature stores offer several advantages: 1. Persistence across sessions 2. Accessibility through Snowsight UI 3. Ability to share with other users/roles 4. Integration with other Snowflake tools\n\n# Specify all connection parameters directly\ndatabase = \"DATASCIENCE\"\nschema = \"FEATURE_STORE_DEMO\"\nwarehouse = \"DS_WH_XS\"\nrole = \"DATA_SCIENTIST\"\n\n# Get connection with all custom parameters\nconn = get_connection(\n    database=database, \n    schema=schema, \n    warehouse=warehouse,\n    role=role\n)\n\n# Now you can use the connection with the proper context\nlogger.info(f\"Connected to {database}.{schema} as {role}\")\n\n2025-02-26 18:57:01,857 - snowflake_feature_store - INFO - Using active Snowflake session\n2025-02-26 18:57:01,858 - snowflake_feature_store - INFO - Initialized connection to \"DATASCIENCE\".\"FEATURE_STORE_DEMO\"\n2025-02-26 18:57:04,045 - snowflake_feature_store - INFO - Using role: \"DATA_SCIENTIST\", warehouse: \"DS_WH_XS\", database: DATASCIENCE, schema: FEATURE_STORE_DEMO\n2025-02-26 18:57:04,047 - snowflake_feature_store - INFO - Connected to DATASCIENCE.FEATURE_STORE_DEMO as DATA_SCIENTIST",
    "crumbs": [
      "End-to-End Feature Store Example"
    ]
  },
  {
    "objectID": "simple_example.html#step-2-data-generation-and-loading",
    "href": "simple_example.html#step-2-data-generation-and-loading",
    "title": "End-to-End Feature Store Example",
    "section": "Step 2: Data Generation and Loading",
    "text": "Step 2: Data Generation and Loading\nIn this section, we’ll create example customer data. In a real scenario, you’d load your own data, but this example shows: 1. Proper data typing for Snowflake 2. Handling temporal data correctly 3. Setting up data quality checks 4. Creating realistic patterns in the data\n\nWhy This Structure?\nThis structure demonstrates common ML feature engineering challenges: 1. Time-based Features: Rolling averages, time windows 2. Missing Data: Handling sparse observations 3. Multiple Metrics: Combining different data types 4. Entity Resolution: Linking data to customers\n\n\nWhy LTV Prediction?\nLTV prediction is a common ML use case that demonstrates key feature store benefits: 1. Time-based Features: Customer spending patterns over time 2. Multiple Data Sources: Combining transactions, web analytics, and customer data 3. Feature Freshness: Regular updates as new transactions occur 4. Point-in-Time Correctness: Avoiding data leakage in training\n\n\nData Structure\nOur LTV example includes: - LIFE_TIME_VALUE: Current customer value (target) - SESSION_LENGTH: Customer engagement metric - TRANSACTIONS: Number of transactions - TIME_ON_APP/WEBSITE: Engagement channels\n\n# Get start date\nnum_customers =  100\n\n# Generate data with patterns\ndf = get_example_data(\n    conn.session,\n    schema,\n    num_customers,\n)\n\n# Show data profile\nlogger.info(\"\\nData Profile:\")\nfor col in df.columns:\n    null_count = df.filter(F.col(col).is_null()).count()\n    null_pct = null_count / df.count() * 100\n    logger.info(f\"{col}: {null_pct:.1f}% null\")\n\n2025-02-26 18:57:20,976 - snowflake_feature_store - INFO - Generated 2400 rows of demo data in \"DATASCIENCE\".FEATURE_STORE_DEMO.CUSTOMER_ACTIVITY\n2025-02-26 18:57:20,979 - snowflake_feature_store - INFO - \nSample Data:\n---------------------------------------------------------------------------------------------------------------------------------\n|\"CUSTOMER_ID\"  |\"DATE\"      |\"LIFE_TIME_VALUE\"  |\"SESSION_LENGTH\"   |\"TIME_ON_APP\"       |\"TIME_ON_WEBSITE\"   |\"TRANSACTIONS\"  |\n---------------------------------------------------------------------------------------------------------------------------------\n|C26            |2025-01-27  |564.6091706355498  |7.616530664264483  |12.142466259190991  |10.138495747448552  |5               |\n|C85            |2025-02-25  |59.41299820191271  |4.727356127459224  |5.495725181799517   |3.859344227079125   |1               |\n|C72            |2025-02-25  |646.3111447043822  |8.908367451703572  |9.728204471331994   |13.191869376320893  |6               |\n|C60            |2025-02-25  |447.0367329096978  |8.704973243200099  |10.936684313415565  |11.222118890927462  |4               |\n|C48            |2025-02-25  |294.3029664683961  |NULL               |5.2542111627227275  |7.048625379945507   |2               |\n---------------------------------------------------------------------------------------------------------------------------------\n\n2025-02-26 18:57:22,023 - snowflake_feature_store - INFO - \nSchema:\n2025-02-26 18:57:22,252 - snowflake_feature_store - INFO - CUSTOMER_ID: StringType()\n2025-02-26 18:57:22,253 - snowflake_feature_store - INFO - DATE: DateType()\n2025-02-26 18:57:22,254 - snowflake_feature_store - INFO - LIFE_TIME_VALUE: DoubleType()\n2025-02-26 18:57:22,255 - snowflake_feature_store - INFO - SESSION_LENGTH: DoubleType()\n2025-02-26 18:57:22,255 - snowflake_feature_store - INFO - TIME_ON_APP: DoubleType()\n2025-02-26 18:57:22,256 - snowflake_feature_store - INFO - TIME_ON_WEBSITE: DoubleType()\n2025-02-26 18:57:22,257 - snowflake_feature_store - INFO - TRANSACTIONS: LongType()\n2025-02-26 18:57:22,257 - snowflake_feature_store - INFO - \nData Profile:\n2025-02-26 18:57:23,040 - snowflake_feature_store - INFO - CUSTOMER_ID: 0.0% null\n2025-02-26 18:57:23,442 - snowflake_feature_store - INFO - DATE: 0.0% null\n2025-02-26 18:57:23,879 - snowflake_feature_store - INFO - LIFE_TIME_VALUE: 0.0% null\n2025-02-26 18:57:24,264 - snowflake_feature_store - INFO - SESSION_LENGTH: 20.5% null\n2025-02-26 18:57:24,797 - snowflake_feature_store - INFO - TIME_ON_APP: 0.0% null\n2025-02-26 18:57:25,175 - snowflake_feature_store - INFO - TIME_ON_WEBSITE: 0.0% null\n2025-02-26 18:57:25,571 - snowflake_feature_store - INFO - TRANSACTIONS: 0.0% null",
    "crumbs": [
      "End-to-End Feature Store Example"
    ]
  },
  {
    "objectID": "simple_example.html#step-3-entity-creation",
    "href": "simple_example.html#step-3-entity-creation",
    "title": "End-to-End Feature Store Example",
    "section": "Step 3: Entity Creation",
    "text": "Step 3: Entity Creation\nEntities are the foundation of your feature store. They represent the objects you’re collecting features about (e.g., customers, products, transactions).\n\nWhy Entities Matter\nProper entity design is crucial because: 1. Entities determine how features can be joined 2. Entities define the granularity of your features 3. Entities enable point-in-time correct feature retrieval 4. Entities help organize and discover features\n\n\nEntity Best Practices\n\nUnique Keys: Choose stable, unique identifiers\nGranularity: Pick the right level (e.g., customer vs. session)\nDocumentation: Clearly describe what the entity represents\nConsistency: Use the same keys across feature views\n\n\n\nEntity Design for LTV\nFor LTV prediction, we need a customer entity that: 1. Has a stable identifier 2. Links to all customer interactions 3. Supports time-based feature aggregation\nKey considerations for our customer entity: 1. Identifier: Use CUSTOMER_ID as stable key 2. Temporal Aspect: Track customer since first transaction 3. Granularity: Customer-level for LTV prediction 4. Documentation: Clear description for feature discovery\n\n# Create detailed documentation\ndescription = \"\"\"\nCustomer Entity for LTV Prediction\n\nThis entity represents individual customers and their behavior over time.\nIt serves as the primary entity for customer lifetime value prediction.\n\nKey Information:\n- Primary Key: CUSTOMER_ID (stable identifier)\n- Temporal Key: DATE (for point-in-time correct features)\n- Granularity: One record per customer per day\n\nUsage:\n1. Base entity for customer-level features\n2. Join key for transaction and session data\n3. Temporal alignment for time-based features\n\nBest Practices:\n- Always join using CUSTOMER_ID\n- Use DATE for point-in-time correctness\n- Aggregate features to customer-day level\n\nExample:\n```sql\nSELECT CUSTOMER_ID, DATE, COUNT(*) as daily_transactions\nFROM transactions\nGROUP BY CUSTOMER_ID, DATE\n```\n\"\"\".strip()\n\nimport tempfile\nfrom pathlib import Path\n\nmetrics_dir = Path(tempfile.mkdtemp()) / \"feature_store_metrics\"\n\nmanager = FeatureStoreManager(\n    connection=conn,\n    metrics_path=metrics_dir,\n    overwrite=True\n)\n\n# Create entity\nmanager.add_entity(\n    name=\"CUSTOMER\",\n    join_keys=[\"CUSTOMER_ID\"],\n    description=description,\n)\n\nlogger.info(\"Created CUSTOMER entity for LTV prediction\")\n# Verify entity creation\nentity = manager.feature_store.get_entity(\"CUSTOMER\")\nlogger.info(\"\\nEntity Details:\")\nlogger.info(f\"Name: {entity.name}\")\nlogger.info(f\"Join Keys: {entity.join_keys}\")\n\n2025-02-26 18:57:35,615 - snowflake_feature_store - INFO - FeatureStoreManager initialized\n2025-02-26 18:57:37,238 - snowflake_feature_store - INFO - Created entity: CUSTOMER with keys: ['CUSTOMER_ID']\n2025-02-26 18:57:37,240 - snowflake_feature_store - INFO - Created CUSTOMER entity for LTV prediction\n2025-02-26 18:57:38,054 - snowflake_feature_store - INFO - \nEntity Details:\n2025-02-26 18:57:38,055 - snowflake_feature_store - INFO - Name: CUSTOMER\n2025-02-26 18:57:38,056 - snowflake_feature_store - INFO - Join Keys: ['CUSTOMER_ID']\n\n\n\n\nKey Points About This Implementation\n\nDocumentation\n\nClear description of entity purpose\nUsage examples included\nBest practices documented\nSQL example provided\n\nValidation\n\nEntity creation is verified\nJoin keys are explicitly defined\nLogging provides creation confirmation\nError handling is included\n\nLTV Specific\n\nDesigned for customer-level predictions\nSupports temporal feature creation\nEnables point-in-time correct joins\nFacilitates customer behavior tracking",
    "crumbs": [
      "End-to-End Feature Store Example"
    ]
  },
  {
    "objectID": "simple_example.html#step-4-feature-configuration",
    "href": "simple_example.html#step-4-feature-configuration",
    "title": "End-to-End Feature Store Example",
    "section": "Step 4: Feature Configuration",
    "text": "Step 4: Feature Configuration\nFeature configuration is where we define what features we want to create and how they should behave.\n\nFeature Configuration Concepts\nA feature configuration defines: 1. Validation Rules: Data quality checks and thresholds 2. Dependencies: What other features this feature needs 3. Metadata: Description, tags, and ownership 4. Refresh Settings: How often to update the feature\n\n\nWhy Configuration Matters\nGood feature configuration ensures: 1. Data quality is maintained 2. Features are well-documented 3. Dependencies are tracked 4. Feature freshness is appropriate\n\n\nLTV Feature Configuration\nFor LTV prediction, we need several types of features: 1. Behavioral Features - Session metrics - Engagement patterns - Transaction history\n\nTemporal Features\n\nTime since first purchase\nWeekly/monthly patterns\nRolling aggregations\n\nDerived Features\n\nAverage transaction value\nEngagement ratios\nTime-based metrics\n\n\n\n# Base features (from source data)\nfeature_configs = {\n    \"LIFE_TIME_VALUE\": FeatureConfig(\n        name=\"LIFE_TIME_VALUE\",\n        description=\"Current customer lifetime value\",\n        validation=FeatureValidationConfig(\n            null_threshold=0.0,\n            range_check=True,\n            min_value=0\n        )\n    ),\n    \"SESSION_LENGTH\": FeatureConfig(\n        name=\"SESSION_LENGTH\",\n        description=\"Session length in minutes\",\n        validation=FeatureValidationConfig(\n            null_threshold=0.2,\n            range_check=True,\n            min_value=0\n        )\n    ),\n    \"TRANSACTIONS\": FeatureConfig(\n        name=\"TRANSACTIONS\",\n        description=\"Number of transactions\",\n        validation=FeatureValidationConfig(\n            null_threshold=0.0,\n            range_check=True,\n            min_value=0\n        )\n    )\n}\n\n# Time window features (match moving_agg output names)\nfor window in [7, 30]:\n    for metric in ['TRANSACTIONS', 'LIFE_TIME_VALUE']:\n        for agg in ['SUM', 'AVG']:\n            feature_name = f\"{agg}_{metric}_{window}\"\n            feature_configs[feature_name] = FeatureConfig(\n                name=feature_name,\n                description=f\"{agg.lower()} of {metric.lower()} over {window} days\",\n                validation=FeatureValidationConfig(\n                    null_threshold=0.1,\n                    range_check=True,\n                    min_value=0\n                ),\n                dependencies=[metric]\n            )\n\n# Derived features\nfeature_configs.update({\n    \"ENGAGEMENT_SCORE\": FeatureConfig(\n        name=\"ENGAGEMENT_SCORE\",\n        description=\"Combined engagement metric\",\n        validation=FeatureValidationConfig(\n            null_threshold=0.1,\n            range_check=True,\n            min_value=0\n        ),\n        dependencies=[\"SESSION_LENGTH\", \"TIME_ON_APP\", \"TIME_ON_WEBSITE\"]\n    ),\n    \"AVG_TRANSACTION_VALUE\": FeatureConfig(\n        name=\"AVG_TRANSACTION_VALUE\",\n        description=\"Average value per transaction\",\n        validation=FeatureValidationConfig(\n            null_threshold=0.1,\n            range_check=True,\n            min_value=0\n        ),\n        dependencies=[\"LIFE_TIME_VALUE\", \"TRANSACTIONS\"]\n    )\n})\n\nfeature_configs\n\n{'LIFE_TIME_VALUE': FeatureConfig(name='LIFE_TIME_VALUE', description='Current customer lifetime value', validation=FeatureValidationConfig(null_check=True, null_threshold=0.0, range_check=True, min_value=0.0, max_value=None, unique_check=False, unique_threshold=0.9), dependencies=[]),\n 'SESSION_LENGTH': FeatureConfig(name='SESSION_LENGTH', description='Session length in minutes', validation=FeatureValidationConfig(null_check=True, null_threshold=0.2, range_check=True, min_value=0.0, max_value=None, unique_check=False, unique_threshold=0.9), dependencies=[]),\n 'TRANSACTIONS': FeatureConfig(name='TRANSACTIONS', description='Number of transactions', validation=FeatureValidationConfig(null_check=True, null_threshold=0.0, range_check=True, min_value=0.0, max_value=None, unique_check=False, unique_threshold=0.9), dependencies=[]),\n 'SUM_TRANSACTIONS_7': FeatureConfig(name='SUM_TRANSACTIONS_7', description='sum of transactions over 7 days', validation=FeatureValidationConfig(null_check=True, null_threshold=0.1, range_check=True, min_value=0.0, max_value=None, unique_check=False, unique_threshold=0.9), dependencies=['TRANSACTIONS']),\n 'AVG_TRANSACTIONS_7': FeatureConfig(name='AVG_TRANSACTIONS_7', description='avg of transactions over 7 days', validation=FeatureValidationConfig(null_check=True, null_threshold=0.1, range_check=True, min_value=0.0, max_value=None, unique_check=False, unique_threshold=0.9), dependencies=['TRANSACTIONS']),\n 'SUM_LIFE_TIME_VALUE_7': FeatureConfig(name='SUM_LIFE_TIME_VALUE_7', description='sum of life_time_value over 7 days', validation=FeatureValidationConfig(null_check=True, null_threshold=0.1, range_check=True, min_value=0.0, max_value=None, unique_check=False, unique_threshold=0.9), dependencies=['LIFE_TIME_VALUE']),\n 'AVG_LIFE_TIME_VALUE_7': FeatureConfig(name='AVG_LIFE_TIME_VALUE_7', description='avg of life_time_value over 7 days', validation=FeatureValidationConfig(null_check=True, null_threshold=0.1, range_check=True, min_value=0.0, max_value=None, unique_check=False, unique_threshold=0.9), dependencies=['LIFE_TIME_VALUE']),\n 'SUM_TRANSACTIONS_30': FeatureConfig(name='SUM_TRANSACTIONS_30', description='sum of transactions over 30 days', validation=FeatureValidationConfig(null_check=True, null_threshold=0.1, range_check=True, min_value=0.0, max_value=None, unique_check=False, unique_threshold=0.9), dependencies=['TRANSACTIONS']),\n 'AVG_TRANSACTIONS_30': FeatureConfig(name='AVG_TRANSACTIONS_30', description='avg of transactions over 30 days', validation=FeatureValidationConfig(null_check=True, null_threshold=0.1, range_check=True, min_value=0.0, max_value=None, unique_check=False, unique_threshold=0.9), dependencies=['TRANSACTIONS']),\n 'SUM_LIFE_TIME_VALUE_30': FeatureConfig(name='SUM_LIFE_TIME_VALUE_30', description='sum of life_time_value over 30 days', validation=FeatureValidationConfig(null_check=True, null_threshold=0.1, range_check=True, min_value=0.0, max_value=None, unique_check=False, unique_threshold=0.9), dependencies=['LIFE_TIME_VALUE']),\n 'AVG_LIFE_TIME_VALUE_30': FeatureConfig(name='AVG_LIFE_TIME_VALUE_30', description='avg of life_time_value over 30 days', validation=FeatureValidationConfig(null_check=True, null_threshold=0.1, range_check=True, min_value=0.0, max_value=None, unique_check=False, unique_threshold=0.9), dependencies=['LIFE_TIME_VALUE']),\n 'ENGAGEMENT_SCORE': FeatureConfig(name='ENGAGEMENT_SCORE', description='Combined engagement metric', validation=FeatureValidationConfig(null_check=True, null_threshold=0.1, range_check=True, min_value=0.0, max_value=None, unique_check=False, unique_threshold=0.9), dependencies=['SESSION_LENGTH', 'TIME_ON_APP', 'TIME_ON_WEBSITE']),\n 'AVG_TRANSACTION_VALUE': FeatureConfig(name='AVG_TRANSACTION_VALUE', description='Average value per transaction', validation=FeatureValidationConfig(null_check=True, null_threshold=0.1, range_check=True, min_value=0.0, max_value=None, unique_check=False, unique_threshold=0.9), dependencies=['LIFE_TIME_VALUE', 'TRANSACTIONS'])}",
    "crumbs": [
      "End-to-End Feature Store Example"
    ]
  },
  {
    "objectID": "simple_example.html#step-5-feature-transformations",
    "href": "simple_example.html#step-5-feature-transformations",
    "title": "End-to-End Feature Store Example",
    "section": "Step 5: Feature Transformations",
    "text": "Step 5: Feature Transformations\nFeature transformations convert raw data into ML-ready features. This is a critical step in the ML pipeline.\n\nWhy Transformations Matter\nTransformations serve multiple purposes: 1. Data Quality: Handle missing values and outliers 2. Feature Engineering: Create more predictive features 3. ML Requirements: Format data for model consumption 4. Business Logic: Encode domain knowledge\n\n\nTypes of Transformations\n\nBasic Transformations\n\nMissing value imputation\nType conversion\nScaling/normalization\n\nTime-Based Transformations\n\nRolling windows\nTime since event\nSeasonal patterns\n\nBusiness Transformations\n\nDerived metrics\nDomain-specific calculations\nFeature combinations\n\n\n\n\nLTV-Specific Transformations\nFor LTV prediction, we need several key transformations:\n\nfrom snowflake.snowpark import DataFrame\nfrom typing import Callable\n\nfrom snowflake_feature_store.transforms import ValidationMixin\n\nclass CustomTransform(ValidationMixin):\n    \"\"\"Wrapper for custom transformations\"\"\"\n    def __init__(\n        self,\n        transform_func: Callable[[DataFrame], DataFrame],\n        config: TransformConfig\n    ):\n        self._transform = transform_func\n        self._config = config\n        \n    @property\n    def config(self) -&gt; TransformConfig:\n        return self._config\n        \n    def __call__(self, df: DataFrame) -&gt; DataFrame:\n        return self._transform(df)\n\n\n# Default config if none provided\ntransform_config = TransformConfig(\n    name=\"ltv_transforms\",\n    null_threshold=0.1,\n    expected_types=['DECIMAL', 'DOUBLE', 'NUMBER']\n)\n\ntransforms = [\n    # 1. Handle Missing Values\n    fill_na(\n        ['SESSION_LENGTH', 'TIME_ON_APP', 'TIME_ON_WEBSITE'],\n        fill_value=0,\n        config=TransformConfig(\n            name=\"engagement_imputation\",\n            description=\"Fill missing engagement metrics with 0\"\n        )\n    ),\n    \n    # 2. Time-Based Features\n    moving_agg(\n        cols=['TRANSACTIONS', 'LIFE_TIME_VALUE'],\n        window_sizes=[7, 30],  # 7 and 30 day windows\n        agg_funcs=['SUM', 'AVG'],\n        partition_by=['CUSTOMER_ID'],\n        order_by=['DATE'],\n        config=TransformConfig(\n            name=\"time_windows\",\n            description=\"Rolling window aggregations\"\n        )\n    ),\n    # 3. Engagement Metrics\n    CustomTransform(\n        transform_func=lambda df: df.with_column(\n            'ENGAGEMENT_SCORE',\n            (F.col('SESSION_LENGTH') + \n                F.col('TIME_ON_APP') + \n                F.col('TIME_ON_WEBSITE')) / 3.0\n        ),\n        config=TransformConfig(\n            name=\"engagement_score\",\n            description=\"Combined engagement metric\",\n            expected_types=['DOUBLE']\n        )\n    ),\n    \n    # 4. Transaction Metrics\n    CustomTransform(\n        transform_func=lambda df: df.with_column(\n            'AVG_TRANSACTION_VALUE',\n            F.col('LIFE_TIME_VALUE') / \n            F.when(F.col('TRANSACTIONS') &gt; 0, F.col('TRANSACTIONS'))\n            .otherwise(1)\n        ),\n        config=TransformConfig(\n            name=\"avg_transaction_value\",\n            description=\"Average value per transaction\",\n            expected_types=['DOUBLE']\n        )\n    )\n]\n\n\n\nBest Practices for Transformations\n\nValidation\n\nCheck input data types\nValidate output ranges\nMonitor null ratios\n\nPerformance\n\nUse vectorized operations\nMinimize data movement\nLeverage Snowflake optimizations\n\nDocumentation\n\nDocument business logic\nExplain transformation choices\nTrack dependencies\n\n\n\n\nExample Usage\nLet’s apply our transformations and examine the results:\n\nfrom snowflake_feature_store.transforms import apply_transforms\n\n\n# Apply transforms\ntransformed_df = apply_transforms(df, transforms)\nprint(transformed_df.show(2))\n\nshow = False\nif show:\n    # Show new features\n    print(\"\\nNew Features Created:\")\n    new_cols = set(transformed_df.columns) - set(df.columns)\n    for col in sorted(new_cols):\n        print(f\"\\n{col}:\")\n        transformed_df.select([\n            F.min(col).alias('min'),\n            F.max(col).alias('max'),\n            F.avg(col).alias('mean')\n        ]).show()\n\n----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n|\"CUSTOMER_ID\"  |\"DATE\"      |\"LIFE_TIME_VALUE\"   |\"SESSION_LENGTH\"  |\"TIME_ON_APP\"       |\"TIME_ON_WEBSITE\"   |\"TRANSACTIONS\"  |\"SUM_TRANSACTIONS_7\"  |\"AVG_TRANSACTIONS_7\"  |\"SUM_TRANSACTIONS_30\"  |\"AVG_TRANSACTIONS_30\"  |\"SUM_LIFE_TIME_VALUE_7\"  |\"AVG_LIFE_TIME_VALUE_7\"  |\"SUM_LIFE_TIME_VALUE_30\"  |\"AVG_LIFE_TIME_VALUE_30\"  |\"ENGAGEMENT_SCORE\"  |\"AVG_TRANSACTION_VALUE\"  |\n----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n|C17            |2025-01-27  |358.75645239371033  |0.0               |5.8309034937188144  |9.487293041019019   |3               |3.0                   |3.0                   |3.0                    |3.0                    |358.75645239371033       |358.75645239371033       |358.75645239371033        |358.75645239371033        |5.106065511579278   |119.58548413123678       |\n|C17            |2025-01-28  |518.2970633604735   |0.0               |11.55870770225566   |10.304817307763335  |5               |8.0                   |4.0                   |8.0                    |4.0                    |877.0535157541839        |438.52675787709194       |877.0535157541839         |438.52675787709194        |7.287841670006332   |103.6594126720947        |\n----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\nNone",
    "crumbs": [
      "End-to-End Feature Store Example"
    ]
  },
  {
    "objectID": "simple_example.html#step-6-feature-view-creation",
    "href": "simple_example.html#step-6-feature-view-creation",
    "title": "End-to-End Feature Store Example",
    "section": "Step 6: Feature View Creation",
    "text": "Step 6: Feature View Creation\nFeature views combine configurations, transformations, and source data into production-ready features.\n\nWhat is a Feature View?\nA feature view is: 1. Source Data: Raw data input 2. Transformations: Feature engineering logic 3. Configurations: Validation and refresh rules 4. Metadata: Documentation and lineage\n\n\nWhy Feature Views Matter\nFeature views provide: 1. Reproducibility: Consistent feature computation 2. Monitoring: Track feature health 3. Discovery: Make features findable 4. Governance: Control access and updates\n\n\nLTV Feature View Design\nFor LTV prediction, our feature view needs to: 1. Combine engagement and transaction data 2. Apply time-based transformations 3. Maintain point-in-time correctness 4. Enable regular refreshes\n\n# 1. Create feature view config\nentity_name = \"CUSTOMER\"\nfeature_view_name = \"customer_ltv_features\"\n\n# 2. Create feature view config\nconfig = FeatureViewConfig(\n    name=feature_view_name,\n    domain=\"RETAIL\",\n    entity=entity_name,\n    feature_type=\"BEHAVIOR\",\n    refresh=RefreshConfig(\n        frequency=\"1 day\",\n        mode=\"INCREMENTAL\"\n    ),\n    features=feature_configs,  #  Created Above Pass the dictionary of FeatureConfigs\n    description=\"\"\"\n    Customer LTV prediction features combining:\n    - Transaction history\n    - Engagement metrics\n    - Time-based patterns\n    \n    Updated daily with incremental processing.\n    Use for LTV prediction and customer segmentation.\n    \"\"\".strip(),\n    timestamp_col=\"DATE\"\n)\n\n# 3. Create transformations\n# Created above\n\n# 4. Create and register feature view\nfeature_view = manager.add_feature_view(\n    config=config,\n    df=df, # Original DataFrame above\n    entity_name=entity_name,\n    transforms=transforms,\n    collect_stats=True  # Enable monitoring\n)\n\n# 5. Log feature view details\nlogger.info(f\"\\nCreated feature view: {feature_view_name}\")\nlogger.info(f\"Features created: {len(feature_configs)}\")\nlogger.info(f\"Transformations applied: {len(transforms)}\")\n\n# 6. Show feature statistics\nlogger.info(\"\\nFeature Statistics:\")\nfor feature_name, stats in manager.feature_stats[config.name].items():\n    logger.info(f\"\\n{feature_name}:\")\n    logger.info(str(stats))\n\n2025-02-26 18:58:47,395 - snowflake_feature_store - INFO - Validated feature LIFE_TIME_VALUE (stats: {'timestamp': '2025-02-27T02:58:46.549388', 'row_count': 2400, 'null_count': 0, 'null_ratio': 0.0, 'unique_count': 2400, 'min_value': 2.239592053174099, 'max_value': 749.1277222421559, 'mean_value': 371.0923268443173, 'std_value': 217.24439799534923})\n2025-02-26 18:58:49,708 - snowflake_feature_store - INFO - Validated feature SESSION_LENGTH (stats: {'timestamp': '2025-02-27T02:58:47.764298', 'row_count': 2400, 'null_count': 0, 'null_ratio': 0.0, 'unique_count': 1908, 'min_value': 0.0, 'max_value': 12.219298377916665, 'mean_value': 4.924390829623488, 'std_value': 3.4026523269547897})\n2025-02-26 18:58:51,019 - snowflake_feature_store - INFO - Validated feature TRANSACTIONS (stats: {'timestamp': '2025-02-27T02:58:50.110091', 'row_count': 2400, 'null_count': 0, 'null_ratio': 0.0, 'unique_count': 7, 'min_value': 1.0, 'max_value': 7.0, 'mean_value': 3.366667, 'std_value': 2.0001389951700856})\n2025-02-26 18:58:52,525 - snowflake_feature_store - INFO - Validated feature SUM_TRANSACTIONS_7 (stats: {'timestamp': '2025-02-27T02:58:51.405251', 'row_count': 2400, 'null_count': 0, 'null_ratio': 0.0, 'unique_count': 39, 'min_value': 1.0, 'max_value': 39.0, 'mean_value': 20.647083333333335, 'std_value': 7.7325036694767615})\n2025-02-26 18:58:54,016 - snowflake_feature_store - INFO - Validated feature AVG_TRANSACTIONS_7 (stats: {'timestamp': '2025-02-27T02:58:52.964874', 'row_count': 2400, 'null_count': 0, 'null_ratio': 0.0, 'unique_count': 82, 'min_value': 1.0, 'max_value': 7.0, 'mean_value': 3.3584570833333336, 'std_value': 0.9081127243491726})\n2025-02-26 18:58:55,361 - snowflake_feature_store - INFO - Validated feature SUM_LIFE_TIME_VALUE_7 (stats: {'timestamp': '2025-02-27T02:58:54.404302', 'row_count': 2400, 'null_count': 0, 'null_ratio': 0.0, 'unique_count': 2400, 'min_value': 13.077530619670986, 'max_value': 4236.6778134791375, 'mean_value': 2274.693079593752, 'std_value': 855.943387600666})\n2025-02-26 18:58:56,861 - snowflake_feature_store - INFO - Validated feature AVG_LIFE_TIME_VALUE_7 (stats: {'timestamp': '2025-02-27T02:58:55.772679', 'row_count': 2400, 'null_count': 0, 'null_ratio': 0.0, 'unique_count': 2400, 'min_value': 13.077530619670986, 'max_value': 746.5940585674848, 'mean_value': 370.0081979547277, 'std_value': 101.06575000463832})\n2025-02-26 18:58:58,333 - snowflake_feature_store - INFO - Validated feature SUM_TRANSACTIONS_30 (stats: {'timestamp': '2025-02-27T02:58:57.353567', 'row_count': 2400, 'null_count': 0, 'null_ratio': 0.0, 'unique_count': 106, 'min_value': 1.0, 'max_value': 112.0, 'mean_value': 42.33708333333333, 'std_value': 24.726804909385617})\n2025-02-26 18:58:59,571 - snowflake_feature_store - INFO - Validated feature AVG_TRANSACTIONS_30 (stats: {'timestamp': '2025-02-27T02:58:58.695444', 'row_count': 2400, 'null_count': 0, 'null_ratio': 0.0, 'unique_count': 409, 'min_value': 1.0, 'max_value': 7.0, 'mean_value': 3.3446108333333333, 'std_value': 0.7701141182408852})\n2025-02-26 18:59:00,875 - snowflake_feature_store - INFO - Validated feature SUM_LIFE_TIME_VALUE_30 (stats: {'timestamp': '2025-02-27T02:58:59.948270', 'row_count': 2400, 'null_count': 0, 'null_ratio': 0.0, 'unique_count': 2400, 'min_value': 13.077530619670986, 'max_value': 12223.462666733421, 'mean_value': 4667.152806923684, 'std_value': 2730.693558396509})\n2025-02-26 18:59:02,881 - snowflake_feature_store - INFO - Validated feature AVG_LIFE_TIME_VALUE_30 (stats: {'timestamp': '2025-02-27T02:59:01.831308', 'row_count': 2400, 'null_count': 0, 'null_ratio': 0.0, 'unique_count': 2400, 'min_value': 13.077530619670986, 'max_value': 746.5940585674848, 'mean_value': 368.6911018427686, 'std_value': 85.8407784959337})\n2025-02-26 18:59:04,494 - snowflake_feature_store - INFO - Validated feature ENGAGEMENT_SCORE (stats: {'timestamp': '2025-02-27T02:59:03.545975', 'row_count': 2400, 'null_count': 0, 'null_ratio': 0.0, 'unique_count': 2400, 'min_value': 1.8904928435296238, 'max_value': 13.00033587035531, 'mean_value': 7.115458021279218, 'std_value': 2.360729920090712})\n2025-02-26 18:59:06,030 - snowflake_feature_store - INFO - Validated feature AVG_TRANSACTION_VALUE (stats: {'timestamp': '2025-02-27T02:59:04.850674', 'row_count': 2400, 'null_count': 0, 'null_ratio': 0.0, 'unique_count': 2400, 'min_value': 2.239592053174099, 'max_value': 199.91701587497403, 'mean_value': 109.86392365044138, 'std_value': 31.398625475995402})\n2025-02-26 18:59:32,339 - snowflake_feature_store - INFO - Created feature view: customer_ltv_features with 17 features\n2025-02-26 18:59:32,352 - snowflake_feature_store - INFO - \nCreated feature view: customer_ltv_features\n2025-02-26 18:59:32,353 - snowflake_feature_store - INFO - Features created: 13\n2025-02-26 18:59:32,354 - snowflake_feature_store - INFO - Transformations applied: 4\n2025-02-26 18:59:32,354 - snowflake_feature_store - INFO - \nFeature Statistics:\n2025-02-26 18:59:32,355 - snowflake_feature_store - INFO - \nLIFE_TIME_VALUE:\n2025-02-26 18:59:32,356 - snowflake_feature_store - INFO - Timestamp: 2025-02-27T02:59:11.568626\nRow count: 2400\nNull count: 0 (0.0%)\nUnique values: 2400\nMin value: 2.24\nMax value: 749.13\nMean value: 371.09\nStd dev: 217.24\n2025-02-26 18:59:32,357 - snowflake_feature_store - INFO - \nSESSION_LENGTH:\n2025-02-26 18:59:32,357 - snowflake_feature_store - INFO - Timestamp: 2025-02-27T02:59:12.977411\nRow count: 2400\nNull count: 0 (0.0%)\nUnique values: 1908\nMin value: 0.00\nMax value: 12.22\nMean value: 4.92\nStd dev: 3.40\n2025-02-26 18:59:32,357 - snowflake_feature_store - INFO - \nTRANSACTIONS:\n2025-02-26 18:59:32,358 - snowflake_feature_store - INFO - Timestamp: 2025-02-27T02:59:14.503363\nRow count: 2400\nNull count: 0 (0.0%)\nUnique values: 7\nMin value: 1.00\nMax value: 7.00\nMean value: 3.37\nStd dev: 2.00\n2025-02-26 18:59:32,358 - snowflake_feature_store - INFO - \nSUM_TRANSACTIONS_7:\n2025-02-26 18:59:32,359 - snowflake_feature_store - INFO - Timestamp: 2025-02-27T02:59:16.021427\nRow count: 2400\nNull count: 0 (0.0%)\nUnique values: 39\nMin value: 1.00\nMax value: 39.00\nMean value: 20.65\nStd dev: 7.73\n2025-02-26 18:59:32,359 - snowflake_feature_store - INFO - \nAVG_TRANSACTIONS_7:\n2025-02-26 18:59:32,360 - snowflake_feature_store - INFO - Timestamp: 2025-02-27T02:59:17.477578\nRow count: 2400\nNull count: 0 (0.0%)\nUnique values: 82\nMin value: 1.00\nMax value: 7.00\nMean value: 3.36\nStd dev: 0.91\n2025-02-26 18:59:32,360 - snowflake_feature_store - INFO - \nSUM_LIFE_TIME_VALUE_7:\n2025-02-26 18:59:32,360 - snowflake_feature_store - INFO - Timestamp: 2025-02-27T02:59:19.190279\nRow count: 2400\nNull count: 0 (0.0%)\nUnique values: 2400\nMin value: 13.08\nMax value: 4236.68\nMean value: 2274.69\nStd dev: 855.94\n2025-02-26 18:59:32,361 - snowflake_feature_store - INFO - \nAVG_LIFE_TIME_VALUE_7:\n2025-02-26 18:59:32,361 - snowflake_feature_store - INFO - Timestamp: 2025-02-27T02:59:20.605574\nRow count: 2400\nNull count: 0 (0.0%)\nUnique values: 2400\nMin value: 13.08\nMax value: 746.59\nMean value: 370.01\nStd dev: 101.07\n2025-02-26 18:59:32,361 - snowflake_feature_store - INFO - \nSUM_TRANSACTIONS_30:\n2025-02-26 18:59:32,362 - snowflake_feature_store - INFO - Timestamp: 2025-02-27T02:59:22.053484\nRow count: 2400\nNull count: 0 (0.0%)\nUnique values: 106\nMin value: 1.00\nMax value: 112.00\nMean value: 42.34\nStd dev: 24.73\n2025-02-26 18:59:32,362 - snowflake_feature_store - INFO - \nAVG_TRANSACTIONS_30:\n2025-02-26 18:59:32,362 - snowflake_feature_store - INFO - Timestamp: 2025-02-27T02:59:23.531577\nRow count: 2400\nNull count: 0 (0.0%)\nUnique values: 409\nMin value: 1.00\nMax value: 7.00\nMean value: 3.34\nStd dev: 0.77\n2025-02-26 18:59:32,362 - snowflake_feature_store - INFO - \nSUM_LIFE_TIME_VALUE_30:\n2025-02-26 18:59:32,363 - snowflake_feature_store - INFO - Timestamp: 2025-02-27T02:59:25.307538\nRow count: 2400\nNull count: 0 (0.0%)\nUnique values: 2400\nMin value: 13.08\nMax value: 12223.46\nMean value: 4667.15\nStd dev: 2730.69\n2025-02-26 18:59:32,363 - snowflake_feature_store - INFO - \nAVG_LIFE_TIME_VALUE_30:\n2025-02-26 18:59:32,363 - snowflake_feature_store - INFO - Timestamp: 2025-02-27T02:59:27.239169\nRow count: 2400\nNull count: 0 (0.0%)\nUnique values: 2400\nMin value: 13.08\nMax value: 746.59\nMean value: 368.69\nStd dev: 85.84\n2025-02-26 18:59:32,364 - snowflake_feature_store - INFO - \nENGAGEMENT_SCORE:\n2025-02-26 18:59:32,364 - snowflake_feature_store - INFO - Timestamp: 2025-02-27T02:59:29.019741\nRow count: 2400\nNull count: 0 (0.0%)\nUnique values: 2400\nMin value: 1.89\nMax value: 13.00\nMean value: 7.12\nStd dev: 2.36\n2025-02-26 18:59:32,364 - snowflake_feature_store - INFO - \nAVG_TRANSACTION_VALUE:\n2025-02-26 18:59:32,364 - snowflake_feature_store - INFO - Timestamp: 2025-02-27T02:59:31.280304\nRow count: 2400\nNull count: 0 (0.0%)\nUnique values: 2400\nMin value: 2.24\nMax value: 199.92\nMean value: 109.86\nStd dev: 31.40\n\n\n\n\nBest Practices for Feature Views\n\nDocumentation\n\nClear descriptions\nUsage examples\nUpdate frequency\nDependencies\n\nMonitoring\n\nFeature statistics\nData quality metrics\nRefresh status\nDrift detection\n\nPerformance\n\nIncremental updates\nEfficient transformations\nAppropriate refresh schedule\n\nGovernance\n\nAccess controls\nVersion control\nAudit logging\nData lineage",
    "crumbs": [
      "End-to-End Feature Store Example"
    ]
  },
  {
    "objectID": "simple_example.html#step-7-feature-monitoring",
    "href": "simple_example.html#step-7-feature-monitoring",
    "title": "End-to-End Feature Store Example",
    "section": "Step 7: Feature Monitoring",
    "text": "Step 7: Feature Monitoring\nMonitoring is crucial for maintaining feature quality and detecting issues early.\n\nWhy Monitor Features?\nFeature monitoring helps: 1. Detect Data Quality Issues: Missing values, outliers, type mismatches 2. Track Feature Drift: Changes in feature distributions 3. Ensure Freshness: Verify timely updates 4. Validate Business Rules: Check domain-specific constraints\n\n\nTypes of Monitoring\n\nData Quality\n\nNull ratios\nType consistency\nValue ranges\nCardinality\n\nStatistical Monitoring\n\nDistribution shifts\nCorrelation changes\nSeasonality patterns\nOutlier detection\n\nOperational Monitoring\n\nRefresh status\nComputation time\nResource usage\nError rates\n\n\n\n\nLTV-Specific Monitoring\n\nimport json\n\nfrom typing import Union, List, Callable, Optional, Protocol, Dict, Any\n\nimport decimal\nfrom typing import Any\n\n\nclass LTVMonitor:\n    \"\"\"Monitor for LTV feature quality and drift\"\"\"\n    \n    def __init__(\n        self,\n        manager: FeatureStoreManager,\n        feature_view_name: str,\n        metrics_path: Optional[str] = None\n    ):\n        self.manager = manager\n        self.feature_view_name = feature_view_name\n        self.metrics_path = metrics_path\n        self.baseline_stats = {}\n        \n    def _convert_decimal(self, obj: Any) -&gt; Any:\n        \"\"\"Convert Decimal objects to float for JSON serialization\"\"\"\n        if isinstance(obj, decimal.Decimal):\n            return float(obj)\n        return obj\n    \n    def _process_metrics(self, metrics: Dict) -&gt; Dict:\n        \"\"\"Process metrics dictionary to ensure JSON serializable values\"\"\"\n        return {\n            k: {\n                'timestamp': v['timestamp'],\n                'metrics': {\n                    mk: self._convert_decimal(mv)\n                    for mk, mv in v['metrics'].items()\n                }\n            }\n            for k, v in metrics.items()\n        }\n    \n    def compute_feature_metrics(\n        self,\n        df: DataFrame,\n        timestamp: Optional[datetime] = None\n    ) -&gt; Dict[str, Dict]:\n        \"\"\"Compute comprehensive feature metrics\"\"\"\n        metrics = {}\n        timestamp = timestamp or datetime.now()\n        \n        for col in df.columns:\n            # Skip identifier columns\n            if col in ['CUSTOMER_ID', 'DATE']:\n                continue\n                \n            # Basic stats\n            stats = df.select([\n                F.count(col).alias('count'),\n                F.count_distinct(col).alias('unique'),\n                F.sum(F.when(F.col(col).is_null(), 1).otherwise(0)).alias('nulls')\n            ]).collect()[0].asDict()\n            \n            # Convert Decimal to float\n            stats = {k: self._convert_decimal(v) for k, v in stats.items()}\n            \n            # Numeric stats for appropriate columns\n            if col in ['LIFE_TIME_VALUE', 'SESSION_LENGTH', 'TRANSACTIONS']:\n                numeric_stats = df.select([\n                    F.min(col).alias('min'),\n                    F.max(col).alias('max'),\n                    F.avg(col).alias('mean'),\n                    F.stddev(col).alias('std')\n                ]).collect()[0].asDict()\n                \n                # Convert Decimal to float\n                numeric_stats = {k: self._convert_decimal(v) for k, v in numeric_stats.items()}\n                stats.update(numeric_stats)\n            \n            metrics[col] = {\n                'timestamp': timestamp.isoformat(),\n                'metrics': stats\n            }\n            \n        return metrics\n    \n    def set_baseline(self, df: DataFrame) -&gt; None:\n        \"\"\"Set baseline statistics for drift detection\"\"\"\n        self.baseline_stats = self.compute_feature_metrics(df)\n        logger.info(\"Set baseline statistics\")\n        \n        # Save baseline if metrics path provided\n        if self.metrics_path:\n            baseline_file = Path(self.metrics_path) / \"baseline_stats.json\"\n            processed_stats = self._process_metrics(self.baseline_stats)\n            with open(baseline_file, 'w') as f:\n                json.dump(processed_stats, f, indent=2)\n    \n    def check_feature_health(\n        self,\n        df: DataFrame,\n        drift_threshold: float = 0.1\n    ) -&gt; None:\n        \"\"\"Check overall feature health\"\"\"\n        try:\n            # Compute current metrics\n            current_metrics = self.compute_feature_metrics(df)\n            \n            # Detect drift if baseline exists\n            if self.baseline_stats:\n                drift_alerts = self.detect_drift(\n                    current_metrics,\n                    drift_threshold\n                )\n                \n                if drift_alerts:\n                    logger.warning(\"\\nFeature Drift Detected:\")\n                    for feature, alerts in drift_alerts.items():\n                        logger.warning(f\"\\n{feature}:\")\n                        for alert in alerts:\n                            logger.warning(f\"- {alert}\")\n            \n            # Log current metrics\n            logger.info(\"\\nCurrent Feature Metrics:\")\n            for feature, metrics in current_metrics.items():\n                logger.info(f\"\\n{feature}:\")\n                for metric, value in metrics['metrics'].items():\n                    logger.info(f\"  {metric}: {value}\")\n                    \n            # Save metrics if path provided\n            if self.metrics_path:\n                metrics_file = Path(self.metrics_path) / f\"metrics_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n                processed_metrics = self._process_metrics(current_metrics)\n                with open(metrics_file, 'w') as f:\n                    json.dump(processed_metrics, f, indent=2)\n                    \n        except Exception as e:\n            logger.error(f\"Error checking feature health: {str(e)}\")\n            raise\n\n    def detect_drift(\n        self,\n        current_metrics: Dict,\n        drift_threshold: float = 0.1\n    ) -&gt; Dict[str, List[str]]:\n        \"\"\"Detect significant changes in feature distributions\"\"\"\n        drift_alerts = {}\n        \n        for feature, metrics in current_metrics.items():\n            if feature not in self.baseline_stats:\n                continue\n                \n            alerts = []\n            baseline = self.baseline_stats[feature]['metrics']\n            current = metrics['metrics']\n            \n            # Check for distribution changes\n            for metric in ['mean', 'std']:\n                if metric not in current or metric not in baseline:\n                    continue\n                    \n                change = abs(current[metric] - baseline[metric]) / baseline[metric]\n                if change &gt; drift_threshold:\n                    alerts.append(\n                        f\"{metric.upper()} changed by {change:.1%}\"\n                    )\n            \n            # Check for data quality changes\n            null_ratio = current['NULLS'] / current['COUNT']\n            baseline_null_ratio = baseline['NULLS'] / baseline['COUNT']\n            if abs(null_ratio - baseline_null_ratio) &gt; drift_threshold:\n                alerts.append(\n                    f\"NULL ratio changed from {baseline_null_ratio:.1%} to {null_ratio:.1%}\"\n                )\n            \n            if alerts:\n                drift_alerts[feature] = alerts\n                \n        return drift_alerts\n\n\n\nExample Usage\nLet’s set up monitoring for our LTV features:\n\n# Set up metrics directory\nmetrics_dir = Path(\"feature_metrics\")\nmetrics_dir.mkdir(exist_ok=True)\n\n\n# Set up monitoring\nmonitor = LTVMonitor(\n    manager=manager,\n    feature_view_name=feature_view.name,\n    metrics_path=str(metrics_dir)\n)\n\n\nfrom snowflake_feature_store.examples import get_example_data\n\n\n# Set baseline\nmonitor.set_baseline(feature_view.feature_df)\n# Generate some drift\ndrift_df = get_example_data(\n    conn.session,\n    schema,\n    num_customers=250,\n    ltv_multiplier=4.5,  # Increase values to simulate drift\n    table_type = 'TEST'\n)\n# Check for drift\nmonitor.check_feature_health(drift_df)\n\n2025-02-26 19:00:15,844 - snowflake_feature_store - INFO - Set baseline statistics\n2025-02-26 19:00:22,871 - snowflake_feature_store - INFO - Generated 6000 rows of demo data in \"DATASCIENCE\".FEATURE_STORE_DEMO.CUSTOMER_ACTIVITY_TEST\n2025-02-26 19:00:22,874 - snowflake_feature_store - INFO - \nSample Data:\n---------------------------------------------------------------------------------------------------------------------------------\n|\"CUSTOMER_ID\"  |\"DATE\"      |\"LIFE_TIME_VALUE\"  |\"SESSION_LENGTH\"   |\"TIME_ON_APP\"       |\"TIME_ON_WEBSITE\"   |\"TRANSACTIONS\"  |\n---------------------------------------------------------------------------------------------------------------------------------\n|C26            |2025-01-27  |564.6091706355498  |7.616530664264483  |12.142466259190991  |10.138495747448552  |5               |\n|C85            |2025-02-25  |59.41299820191271  |4.727356127459224  |5.495725181799517   |3.859344227079125   |1               |\n|C72            |2025-02-25  |646.3111447043822  |8.908367451703572  |9.728204471331994   |13.191869376320893  |6               |\n|C60            |2025-02-25  |447.0367329096978  |8.704973243200099  |10.936684313415565  |11.222118890927462  |4               |\n|C48            |2025-02-25  |294.3029664683961  |NULL               |5.2542111627227275  |7.048625379945507   |2               |\n---------------------------------------------------------------------------------------------------------------------------------\n\n2025-02-26 19:00:23,220 - snowflake_feature_store - INFO - \nSchema:\n2025-02-26 19:00:23,346 - snowflake_feature_store - INFO - CUSTOMER_ID: StringType()\n2025-02-26 19:00:23,348 - snowflake_feature_store - INFO - DATE: DateType()\n2025-02-26 19:00:23,349 - snowflake_feature_store - INFO - LIFE_TIME_VALUE: DoubleType()\n2025-02-26 19:00:23,350 - snowflake_feature_store - INFO - SESSION_LENGTH: DoubleType()\n2025-02-26 19:00:23,351 - snowflake_feature_store - INFO - TIME_ON_APP: DoubleType()\n2025-02-26 19:00:23,352 - snowflake_feature_store - INFO - TIME_ON_WEBSITE: DoubleType()\n2025-02-26 19:00:23,353 - snowflake_feature_store - INFO - TRANSACTIONS: LongType()\n2025-02-26 19:00:25,574 - snowflake_feature_store - WARNING - \nFeature Drift Detected:\n2025-02-26 19:00:25,576 - snowflake_feature_store - WARNING - \nSESSION_LENGTH:\n2025-02-26 19:00:25,577 - snowflake_feature_store - WARNING - - NULL ratio changed from 0.0% to 25.9%\n2025-02-26 19:00:25,577 - snowflake_feature_store - INFO - \nCurrent Feature Metrics:\n2025-02-26 19:00:25,578 - snowflake_feature_store - INFO - \nLIFE_TIME_VALUE:\n2025-02-26 19:00:25,579 - snowflake_feature_store - INFO -   COUNT: 2400\n2025-02-26 19:00:25,580 - snowflake_feature_store - INFO -   UNIQUE: 2400\n2025-02-26 19:00:25,580 - snowflake_feature_store - INFO -   NULLS: 0\n2025-02-26 19:00:25,581 - snowflake_feature_store - INFO -   MIN: 2.239592053174099\n2025-02-26 19:00:25,581 - snowflake_feature_store - INFO -   MAX: 749.1277222421559\n2025-02-26 19:00:25,582 - snowflake_feature_store - INFO -   MEAN: 371.0923268443173\n2025-02-26 19:00:25,583 - snowflake_feature_store - INFO -   STD: 217.24439799534923\n2025-02-26 19:00:25,583 - snowflake_feature_store - INFO - \nSESSION_LENGTH:\n2025-02-26 19:00:25,584 - snowflake_feature_store - INFO -   COUNT: 1907\n2025-02-26 19:00:25,585 - snowflake_feature_store - INFO -   UNIQUE: 1907\n2025-02-26 19:00:25,585 - snowflake_feature_store - INFO -   NULLS: 493\n2025-02-26 19:00:25,586 - snowflake_feature_store - INFO -   MIN: 0.08102027782085003\n2025-02-26 19:00:25,587 - snowflake_feature_store - INFO -   MAX: 12.219298377916665\n2025-02-26 19:00:25,587 - snowflake_feature_store - INFO -   MEAN: 6.197450441057352\n2025-02-26 19:00:25,588 - snowflake_feature_store - INFO -   STD: 2.584360647215551\n2025-02-26 19:00:25,588 - snowflake_feature_store - INFO - \nTIME_ON_APP:\n2025-02-26 19:00:25,589 - snowflake_feature_store - INFO -   COUNT: 2400\n2025-02-26 19:00:25,589 - snowflake_feature_store - INFO -   UNIQUE: 2400\n2025-02-26 19:00:25,589 - snowflake_feature_store - INFO -   NULLS: 0\n2025-02-26 19:00:25,590 - snowflake_feature_store - INFO - \nTIME_ON_WEBSITE:\n2025-02-26 19:00:25,591 - snowflake_feature_store - INFO -   COUNT: 2400\n2025-02-26 19:00:25,592 - snowflake_feature_store - INFO -   UNIQUE: 2400\n2025-02-26 19:00:25,592 - snowflake_feature_store - INFO -   NULLS: 0\n2025-02-26 19:00:25,593 - snowflake_feature_store - INFO - \nTRANSACTIONS:\n2025-02-26 19:00:25,593 - snowflake_feature_store - INFO -   COUNT: 2400\n2025-02-26 19:00:25,593 - snowflake_feature_store - INFO -   UNIQUE: 7\n2025-02-26 19:00:25,594 - snowflake_feature_store - INFO -   NULLS: 0\n2025-02-26 19:00:25,594 - snowflake_feature_store - INFO -   MIN: 1\n2025-02-26 19:00:25,595 - snowflake_feature_store - INFO -   MAX: 7\n2025-02-26 19:00:25,595 - snowflake_feature_store - INFO -   MEAN: 3.366667\n2025-02-26 19:00:25,596 - snowflake_feature_store - INFO -   STD: 2.0001389951700856",
    "crumbs": [
      "End-to-End Feature Store Example"
    ]
  },
  {
    "objectID": "simple_example.html#step-8-training-data-generation",
    "href": "simple_example.html#step-8-training-data-generation",
    "title": "End-to-End Feature Store Example",
    "section": "Step 8: Training Data Generation",
    "text": "Step 8: Training Data Generation\nGenerating training data from a feature store requires special consideration to avoid data leakage and ensure point-in-time correctness.\n\nWhy Training Data Generation Matters\nProper training data generation: 1. Prevents Data Leakage: Ensures future data doesn’t leak into training 2. Maintains Consistency: Uses same feature computations as production 3. Enables Reproducibility: Training sets can be recreated exactly 4. Supports Experimentation: Easy to create different feature combinations\n\n\nLTV Training Data Requirements\nFor LTV prediction, we need to: 1. Use historical data to predict future LTV 2. Include time-based features correctly 3. Handle missing values consistently 4. Maintain customer context\n\nimport tempfile\nfrom pathlib import Path\n\nmetrics_dir = Path(tempfile.mkdtemp()) / \"feature_store_metrics\"\n\ntraining_start_date='2025-01-01'\ntraining_end_date='2025-03-01'\nprediction_window=90  # Predict 90-day LTV\nsave_table='DATASCIENCE.FEATURE_STORE_DEMO.LTV_TRAINING_DATA'\n\nmanager = FeatureStoreManager(\n    connection=conn,\n    metrics_path=metrics_dir,\n    overwrite=True\n)\n\n# Get existing feature view\nfeature_view = manager.feature_store.get_feature_view(\n    name=\"customer_ltv_features\",\n    version=\"V1_0\"  # Use the version from earlier\n)\n\n2025-02-26 19:01:11,456 - snowflake_feature_store - INFO - FeatureStoreManager initialized\n\n\n\n# Get the fully qualified table name\ntable_name = (\n    f\"{manager.connection.database}.\"\n    f\"{manager.connection.schema}.\"\n    f\"CUSTOMER_ACTIVITY\"\n)\n\n# 1. Create spine query for point-in-time correct features\nspine_df = manager.connection.session.sql(f\"\"\"\n    WITH customer_dates AS (\n        -- Get all customer-date combinations\n    SELECT DISTINCT\n            CUSTOMER_ID,\n            DATE\n        FROM {table_name}\n        WHERE DATE BETWEEN '{training_start_date}' AND '{training_end_date}'\n    ),\n    future_ltv AS (\n        -- Calculate future LTV for each customer-date\n        SELECT \n            cd.CUSTOMER_ID,\n            cd.DATE as FEATURE_DATE,\n            MAX(f.DATE) as LABEL_DATE,\n            MAX(f.LIFE_TIME_VALUE) as FUTURE_LTV\n        FROM customer_dates cd\n        LEFT JOIN {table_name} f\n            ON cd.CUSTOMER_ID = f.CUSTOMER_ID\n            AND f.DATE BETWEEN cd.DATE \n                AND DATEADD(days, {prediction_window}, cd.DATE)\n        GROUP BY 1, 2\n)\n    -- Final spine query\n    SELECT \n        CUSTOMER_ID,\n        FEATURE_DATE as \"DATE\",\n        FUTURE_LTV as \"TARGET_LTV\",\n        LABEL_DATE as \"LABEL_DATE\"\n    FROM future_ltv\n\"\"\")\nspine_df.show(2)\n\nlogger.info(f\"Created spine with {spine_df.count()} rows\")\n\n-----------------------------------------------------------------\n|\"CUSTOMER_ID\"  |\"DATE\"      |\"TARGET_LTV\"       |\"LABEL_DATE\"  |\n-----------------------------------------------------------------\n|C26            |2025-01-27  |740.132663156064   |2025-02-25    |\n|C85            |2025-02-25  |59.41299820191271  |2025-02-25    |\n-----------------------------------------------------------------\n\n2025-02-26 19:01:24,224 - snowflake_feature_store - INFO - Created spine with 2400 rows\n\n\n\n# 2. Get features using point-in-time correct joins\ntraining_data = manager.get_features(\n    spine_df=spine_df,\n    feature_views=[feature_view],\n    spine_timestamp_col=\"DATE\",\n    label_cols=[\"TARGET_LTV\", \"LABEL_DATE\"]\n)\nlogger.info(\"\\nSample Data:\")\ntraining_data.show(2)\nlogger.info(\"\\nSchema:\")\nfor field in training_data.schema.fields:\n    logger.info(f\"{field.name}: {field.datatype}\")\n\n2025-02-26 19:01:30,389 - snowflake_feature_store - INFO - Spine DataFrame columns: ['CUSTOMER_ID', 'DATE', 'TARGET_LTV', 'LABEL_DATE']\n2025-02-26 19:01:30,390 - snowflake_feature_store - INFO - Spine DataFrame schema: StructType([StructField('CUSTOMER_ID', StringType(), nullable=True), StructField('DATE', DateType(), nullable=True), StructField('TARGET_LTV', DoubleType(), nullable=True), StructField('LABEL_DATE', DateType(), nullable=True)])\n2025-02-26 19:01:30,390 - snowflake_feature_store - INFO - Generating dataset with name: DATASET_20250227_030130_63cf45e6\n2025-02-26 19:01:30,390 - snowflake_feature_store - INFO - Label columns: ['\"TARGET_LTV\"', '\"LABEL_DATE\"']\n2025-02-26 19:01:30,390 - snowflake_feature_store - INFO - Timestamp column: \"DATE\"\n2025-02-26 19:01:34,597 - snowflake_feature_store - INFO - \nSample Data:\n---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n|\"CUSTOMER_ID\"  |\"DATE\"      |\"TARGET_LTV\"       |\"LABEL_DATE\"  |\"LIFE_TIME_VALUE\"   |\"SESSION_LENGTH\"   |\"TIME_ON_APP\"      |\"TIME_ON_WEBSITE\"   |\"TRANSACTIONS\"  |\"SUM_TRANSACTIONS_7\"  |\"AVG_TRANSACTIONS_7\"  |\"SUM_TRANSACTIONS_30\"  |\"AVG_TRANSACTIONS_30\"  |\"SUM_LIFE_TIME_VALUE_7\"  |\"AVG_LIFE_TIME_VALUE_7\"  |\"SUM_LIFE_TIME_VALUE_30\"  |\"AVG_LIFE_TIME_VALUE_30\"  |\"ENGAGEMENT_SCORE\"  |\"AVG_TRANSACTION_VALUE\"  |\n---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n|C54            |2025-01-27  |748.4986572265625  |2025-02-25    |473.930419921875    |8.07875919342041   |8.7145357131958    |11.581034660339355  |4               |4.0                   |4.0                   |4.0                    |4.0                    |473.930419921875         |473.930419921875         |473.930419921875          |473.930419921875          |9.458109855651855   |118.48260498046875       |\n|C54            |2025-01-28  |748.4986572265625  |2025-02-25    |258.25628662109375  |5.443594455718994  |6.002151012420654  |5.871957778930664   |2               |6.0                   |3.0                   |6.0                    |3.0                    |732.1867065429688        |366.0933532714844        |732.1867065429688         |366.0933532714844         |5.7725677490234375  |129.12814331054688       |\n---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\n2025-02-26 19:01:35,803 - snowflake_feature_store - INFO - \nSchema:\n2025-02-26 19:01:35,805 - snowflake_feature_store - INFO - CUSTOMER_ID: StringType()\n2025-02-26 19:01:35,805 - snowflake_feature_store - INFO - DATE: DateType()\n2025-02-26 19:01:35,806 - snowflake_feature_store - INFO - TARGET_LTV: DoubleType()\n2025-02-26 19:01:35,807 - snowflake_feature_store - INFO - LABEL_DATE: DateType()\n2025-02-26 19:01:35,807 - snowflake_feature_store - INFO - LIFE_TIME_VALUE: DoubleType()\n2025-02-26 19:01:35,808 - snowflake_feature_store - INFO - SESSION_LENGTH: DoubleType()\n2025-02-26 19:01:35,809 - snowflake_feature_store - INFO - TIME_ON_APP: DoubleType()\n2025-02-26 19:01:35,809 - snowflake_feature_store - INFO - TIME_ON_WEBSITE: DoubleType()\n2025-02-26 19:01:35,810 - snowflake_feature_store - INFO - TRANSACTIONS: LongType()\n2025-02-26 19:01:35,811 - snowflake_feature_store - INFO - SUM_TRANSACTIONS_7: DoubleType()\n2025-02-26 19:01:35,811 - snowflake_feature_store - INFO - AVG_TRANSACTIONS_7: DoubleType()\n2025-02-26 19:01:35,812 - snowflake_feature_store - INFO - SUM_TRANSACTIONS_30: DoubleType()\n2025-02-26 19:01:35,812 - snowflake_feature_store - INFO - AVG_TRANSACTIONS_30: DoubleType()\n2025-02-26 19:01:35,813 - snowflake_feature_store - INFO - SUM_LIFE_TIME_VALUE_7: DoubleType()\n2025-02-26 19:01:35,813 - snowflake_feature_store - INFO - AVG_LIFE_TIME_VALUE_7: DoubleType()\n2025-02-26 19:01:35,814 - snowflake_feature_store - INFO - SUM_LIFE_TIME_VALUE_30: DoubleType()\n2025-02-26 19:01:35,814 - snowflake_feature_store - INFO - AVG_LIFE_TIME_VALUE_30: DoubleType()\n2025-02-26 19:01:35,815 - snowflake_feature_store - INFO - ENGAGEMENT_SCORE: DoubleType()\n2025-02-26 19:01:35,815 - snowflake_feature_store - INFO - AVG_TRANSACTION_VALUE: DoubleType()\n\n\n\n# 3. Add metadata columns\ntraining_data = training_data.select(\n    \"*\",  # Keep all existing columns\n    F.lit(training_start_date).alias(\"TRAINING_START_DATE\"),\n    F.lit(training_end_date).alias(\"TRAINING_END_DATE\"),\n    F.lit(prediction_window).alias(\"PREDICTION_WINDOW_DAYS\"),\n    F.current_timestamp().alias(\"GENERATED_AT\")\n)\n\n# 4. Save if table name provided\nif save_table:\n    training_data.write.mode(\"overwrite\").save_as_table(save_table)\n    logger.info(f\"Saved training data to {save_table}\")\n\n# 5. Log data generation stats\nlogger.info(\"\\nTraining Data Statistics:\")\nlogger.info(f\"Total rows: {training_data.count()}\")\nlogger.info(f\"Date range: {training_start_date} to {training_end_date}\")\nlogger.info(f\"Prediction window: {prediction_window} days\")\n\n2025-02-26 19:01:40,964 - snowflake_feature_store - INFO - Saved training data to DATASCIENCE.FEATURE_STORE_DEMO.LTV_TRAINING_DATA\n2025-02-26 19:01:40,965 - snowflake_feature_store - INFO - \nTraining Data Statistics:\n2025-02-26 19:01:41,855 - snowflake_feature_store - INFO - Total rows: 2400\n2025-02-26 19:01:41,857 - snowflake_feature_store - INFO - Date range: 2025-01-01 to 2025-03-01\n2025-02-26 19:01:41,858 - snowflake_feature_store - INFO - Prediction window: 90 days\n\n\n\n! rm -rf feature_metrics/\n\n\n\nBest Practices for Training Data\n\nTime Windows\n\nUse appropriate training/validation splits\nConsider seasonal patterns\nMatch prediction window to business needs\n\nFeature Selection\n\nInclude all relevant features\nDocument feature importance\nTrack feature dependencies\n\nData Quality\n\nHandle missing values consistently\nCheck for data leakage\nValidate label quality\n\nDocumentation\n\nRecord generation parameters\nTrack data lineage\nDocument assumptions",
    "crumbs": [
      "End-to-End Feature Store Example"
    ]
  },
  {
    "objectID": "simple_example.html#conclusion-and-next-steps",
    "href": "simple_example.html#conclusion-and-next-steps",
    "title": "End-to-End Feature Store Example",
    "section": "Conclusion and Next Steps",
    "text": "Conclusion and Next Steps\n\nWhat We’ve Built\nWe’ve created a comprehensive example of using Snowflake’s Feature Store for LTV prediction that demonstrates: 1. Feature Store Setup: Creating and managing a feature store 2. Entity Management: Defining and documenting customer entities 3. Feature Engineering: Creating and transforming features 4. Feature Views: Organizing and versioning features 5. Monitoring: Tracking feature quality and drift 6. Training Data: Generating point-in-time correct datasets\n\n\nPotential Enhancements\nFuture versions could include: 1. Advanced Monitoring - Automated drift detection alerts - Custom validation rules - Feature quality dashboards - Historical metrics tracking\n\nFeature Discovery\n\nFeature search capabilities\nMetadata management\nUsage tracking\nDocumentation generation\n\nProduction Integration\n\nCI/CD pipeline integration\nAutomated testing\nDeployment workflows\nModel registry integration\n\nPerformance Optimization\n\nIncremental updates\nCaching strategies\nQuery optimization\nResource management\n\n\n\n\nBest Practices\nWhen using this template: 1. Documentation - Document feature definitions - Explain business logic - Track dependencies - Maintain version history\n\nTesting\n\nValidate feature logic\nCheck data quality\nTest transformations\nVerify point-in-time correctness\n\nMonitoring\n\nSet up drift detection\nTrack feature freshness\nMonitor data quality\nAlert on issues\n\nGovernance\n\nManage access controls\nTrack lineage\nEnforce standards\nMaintain audit logs\n\n\n\n\nUsing This Template\nTo adapt this example: 1. Replace LTV-specific logic with your use case 2. Adjust feature definitions and transformations 3. Customize monitoring thresholds 4. Add domain-specific validation\n\n\nResources\nFor more information: 1. Snowflake Feature Store Documentation 2. Feature Store Best Practices 3. Snowpark ML Documentation 4. Feature Store Examples",
    "crumbs": [
      "End-to-End Feature Store Example"
    ]
  },
  {
    "objectID": "base.html",
    "href": "base.html",
    "title": "Load Kaggle Competition Data",
    "section": "",
    "text": "import os\nimport zipfile\nfrom snowflake.snowpark import Session\nimport pandas as pd\n\n\nimport os\nfrom kaggle.api.kaggle_api_extended import KaggleApi\nfrom typing import List\n\n\nclass InstacartDataLoader:\n    def __init__(self, session: Session):\n        self.session = session\n        \n    def unzip_data_files(self, path: str):\n        \"\"\"Unzip all CSV files in the directory\"\"\"\n        print(\"Unzipping data files...\")\n        for file in os.listdir(path):\n            if file.endswith('.zip'):\n                zip_path = os.path.join(path, file)\n                with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n                    zip_ref.extractall(path)\n                print(f\"Unzipped: {file}\")\n                \n    def load_csv_to_snowflake(self, file_path: str, table_name: str, schema: str = 'INSTACART_RAW'):\n        \"\"\"Load CSV file to Snowflake table\"\"\"\n        print(f\"Loading {file_path} to {schema}.{table_name}\")\n        chunk_size = 100000\n        for chunk in pd.read_csv(file_path, chunksize=chunk_size):\n            df = self.session.create_dataframe(chunk)\n            df.write.save_as_table(f\"{schema}.{table_name}\", mode=\"append\")\n            \n    def setup_incremental_loading(self):\n        \"\"\"Setup tables for incremental loading\"\"\"\n        self.session.sql(\"\"\"\n            CREATE OR REPLACE TABLE INSTACART_RAW.ORDER_PRODUCTS_STAGE (\n                order_id INTEGER,\n                product_id INTEGER,\n                add_to_cart_order INTEGER,\n                reordered INTEGER,\n                file_name VARCHAR,\n                loaded_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP()\n            )\n        \"\"\").collect()\n\n\ndef download_instacart_data(path: str = \"./data\") -&gt; List[str]:\n    \"\"\"\n    Download Instacart Market Basket Analysis data\n    \n    Args:\n        path: Directory to save the data\n        \n    Returns:\n        List of downloaded file paths\n    \n    Raises:\n        Exception: If competition rules haven't been accepted or other API errors\n    \"\"\"\n    try:\n        api = KaggleApi()\n        api.authenticate()\n        \n        # Create data directory if it doesn't exist\n        os.makedirs(path, exist_ok=True)\n        \n        # Check if we've already downloaded the data\n        zip_path = f\"{path}/instacart-market-basket-analysis.zip\"\n        if os.path.exists(zip_path):\n            print(\"Data already downloaded, using existing files...\")\n        else:\n            print(\"Downloading competition data...\")\n            try:\n                api.competition_download_files(\n                    'instacart-market-basket-analysis',\n                    path=path\n                )\n            except Exception as e:\n                if \"rules\" in str(e):\n                    raise Exception(\n                        \"Please accept the competition rules first at \"\n                        \"https://www.kaggle.com/competitions/instacart-market-basket-analysis\"\n                    ) from e\n                raise\n        \n        # Unzip if needed\n        csv_files = [f for f in os.listdir(path) if f.endswith('.csv')]\n        if not csv_files:\n            print(\"Extracting files...\")\n            import zipfile\n            with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n                zip_ref.extractall(path)\n        \n        # List available files\n        csv_files = [f for f in os.listdir(path) if f.endswith('.csv')]\n        print(\"\\nAvailable files:\")\n        for file in csv_files:\n            print(f\"- {file}\")\n            \n        return [os.path.join(path, f) for f in csv_files]\n        \n    except Exception as e:\n        print(f\"Error downloading data: {str(e)}\")\n        raise",
    "crumbs": [
      "Load Kaggle Competition Data"
    ]
  },
  {
    "objectID": "logging.html",
    "href": "logging.html",
    "title": "Expections",
    "section": "",
    "text": "source\n\nsetup_logger\n\n setup_logger (name:str='snowflake_feature_store', level:int=20,\n               log_file:Optional[str]=None)\n\n*Set up logger with consistent formatting\nArgs: name: Logger name level: Logging level log_file: Optional file path for logging\nReturns: Configured logger*",
    "crumbs": [
      "Expections"
    ]
  },
  {
    "objectID": "example_functions.html",
    "href": "example_functions.html",
    "title": "Example Module",
    "section": "",
    "text": "source\n\ngenerate_demo_data\n\n generate_demo_data (session:snowflake.snowpark.session.Session,\n                     schema:str, num_customers:int=1000,\n                     ltv_multiplier:float=1.0,\n                     session_length_multiplier:float=1.0,\n                     start_date:Optional[datetime.datetime]=None,\n                     num_days:int=30, table_type:str='')\n\n*Generate synthetic customer data for demonstration\nArgs: session: Snowflake session schema: Schema to create tables in num_customers: Number of customers to generate ltv_multiplier: Multiplier for lifetime value session_length_multiplier: Multiplier for session length start_date: Start date for data (default: 30 days ago) num_days: Number of days of data to generate*\n\nsource\n\n\nget_example_data\n\n get_example_data (session:snowflake.snowpark.session.Session, schema:str,\n                   num_customers:int=100, ltv_multiplier:float=1.0,\n                   session_length_multiplier:float=1.0,\n                   start_date:Optional[datetime.datetime]=None,\n                   num_days:int=30, table_type:str='')\n\n*Load or generate example customer data\nArgs: session: Active Snowflake session schema: Schema where tables are located num_customers: Number of customers if generating new data\nReturns: DataFrame with customer activity data*\n\nsource\n\n\ncreate_feature_configs\n\n create_feature_configs ()\n\nCreate example feature configurations\n\nsource\n\n\nrun_end_to_end_example\n\n run_end_to_end_example (metrics_path:Optional[str]=None,\n                         num_customers:int=100)\n\n*Run end-to-end feature store example\nArgs: metrics_path: Optional path to save metrics num_customers: Number of customers to generate*\n\nfrom snowflake_feature_store.examples import run_end_to_end_example\nimport tempfile\nfrom pathlib import Path\n\n# Create temporary directory for metrics\nmetrics_dir = Path(tempfile.mkdtemp()) / \"feature_store_metrics\"\n\n# Run example\nrun_end_to_end_example(\n    metrics_path=str(metrics_dir),\n    num_customers=100\n)\n\n2025-02-17 20:47:42,962 - snowflake_feature_store - INFO - No active session found, creating new connection from environment\n2025-02-17 20:47:43,696 - snowflake_feature_store - INFO - Initialized connection to \"DATASCIENCE\".\"FEATURE_STORE_DEMO\"\n2025-02-17 20:47:46,024 - snowflake_feature_store - INFO - FeatureStoreManager initialized\n2025-02-17 20:47:49,023 - snowflake_feature_store - INFO - Generated 2400 rows of demo data in \"DATASCIENCE\".FEATURE_STORE_20250218_044743_b7ff82cd.CUSTOMER_ACTIVITY\n2025-02-17 20:47:49,025 - snowflake_feature_store - INFO - \nSample Data:\n---------------------------------------------------------------------------------------------------------------------------------\n|\"CUSTOMER_ID\"  |\"DATE\"      |\"LIFE_TIME_VALUE\"   |\"SESSION_LENGTH\"   |\"TIME_ON_APP\"      |\"TIME_ON_WEBSITE\"   |\"TRANSACTIONS\"  |\n---------------------------------------------------------------------------------------------------------------------------------\n|C8             |2025-01-18  |538.086304232647    |9.293443367138973  |8.435992889854191  |10.464463447436238  |5               |\n|C23            |2025-02-16  |360.85532792500635  |NULL               |8.160214513528983  |6.733980655675666   |3               |\n|C34            |2025-02-16  |361.59091693705415  |NULL               |10.04069304006288  |9.42791752502259    |3               |\n|C76            |2025-02-16  |46.97853516274013   |NULL               |3.782038923769274  |3.7123425091379185  |1               |\n|C0             |2025-02-16  |398.1945730552295   |NULL               |9.26915277397465   |8.431093341017991   |3               |\n---------------------------------------------------------------------------------------------------------------------------------\n\n2025-02-17 20:47:49,585 - snowflake_feature_store - INFO - \nSchema:\n2025-02-17 20:47:49,881 - snowflake_feature_store - INFO - CUSTOMER_ID: StringType()\n2025-02-17 20:47:49,882 - snowflake_feature_store - INFO - DATE: DateType()\n2025-02-17 20:47:49,882 - snowflake_feature_store - INFO - LIFE_TIME_VALUE: DoubleType()\n2025-02-17 20:47:49,882 - snowflake_feature_store - INFO - SESSION_LENGTH: DoubleType()\n2025-02-17 20:47:49,883 - snowflake_feature_store - INFO - TIME_ON_APP: DoubleType()\n2025-02-17 20:47:49,883 - snowflake_feature_store - INFO - TIME_ON_WEBSITE: DoubleType()\n2025-02-17 20:47:49,883 - snowflake_feature_store - INFO - TRANSACTIONS: LongType()\n2025-02-17 20:47:51,477 - snowflake_feature_store - INFO - Created entity: CUSTOMER with keys: ['CUSTOMER_ID']\n2025-02-17 20:47:56,646 - snowflake_feature_store - INFO - Validated feature LIFE_TIME_VALUE (stats: {'timestamp': '2025-02-18T04:47:55.431732', 'row_count': 2400, 'null_count': 0, 'null_ratio': 0.0, 'unique_count': 2400, 'min_value': 1.0103904399909094, 'max_value': 749.1914533514039, 'mean_value': 370.8857998161906, 'std_value': 216.29399706133577})\n2025-02-17 20:47:58,584 - snowflake_feature_store - INFO - Validated feature SESSION_LENGTH (stats: {'timestamp': '2025-02-18T04:47:57.098022', 'row_count': 2400, 'null_count': 0, 'null_ratio': 0.0, 'unique_count': 1922, 'min_value': 0.0, 'max_value': 12.375177047794647, 'mean_value': 4.924054819640092, 'std_value': 3.3871291643468586})\n2025-02-17 20:48:00,098 - snowflake_feature_store - INFO - Validated feature TRANSACTIONS (stats: {'timestamp': '2025-02-18T04:47:59.060760', 'row_count': 2400, 'null_count': 0, 'null_ratio': 0.0, 'unique_count': 7, 'min_value': 1.0, 'max_value': 7.0, 'mean_value': 3.364167, 'std_value': 1.9934989340353308})\n2025-02-17 20:48:01,613 - snowflake_feature_store - INFO - Validated feature AVG_SESSION_LENGTH_7 (stats: {'timestamp': '2025-02-18T04:48:00.529359', 'row_count': 2400, 'null_count': 0, 'null_ratio': 0.0, 'unique_count': 2307, 'min_value': 0.0, 'max_value': 11.6070937122751, 'mean_value': 4.916869536323822, 'std_value': 1.5555037318685312})\n2025-02-17 20:48:03,081 - snowflake_feature_store - INFO - Validated feature SUM_TRANSACTIONS_7 (stats: {'timestamp': '2025-02-18T04:48:01.994533', 'row_count': 2400, 'null_count': 0, 'null_ratio': 0.0, 'unique_count': 41, 'min_value': 1.0, 'max_value': 43.0, 'mean_value': 20.574166666666667, 'std_value': 7.77944187289988})\n2025-02-17 20:48:16,885 - snowflake_feature_store - INFO - Created feature view: customer_behavior with 11 features\n2025-02-17 20:48:16,890 - snowflake_feature_store - INFO - \nFeature Statistics:\n2025-02-17 20:48:16,890 - snowflake_feature_store - INFO - \nLIFE_TIME_VALUE:\n2025-02-17 20:48:16,891 - snowflake_feature_store - INFO - Timestamp: 2025-02-18T04:48:08.878860\nRow count: 2400\nNull count: 0 (0.0%)\nUnique values: 2400\nMin value: 1.01\nMax value: 749.19\nMean value: 370.89\nStd dev: 216.29\n2025-02-17 20:48:16,892 - snowflake_feature_store - INFO - \nSESSION_LENGTH:\n2025-02-17 20:48:16,892 - snowflake_feature_store - INFO - Timestamp: 2025-02-18T04:48:10.403487\nRow count: 2400\nNull count: 0 (0.0%)\nUnique values: 1922\nMin value: 0.00\nMax value: 12.38\nMean value: 4.92\nStd dev: 3.39\n2025-02-17 20:48:16,892 - snowflake_feature_store - INFO - \nTRANSACTIONS:\n2025-02-17 20:48:16,893 - snowflake_feature_store - INFO - Timestamp: 2025-02-18T04:48:11.836755\nRow count: 2400\nNull count: 0 (0.0%)\nUnique values: 7\nMin value: 1.00\nMax value: 7.00\nMean value: 3.36\nStd dev: 1.99\n2025-02-17 20:48:16,893 - snowflake_feature_store - INFO - \nAVG_SESSION_LENGTH_7:\n2025-02-17 20:48:16,894 - snowflake_feature_store - INFO - Timestamp: 2025-02-18T04:48:13.383972\nRow count: 2400\nNull count: 0 (0.0%)\nUnique values: 2307\nMin value: 0.00\nMax value: 11.61\nMean value: 4.92\nStd dev: 1.56\n2025-02-17 20:48:16,894 - snowflake_feature_store - INFO - \nSUM_TRANSACTIONS_7:\n2025-02-17 20:48:16,894 - snowflake_feature_store - INFO - Timestamp: 2025-02-18T04:48:15.859696\nRow count: 2400\nNull count: 0 (0.0%)\nUnique values: 41\nMin value: 1.00\nMax value: 43.00\nMean value: 20.57\nStd dev: 7.78\n2025-02-17 20:48:17,050 - snowflake_feature_store - INFO - Spine DataFrame columns: ['CUSTOMER_ID', 'DATE']\n2025-02-17 20:48:17,051 - snowflake_feature_store - INFO - Spine DataFrame schema: StructType([StructField('CUSTOMER_ID', StringType(), nullable=True), StructField('DATE', DateType(), nullable=True)])\n2025-02-17 20:48:19,157 - snowflake_feature_store - INFO - Generating dataset with name: DATASET_20250218_044819_3c2eda3b\n2025-02-17 20:48:19,158 - snowflake_feature_store - INFO - Label columns: ['\"LIFE_TIME_VALUE\"']\n2025-02-17 20:48:19,158 - snowflake_feature_store - INFO - Timestamp column: \"DATE\"\n2025-02-17 20:48:23,181 - snowflake_feature_store - INFO - \nTraining Data Sample:\n----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n|\"CUSTOMER_ID\"  |\"DATE\"      |\"LIFE_TIME_VALUE\"   |\"SESSION_LENGTH\"    |\"TIME_ON_APP\"       |\"TIME_ON_WEBSITE\"  |\"TRANSACTIONS\"  |\"AVG_SESSION_LENGTH_7\"  |\"SUM_SESSION_LENGTH_7\"  |\"AVG_TRANSACTIONS_7\"  |\"SUM_TRANSACTIONS_7\"  |\n----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n|C30            |2025-01-18  |297.583740234375    |0.0                 |6.600606441497803   |8.969278335571289  |2               |0.0                     |0.0                     |2.0                   |2.0                   |\n|C30            |2025-01-19  |47.4866828918457    |2.21079158782959    |2.3981170654296875  |5.841498851776123  |1               |1.105395793914795       |2.21079158782959        |1.5                   |3.0                   |\n|C30            |2025-01-20  |133.73348999023438  |1.8510329723358154  |2.423548698425293   |7.492861270904541  |1               |1.353941559791565       |4.061824798583984       |1.3329999446868896    |4.0                   |\n|C30            |2025-01-21  |55.70567321777344   |1.9283854961395264  |4.55950927734375    |6.430238246917725  |1               |1.497552514076233       |5.990210056304932       |1.25                  |5.0                   |\n|C30            |2025-01-23  |129.9962615966797   |3.824254035949707   |7.2560319900512695  |8.202532768249512  |1               |1.9628928899765015      |9.814464569091797       |1.2000000476837158    |6.0                   |\n----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\n2025-02-17 20:48:24,515 - snowflake_feature_store - INFO - Dependencies for customer_behavior: {'customer_behavior'}\n2025-02-17 20:48:24,516 - snowflake_feature_store - INFO - \nFeature Dependencies: {'customer_behavior'}\n2025-02-17 20:48:24,754 - snowflake_feature_store - INFO - Cleaned up schema FEATURE_STORE_20250218_044743_b7ff82cd",
    "crumbs": [
      "Example Module"
    ]
  },
  {
    "objectID": "expections.html",
    "href": "expections.html",
    "title": "Expections",
    "section": "",
    "text": "source\n\nFeatureStoreException\nBase exception for feature store errors\n\nsource\n\n\nConfigurationError\nRaised when there’s an error in configuration\n\nsource\n\n\nConnectionError\nRaised when there’s an error connecting to Snowflake\n\nsource\n\n\nEntityError\nRaised when there’s an error with entities\n\nsource\n\n\nFeatureViewError\nRaised when there’s an error with feature views\n\nsource\n\n\nValidationError\nRaised when there’s a validation error",
    "crumbs": [
      "Expections"
    ]
  },
  {
    "objectID": "connection.html",
    "href": "connection.html",
    "title": "Connection",
    "section": "",
    "text": "source\n\nConnectionConfig\n\n ConnectionConfig (user:str, password:Optional[str]=None, account:str,\n                   role:str='DATA_SCIENTIST', warehouse:str='DS_WH_XS',\n                   database:Optional[str]=None, schema:Optional[str]=None,\n                   private_key_path:Optional[pathlib.Path]=None,\n                   private_key_pem:Optional[str]=None,\n                   authenticator:Optional[str]=None,\n                   query_tag:Optional[Dict[str,Any]]=None)\n\nConfiguration for Snowflake connection\n\nsource\n\n\nSnowflakeConnection\n\n SnowflakeConnection (session:snowflake.snowpark.session.Session,\n                      warehouse:Optional[str]=None,\n                      database:Optional[str]=None,\n                      schema:Optional[str]=None,\n                      config:Optional[__main__.ConnectionConfig]=None)\n\nManages Snowflake connection and configuration\n\nsource\n\n\nget_connection\n\n get_connection (database:Optional[str]=None, schema:Optional[str]=None,\n                 warehouse:Optional[str]=None, role:Optional[str]=None,\n                 create_objects:bool=True)\n\n*Get Snowflake connection from active session or environment\nFirst tries to get active session, falls back to environment variables. Optional parameters override both active session and environment variables.\nArgs: database: Optional database to use schema: Optional schema to use warehouse: Optional warehouse to use role: Optional role to use create_objects: Whether to create database/schema if they don’t exist\nReturns: SnowflakeConnection object\nRaises: ConnectionError: If connection cannot be established*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndatabase\nOptional\nNone\n\n\n\nschema\nOptional\nNone\n\n\n\nwarehouse\nOptional\nNone\n\n\n\nrole\nOptional\nNone\n\n\n\ncreate_objects\nbool\nTrue\nAdd parameter to control object creation\n\n\nReturns\nSnowflakeConnection\n\n\n\n\n\n\n# Example usage\nfrom snowflake_feature_store.connection import get_connection, ConnectionConfig\n\n# Method 1: Get connection automatically\nconn = get_connection()\n\n# Method 2: From environment variables\nconn = SnowflakeConnection.from_env()\n\n# Method 3: From YAML config create yaml file\n# config = ConnectionConfig.from_yaml('config.yaml')\n# conn = SnowflakeConnection.from_config(config)\n\n# Test connection\nif conn.test_connection():\n    print(\"Connected successfully!\")\n    \n# Use connection\ndf = conn.session.table('MY_TABLE')\n\n# Clean up\nconn.close()\n\n2025-02-26 18:56:36,880 - snowflake_feature_store - INFO - No active session found, creating new connection from environment\n2025-02-26 18:56:37,393 - snowflake_feature_store - INFO - Initialized connection to \"CONTAINER_DEMO_DB\".\"PUBLIC\"\n2025-02-26 18:56:37,394 - snowflake_feature_store - INFO - Using role: \"ACCOUNTADMIN\", warehouse: \"CONTAINER_DEMO_WH\", database: \"CONTAINER_DEMO_DB\", schema: \"PUBLIC\"\n2025-02-26 18:56:37,970 - snowflake_feature_store - INFO - Initialized connection to \"CONTAINER_DEMO_DB\".\"PUBLIC\"\n2025-02-26 18:56:38,140 - snowflake_feature_store - INFO - Connection test successful\nConnected successfully!\n2025-02-26 18:56:38,624 - snowflake_feature_store - INFO - Main session closed successfully",
    "crumbs": [
      "Connection"
    ]
  }
]