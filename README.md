# Snowflake Feature Store


<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->

# Snowflake Feature Store

> A comprehensive toolkit for building and managing ML features in
> Snowflake, with focus on LTV prediction.

## What is this?

This library provides a production-ready interface to Snowflake’s
Feature Store, enabling you to: 1. Create and manage feature
transformations with validation 2. Handle temporal features and windowed
aggregations 3. Generate point-in-time correct training datasets 4.
Monitor feature quality and detect drift 5. Maintain feature
documentation and versioning

## Install

``` bash
pip install snowflake-feature-store
```

## How to use

### Complete LTV Example

Here’s a full example of setting up a feature store for LTV prediction:

``` python
from snowflake_feature_store.connection import get_connection
from snowflake_feature_store.manager import feature_store_session 
from snowflake_feature_store.config import ( FeatureViewConfig, FeatureConfig, FeatureValidationConfig )

Connect to Snowflake
conn = get_connection()

Create feature store session
with feature_store_session(conn, cleanup=False) as manager: # 1. Create Customer Entity manager.add_entity( name="CUSTOMER", join_keys=["CUSTOMER_ID"], description="Customer entity for LTV prediction" )

# 2. Configure Features
feature_configs = {
    "LIFE_TIME_VALUE": FeatureConfig(
        name="LIFE_TIME_VALUE",
        description="Current customer value",
        validation=FeatureValidationConfig(
            null_threshold=0.1,
            range_check=True,
            min_value=0
        )
    ),
    "SESSION_LENGTH": FeatureConfig(
        name="SESSION_LENGTH",
        description="Session duration in minutes",
        validation=FeatureValidationConfig(
            null_threshold=0.3,
            range_check=True,
            min_value=0
        )
    )
}

# 3. Create Feature View
config = FeatureViewConfig(
    name="customer_behavior",
    domain="RETAIL",
    entity="CUSTOMER",
    feature_type="BEHAVIOR",
    features=feature_configs,
    refresh=RefreshConfig(frequency="1 day")
)

# 4. Add Transformations
transforms = [
    fill_na(['SESSION_LENGTH'], 0),
    moving_agg(
        cols=['TRANSACTIONS'],
        window_sizes=[7, 30],
        agg_funcs=['SUM', 'AVG'],
        partition_by=['CUSTOMER_ID'],
        order_by=['DATE']
    )
]

# 5. Create Feature View
feature_view = manager.add_feature_view(
    config=config,
    df=source_df,
    entity_name="CUSTOMER",
    transforms=transforms
)

# 6. Generate Training Data
training_data = generate_ltv_training_data(
    manager=manager,
    feature_view=feature_view,
    training_start_date='2024-01-01',
    training_end_date='2024-03-01',
    prediction_window=90
)
```

### Key Components

1.  **Feature Configuration**
    - Validation rules
    - Data quality checks
    - Documentation
    - Dependencies
2.  **Transformations**
    - Missing value handling
    - Window aggregations
    - Custom transforms
    - Feature combination
3.  **Monitoring**
    - Feature statistics
    - Drift detection
    - Quality metrics
    - Alert configuration
4.  **Training Data**
    - Point-in-time correctness
    - Label generation
    - Feature selection
    - Data validation

## Documentation

The library includes detailed documentation for each component:

1.  [Connection Management](./01_connection.ipynb): Snowflake connection
    setup and management
2.  [Feature Transforms](./02_transforms.ipynb): Feature engineering and
    validation
3.  [Feature Views](./03_feature_view.ipynb): Feature organization and
    versioning
4.  [Feature Store](./04_manager.ipynb): Feature store operations and
    monitoring
5.  [End-to-End Example](./06_simple_example.ipynb): Complete LTV
    prediction workflow

## Advanced Features

1.  **Feature Monitoring**

``` python
# Monitor feature drift
monitor = LTVMonitor(manager, feature_view.name) 
monitor.set_baseline(feature_view.feature_df) 
drift_metrics = monitor.check_feature_health(new_data)
```

2.  **Custom Transformations**

``` python
# Create custom transform
@transform_config(name="engagement_score")
def calculate_engagement(df): 
    return df.with_column( 'ENGAGEMENT_SCORE', (F.col('SESSION_LENGTH') + F.col('TIME_ON_APP')) / 2.0 )
```

3.  **Point-in-Time Training**

``` python
# Generate training data create a function like this
training_data = generate_ltv_training_data( manager=manager, feature_view=feature_view, training_start_date='2024-01-01', prediction_window=90 )
```

### Installation

Install latest from the GitHub
[repository](https://github.com/Jeremy-Demlow/sf-feature-store):

``` sh
$ pip install git+https://github.com/Jeremy-Demlow/sf-feature-store.git
```

or from [pypi](https://pypi.org/project/sf-feature-store/)

``` sh
$ pip install sf_feature_store
```


Hi Everyone, 

Network rules specify the external destinations (hostnames, IPs, ports) that your container service is allowed to communicate with. They define the boundaries of where your service can send network traffic. Think of them as a whitelist of destinations. The values from SYSTEM$GET_PRIVATELINK_CONFIG() are what you should be focusing on if you want to work through your privatelink connection. You have to create access points for these to work.

1. First, identify exactly what your service needs:
```sql
-- Get the exact PrivateLink endpoints for your account
SELECT SYSTEM$GET_PRIVATELINK_CONFIG();
```
2. Create a targeted network rule:
```sql
-- Create a network rule specifically for Snowflake's endpoints
CREATE OR REPLACE NETWORK RULE jupyter_snowflake_connectivity
  MODE = EGRESS
  TYPE = PRIVATE_HOST_PORT
  VALUE_LIST = (
    '<your-spcs-auth-privatelink-url-value>',
    '<your-app-service-privatelink-url-value>'
  );
```
This approach only grants access to the specific Snowflake PrivateLink endpoints needed for authentication and accessing your service, rather than including S3 and PyPI which may not be required for basic functionality.

3. For additional functionality:

If your Jupyter service needs to access specific packages or data sources, you can create separate, purpose-specific network rules:
```sql
-- Only add if your service needs to fetch packages
CREATE OR REPLACE NETWORK RULE jupyter_package_access
  MODE = EGRESS
  TYPE = HOST_PORT
  VALUE_LIST = ('pypi.org:443', 'files.pythonhosted.org:443');

-- Only add if your service needs S3 access
CREATE OR REPLACE NETWORK RULE jupyter_s3_access
  MODE = EGRESS
  TYPE = PRIVATE_HOST_PORT
  VALUE_LIST = ('<your-specific-s3-bucket>.s3.<your-region>.amazonaws.com');
```

4. Create a more targeted EAI:

```sql
-- Include only the network rules your service actually needs
CREATE OR REPLACE EXTERNAL ACCESS INTEGRATION jupyter_eai
  ALLOWED_NETWORK_RULES = (jupyter_snowflake_connectivity)
  ENABLED = true;

-- If you added the optional rules above, you might use:
-- ALLOWED_NETWORK_RULES = (jupyter_snowflake_connectivity, jupyter_package_access, jupyter_s3_access)
```
This approach follows the principle of least privilege by only granting the specific access needed for the service to function, which is a security best practice.

For your POC environment, you might start with just the Snowflake PrivateLink endpoints and then add additional access only if you encounter specific functionality issues.