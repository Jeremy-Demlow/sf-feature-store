{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Views\n",
    "\n",
    "> This module implements a robust feature view creation system with monitoring capabilities for Snowflake ML Feature Store. It provides comprehensive feature validation and statistical monitoring through the FeatureMonitor class, which tracks metrics like null ratios, cardinality, and distribution shifts. The FeatureViewBuilder handles the end-to-end process of creating properly configured feature views with validation checks and metadata attachment, while the create_feature_view function offers a simplified API for this complex process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp feature_view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from __future__ import annotations\n",
    "from typing import Dict, List, Optional, Union, Set\n",
    "from datetime import datetime\n",
    "from snowflake.snowpark import DataFrame\n",
    "from snowflake.ml.feature_store import FeatureView, Entity\n",
    "import snowflake.snowpark.functions as F\n",
    "import json\n",
    "\n",
    "# Import our modules\n",
    "from snowflake_feature_store.exceptions import FeatureViewError, ValidationError\n",
    "from snowflake_feature_store.logging import logger\n",
    "from snowflake_feature_store.config import (\n",
    "    BaseModel, Field, FeatureViewConfig, \n",
    "    FeatureConfig, RefreshConfig\n",
    ")\n",
    "from snowflake.snowpark.types import (\n",
    "    StructType, StructField, StringType, DateType,\n",
    "    DoubleType, LongType, TimestampType\n",
    ")\n",
    "from snowflake_feature_store.transforms import Transform, TransformConfig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class FeatureStats(BaseModel):\n",
    "    \"\"\"Statistics for feature monitoring\"\"\"\n",
    "    timestamp: datetime\n",
    "    row_count: int\n",
    "    null_count: int\n",
    "    null_ratio: float\n",
    "    unique_count: Optional[int] = None\n",
    "    min_value: Optional[float] = None\n",
    "    max_value: Optional[float] = None\n",
    "    mean_value: Optional[float] = None\n",
    "    std_value: Optional[float] = None\n",
    "    \n",
    "    def __str__(self) -> str:\n",
    "        \"\"\"Pretty print statistics\"\"\"\n",
    "        stats = [\n",
    "            f\"Timestamp: {self.timestamp.isoformat()}\",\n",
    "            f\"Row count: {self.row_count}\",\n",
    "            f\"Null count: {self.null_count} ({self.null_ratio:.1%})\",\n",
    "            f\"Unique values: {self.unique_count}\"\n",
    "        ]\n",
    "        \n",
    "        if all(v is not None for v in [self.min_value, self.max_value, self.mean_value]):\n",
    "            stats.extend([\n",
    "                f\"Min value: {self.min_value:.2f}\",\n",
    "                f\"Max value: {self.max_value:.2f}\",\n",
    "                f\"Mean value: {self.mean_value:.2f}\",\n",
    "                f\"Std dev: {self.std_value:.2f}\" if self.std_value else \"Std dev: N/A\"\n",
    "            ])\n",
    "            \n",
    "        return \"\\n\".join(stats)\n",
    "    \n",
    "    def model_dump(self, **kwargs) -> Dict:\n",
    "        \"\"\"Convert stats to dictionary for storage/display\"\"\"\n",
    "        data = super().model_dump(**kwargs)\n",
    "        data['timestamp'] = self.timestamp.isoformat()\n",
    "        return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class FeatureMonitor:\n",
    "    \"\"\"Monitor feature statistics and detect drift\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 feature_config: FeatureConfig,\n",
    "                 collect_detailed_stats: bool = True):\n",
    "        self.config = feature_config\n",
    "        self._baseline_stats: Optional[FeatureStats] = None\n",
    "        self.collect_detailed_stats = collect_detailed_stats\n",
    "\n",
    "    def _verify_column_names(self, df: DataFrame, column: str) -> None:\n",
    "        \"\"\"Verify column names in DataFrame\"\"\"\n",
    "        logger.debug(f\"All columns: {df.columns}\")\n",
    "        logger.debug(f\"Schema: {df.schema}\")\n",
    "        logger.debug(f\"Looking for column: {column}\")\n",
    "        if column not in df.columns:\n",
    "            matches = [c for c in df.columns if c.upper() == column.upper()]\n",
    "            if matches:\n",
    "                logger.warning(f\"Column case mismatch. Found {matches[0]} instead of {column}\")\n",
    "    \n",
    "    def set_baseline(self, stats: FeatureStats) -> None:\n",
    "        \"\"\"Set baseline statistics for drift detection\"\"\"\n",
    "        self._baseline_stats = stats\n",
    "        \n",
    "    def detect_drift(self, current_stats: FeatureStats) -> Dict[str, float]:\n",
    "        \"\"\"Detect drift from baseline statistics\n",
    "        \n",
    "        Returns dictionary of drift metrics\n",
    "        \"\"\"\n",
    "        if not self._baseline_stats:\n",
    "            raise FeatureViewError(\"No baseline statistics set\")\n",
    "            \n",
    "        drift_metrics = {}\n",
    "        \n",
    "        # Check null ratio drift\n",
    "        drift_metrics['null_ratio_change'] = (\n",
    "            current_stats.null_ratio - self._baseline_stats.null_ratio\n",
    "        )\n",
    "        \n",
    "        # Check numeric drift if stats available\n",
    "        if (current_stats.mean_value is not None and \n",
    "            self._baseline_stats.mean_value is not None):\n",
    "            \n",
    "            # Mean shift\n",
    "            drift_metrics['mean_shift'] = (\n",
    "                current_stats.mean_value - self._baseline_stats.mean_value\n",
    "            )\n",
    "            \n",
    "            # Distribution shift (using std dev)\n",
    "            if current_stats.std_value and self._baseline_stats.std_value:\n",
    "                drift_metrics['std_ratio'] = (\n",
    "                    current_stats.std_value / self._baseline_stats.std_value\n",
    "                )\n",
    "        \n",
    "        return drift_metrics\n",
    "        \n",
    "    def compute_stats(self, df: DataFrame, column: str) -> FeatureStats:\n",
    "        \"\"\"Compute statistics for a feature column\"\"\"\n",
    "        try:\n",
    "            # Verify column names first\n",
    "            self._verify_column_names(df, column)\n",
    "            total_count = df.count()\n",
    "            null_count = df.filter(F.col(column).is_null()).count()\n",
    "            \n",
    "            # Initialize stats\n",
    "            stats = {\n",
    "                'timestamp': datetime.utcnow(),\n",
    "                'row_count': total_count,\n",
    "                'null_count': null_count,\n",
    "                'null_ratio': null_count / total_count if total_count > 0 else 1.0\n",
    "            }\n",
    "            \n",
    "            if self.collect_detailed_stats:\n",
    "                # Get column type and schema field\n",
    "                schema_field = next(field for field in df.schema.fields if field.name.upper() == column.upper())\n",
    "                col_type = str(schema_field.datatype)\n",
    "                logger.debug(f\"Computing stats for {column} (type: {col_type})\")\n",
    "                \n",
    "                # Always compute unique count\n",
    "                unique_count = df.select(column).distinct().count()\n",
    "                stats['unique_count'] = unique_count\n",
    "                \n",
    "                # Check if column is numeric - improved type checking\n",
    "                is_numeric = any(\n",
    "                    col_type.upper().startswith(t) \n",
    "                    for t in ['DOUBLE', 'FLOAT', 'INT', 'LONG', 'DECIMAL', 'NUMBER']\n",
    "                ) or hasattr(schema_field.datatype, 'scale')\n",
    "                \n",
    "                logger.debug(f\"Column {column} is_numeric: {is_numeric} (type: {col_type})\")\n",
    "                \n",
    "                if is_numeric:\n",
    "                    # Compute numeric stats on non-null values\n",
    "                    numeric_df = df.filter(F.col(column).is_not_null())\n",
    "                    if numeric_df.count() > 0:\n",
    "                        # Use agg with explicit column names\n",
    "                        agg_df = numeric_df.agg([\n",
    "                            F.min(column).alias(\"MIN_VAL\"),\n",
    "                            F.max(column).alias(\"MAX_VAL\"),\n",
    "                            F.avg(column).alias(\"AVG_VAL\"),\n",
    "                            F.stddev(column).alias(\"STD_VAL\")\n",
    "                        ])\n",
    "                        \n",
    "                        # Debug logging\n",
    "                        logger.debug(f\"Aggregation columns: {agg_df.schema.names}\")\n",
    "                        \n",
    "                        # Get result row as dictionary\n",
    "                        result_dict = agg_df.collect()[0].asDict()\n",
    "                        \n",
    "                        # Debug logging\n",
    "                        logger.debug(f\"Result dict: {result_dict}\")\n",
    "                        \n",
    "                        # Update stats using the correct column names\n",
    "                        stats.update({\n",
    "                            'min_value': float(result_dict[\"MIN_VAL\"]) if result_dict[\"MIN_VAL\"] is not None else None,\n",
    "                            'max_value': float(result_dict[\"MAX_VAL\"]) if result_dict[\"MAX_VAL\"] is not None else None,\n",
    "                            'mean_value': float(result_dict[\"AVG_VAL\"]) if result_dict[\"AVG_VAL\"] is not None else None,\n",
    "                            'std_value': float(result_dict[\"STD_VAL\"]) if result_dict[\"STD_VAL\"] is not None else None\n",
    "                        })\n",
    "                        \n",
    "                        logger.debug(f\"Numeric stats computed for {column}: {stats}\")\n",
    "                    else:\n",
    "                        # No non-null numeric values\n",
    "                        stats.update({\n",
    "                            'min_value': None,\n",
    "                            'max_value': None,\n",
    "                            'mean_value': None,\n",
    "                            'std_value': None\n",
    "                        })\n",
    "                else:\n",
    "                    # For non-numeric columns, set numeric stats to None\n",
    "                    stats.update({\n",
    "                        'min_value': None,\n",
    "                        'max_value': None,\n",
    "                        'mean_value': None,\n",
    "                        'std_value': None\n",
    "                    })\n",
    "            \n",
    "            logger.debug(f\"Final stats for {column}: {stats}\")\n",
    "            return FeatureStats(**stats)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error computing stats for {column}: {str(e)}\")\n",
    "            logger.error(f\"Exception type: {type(e)}\")\n",
    "            logger.error(f\"Exception args: {e.args}\")\n",
    "            raise FeatureViewError(f\"Stats computation failed: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "class FeatureViewBuilder:\n",
    "    \"\"\"Builder for creating feature views with monitoring\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        config: FeatureViewConfig,\n",
    "        feature_df: DataFrame,\n",
    "        entities: Union[Entity, List[Entity]],\n",
    "        collect_stats: bool = True\n",
    "    ):\n",
    "        self.config = config\n",
    "        self.feature_df = feature_df\n",
    "        self.entities = [entities] if isinstance(entities, Entity) else entities\n",
    "        self.monitors: Dict[str, FeatureMonitor] = {}\n",
    "        self.collect_stats = collect_stats\n",
    "        \n",
    "        # Initialize monitors only for features that exist in the DataFrame\n",
    "        available_columns = set(feature_df.columns)\n",
    "        for name, feature_config in config.features.items():\n",
    "            if name in available_columns:  # Only monitor existing columns\n",
    "                self.monitors[name] = FeatureMonitor(\n",
    "                    feature_config,\n",
    "                    collect_detailed_stats=collect_stats\n",
    "                )\n",
    "    \n",
    "    def _validate_features(self) -> None:\n",
    "        \"\"\"Validate features against their configurations\"\"\"\n",
    "        for name, monitor in self.monitors.items():\n",
    "            try:\n",
    "                # Compute current stats\n",
    "                stats = monitor.compute_stats(self.feature_df, name)\n",
    "                \n",
    "                # Validate against config\n",
    "                if stats.null_ratio > monitor.config.validation.null_threshold:\n",
    "                    raise ValidationError(\n",
    "                        f\"Feature {name} has {stats.null_ratio:.1%} null values, \"\n",
    "                        f\"exceeding threshold of {monitor.config.validation.null_threshold:.1%}\"\n",
    "                    )\n",
    "                \n",
    "                # Set as baseline for future monitoring\n",
    "                monitor.set_baseline(stats)\n",
    "                \n",
    "                logger.info(f\"Validated feature {name} (stats: {stats.model_dump()})\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                raise FeatureViewError(f\"Validation failed for {name}: {str(e)}\")\n",
    "\n",
    "    def _validate_timestamp_col(self, df: DataFrame) -> None:\n",
    "        \"\"\"Validate timestamp column type\"\"\"\n",
    "        if self.config.timestamp_col:\n",
    "            col_type = df.schema[self.config.timestamp_col].datatype\n",
    "            if not isinstance(col_type, (DateType, TimestampType)):\n",
    "                # Try to cast the column\n",
    "                logger.warning(\n",
    "                    f\"Timestamp column {self.config.timestamp_col} has type {col_type}. \"\n",
    "                    \"Attempting to cast to DATE.\"\n",
    "                )\n",
    "                df = df.with_column(\n",
    "                    self.config.timestamp_col,\n",
    "                    F.to_date(F.col(self.config.timestamp_col))\n",
    "                )\n",
    "                self.feature_df = df\n",
    "    \n",
    "    def build(self) -> FeatureView:\n",
    "        \"\"\"Build the feature view with validation and monitoring\"\"\"\n",
    "        try:\n",
    "            # Validate features first\n",
    "            self._validate_features()\n",
    "            \n",
    "            # Validate timestamp column\n",
    "            self._validate_timestamp_col(self.feature_df)\n",
    "            \n",
    "            # Create feature view\n",
    "            feature_view = FeatureView(\n",
    "                name=self.config.name,\n",
    "                entities=self.entities,\n",
    "                feature_df=self.feature_df,\n",
    "                refresh_freq=self.config.refresh.frequency,\n",
    "                timestamp_col=self.config.timestamp_col,\n",
    "                desc=self.config.description or f\"Feature view {self.config.name}\"\n",
    "            )\n",
    "            \n",
    "            # Add feature descriptions\n",
    "            feature_descriptions = {\n",
    "                name: config.description\n",
    "                for name, config in self.config.features.items()\n",
    "            }\n",
    "            feature_view = feature_view.attach_feature_desc(feature_descriptions)\n",
    "            \n",
    "            return feature_view\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error building feature view: {str(e)}\")\n",
    "            raise FeatureViewError(f\"Feature view creation failed: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def create_feature_view(\n",
    "    config: FeatureViewConfig,\n",
    "    feature_df: DataFrame,\n",
    "    entities: Union[Entity, List[Entity]],\n",
    "    collect_stats: bool = True\n",
    ") -> FeatureView:\n",
    "    \"\"\"Create a feature view with validation and monitoring\n",
    "    \n",
    "    Args:\n",
    "        config: Feature view configuration\n",
    "        feature_df: DataFrame containing feature transformations\n",
    "        entities: Entity or list of entities\n",
    "        collect_stats: Whether to collect detailed statistics\n",
    "        \n",
    "    Returns:\n",
    "        Configured FeatureView object\n",
    "        \n",
    "    Example:\n",
    "        >>> config = FeatureViewConfig(\n",
    "        ...     name=\"customer_behavior\",\n",
    "        ...     domain=\"RETAIL\",\n",
    "        ...     features={\n",
    "        ...         \"session_length\": FeatureConfig(\n",
    "        ...             name=\"session_length\",\n",
    "        ...             description=\"Session length in minutes\",\n",
    "        ...             validation=FeatureValidationConfig(\n",
    "        ...                 null_threshold=0.1,\n",
    "        ...                 range_check=True,\n",
    "        ...                 min_value=0\n",
    "        ...             )\n",
    "        ...         )\n",
    "        ...     }\n",
    "        ... )\n",
    "        >>> entity = Entity(\"CUSTOMER\", [\"customer_id\"])\n",
    "        >>> feature_view = create_feature_view(config, df, entity)\n",
    "    \"\"\"\n",
    "    return FeatureViewBuilder(config, feature_df, entities, collect_stats).build()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-16 20:56:04,651 - snowflake_feature_store - INFO - No active session found, creating new connection from environment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jdemlow/miniconda3/envs/feature-store/lib/python3.11/site-packages/pydantic/_internal/_fields.py:192: UserWarning: Field name \"schema\" in \"ConnectionConfig\" shadows an attribute in parent \"BaseModel\"\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-16 20:56:05,094 - snowflake_feature_store - INFO - Initialized connection to \"CONTAINER_DEMO_DB\".\"PUBLIC\"\n",
      "-------------------------------------------------------------------------\n",
      "|\"CUSTOMER_ID\"  |\"DATE\"      |\"AVG_SESSION_LENGTH\"  |\"TOTAL_PURCHASES\"  |\n",
      "-------------------------------------------------------------------------\n",
      "|C1             |2024-01-01  |30                    |2                  |\n",
      "|C1             |2024-01-02  |45                    |3                  |\n",
      "|C2             |2024-01-01  |15                    |1                  |\n",
      "-------------------------------------------------------------------------\n",
      "\n",
      "2025-02-16 20:56:06,339 - snowflake_feature_store - INFO - Validated feature AVG_SESSION_LENGTH (stats: {'timestamp': '2025-02-17T04:56:05.679493', 'row_count': 4, 'null_count': 0, 'null_ratio': 0.0, 'unique_count': 4, 'min_value': 15.0, 'max_value': 60.0, 'mean_value': 37.5, 'std_value': 19.364916731037084})\n",
      "2025-02-16 20:56:07,502 - snowflake_feature_store - INFO - Validated feature TOTAL_PURCHASES (stats: {'timestamp': '2025-02-17T04:56:06.580706', 'row_count': 4, 'null_count': 0, 'null_ratio': 0.0, 'unique_count': 4, 'min_value': 1.0, 'max_value': 4.0, 'mean_value': 2.5, 'std_value': 1.290994577835244})\n",
      "\n",
      "Feature View Data:\n",
      "-------------------------------------------------------------------------\n",
      "|\"CUSTOMER_ID\"  |\"DATE\"      |\"AVG_SESSION_LENGTH\"  |\"TOTAL_PURCHASES\"  |\n",
      "-------------------------------------------------------------------------\n",
      "|C1             |2024-01-01  |30                    |2                  |\n",
      "|C1             |2024-01-02  |45                    |3                  |\n",
      "|C2             |2024-01-01  |15                    |1                  |\n",
      "|C2             |2024-01-02  |60                    |4                  |\n",
      "-------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Stats for AVG_SESSION_LENGTH:\n",
      "{\n",
      "  \"timestamp\": \"2025-02-17T04:56:07.827584\",\n",
      "  \"row_count\": 4,\n",
      "  \"null_count\": 0,\n",
      "  \"null_ratio\": 0.0,\n",
      "  \"unique_count\": 4,\n",
      "  \"min_value\": 15.0,\n",
      "  \"max_value\": 60.0,\n",
      "  \"mean_value\": 37.5,\n",
      "  \"std_value\": 19.364916731037084\n",
      "}\n",
      "\n",
      "Stats for TOTAL_PURCHASES:\n",
      "{\n",
      "  \"timestamp\": \"2025-02-17T04:56:08.595868\",\n",
      "  \"row_count\": 4,\n",
      "  \"null_count\": 0,\n",
      "  \"null_ratio\": 0.0,\n",
      "  \"unique_count\": 4,\n",
      "  \"min_value\": 1.0,\n",
      "  \"max_value\": 4.0,\n",
      "  \"mean_value\": 2.5,\n",
      "  \"std_value\": 1.290994577835244\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "from snowflake_feature_store.feature_view import *\n",
    "from snowflake_feature_store.config import *\n",
    "from snowflake_feature_store.connection import get_connection\n",
    "from snowflake_feature_store.transforms import *\n",
    "\n",
    "from snowflake.ml.feature_store import Entity\n",
    "import snowflake.snowpark.functions as F\n",
    "\n",
    "\n",
    "# Get connection\n",
    "conn = get_connection()\n",
    "session = conn.session\n",
    "\n",
    "# Create sample data\n",
    "data = [\n",
    "    ['C1', '2024-01-01', 30, 2],\n",
    "    ['C1', '2024-01-02', 45, 3],\n",
    "    ['C2', '2024-01-01', 15, 1],\n",
    "    ['C2', '2024-01-02', 60, 4]\n",
    "]\n",
    "\n",
    "# Create feature DataFrame\n",
    "feature_df = session.create_dataframe(\n",
    "    data,\n",
    "    schema=['CUSTOMER_ID', 'DATE', 'AVG_SESSION_LENGTH', 'TOTAL_PURCHASES']\n",
    ")\n",
    "# Create customer entity\n",
    "customer_entity = Entity(\n",
    "    name=\"CUSTOMER\",\n",
    "    join_keys=[\"CUSTOMER_ID\"],\n",
    "    desc=\"Customer entity for retail domain\"\n",
    ")\n",
    "\n",
    "# Create feature configurations\n",
    "feature_configs = {\n",
    "    \"AVG_SESSION_LENGTH\": FeatureConfig(\n",
    "        name=\"AVG_SESSION_LENGTH\",\n",
    "        description=\"Average session length in minutes\",\n",
    "        validation=FeatureValidationConfig(\n",
    "            null_threshold=0.1,\n",
    "            range_check=True,\n",
    "            min_value=0\n",
    "        )\n",
    "    ),\n",
    "    \"TOTAL_PURCHASES\": FeatureConfig(\n",
    "        name=\"TOTAL_PURCHASES\",\n",
    "        description=\"Total number of purchases\",\n",
    "        validation=FeatureValidationConfig(\n",
    "            null_threshold=0.05,\n",
    "            range_check=True,\n",
    "            min_value=0\n",
    "        )\n",
    "    )\n",
    "}\n",
    "\n",
    "# Create feature view config\n",
    "config = FeatureViewConfig(\n",
    "    name=\"customer_behavior\",\n",
    "    domain=\"RETAIL\",\n",
    "    entity=\"CUSTOMER\",\n",
    "    feature_type=\"BEHAVIOR\",\n",
    "    refresh=RefreshConfig(frequency=\"1 day\"),\n",
    "    features=feature_configs,\n",
    "    description=\"Customer behavior features\"\n",
    ")\n",
    "\n",
    "# Create feature view with monitoring\n",
    "feature_view = create_feature_view(\n",
    "    config, \n",
    "    feature_df, \n",
    "    customer_entity,\n",
    "    collect_stats=True\n",
    ")\n",
    "\n",
    "# Show the feature view data\n",
    "print(\"\\nFeature View Data:\")\n",
    "feature_view.feature_df.show()\n",
    "\n",
    "# Access feature monitors\n",
    "builder = FeatureViewBuilder(config, feature_df, customer_entity)\n",
    "for feature_name, monitor in builder.monitors.items():\n",
    "    stats = monitor.compute_stats(feature_df, feature_name)\n",
    "    print(f\"\\nStats for {feature_name}:\")\n",
    "    print(json.dumps(stats.model_dump(), indent=2))\n",
    "\n",
    "# Create transform configurations\n",
    "transform_config = TransformConfig(\n",
    "    name=\"session_metrics\",\n",
    "    null_threshold=0.05,\n",
    "    expected_types=['DECIMAL', 'DOUBLE', 'NUMBER']  # Accept any numeric type\n",
    ")\n",
    "\n",
    "# Apply transforms to create features\n",
    "transforms = [\n",
    "    moving_agg(\n",
    "        cols='AVG_SESSION_LENGTH',\n",
    "        window_sizes=[7],  # 7-day window\n",
    "        agg_funcs=['AVG', 'MAX'],\n",
    "        partition_by=['CUSTOMER_ID'],\n",
    "        order_by=['DATE'],\n",
    "        config=transform_config\n",
    "    )\n",
    "]\n",
    "\n",
    "# Apply transforms to create feature DataFrame\n",
    "feature_df = apply_transforms(feature_df, transforms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "feature-store",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
