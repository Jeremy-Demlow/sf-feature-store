{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NAME\n",
    "\n",
    "> Fill in a module description here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jdemlow/miniconda3/envs/feature-store/lib/python3.11/site-packages/pydantic/_internal/_fields.py:192: UserWarning: Field name \"schema\" in \"ConnectionConfig\" shadows an attribute in parent \"BaseModel\"\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "from __future__ import annotations\n",
    "from typing import List, Optional, Dict, Union, Set\n",
    "from dataclasses import dataclass\n",
    "import uuid\n",
    "from contextlib import contextmanager\n",
    "from datetime import datetime, timezone\n",
    "import networkx as nx\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "from snowflake.ml.feature_store import (\n",
    "    FeatureStore, Entity, FeatureView, CreationMode\n",
    ")\n",
    "from snowflake.snowpark import DataFrame\n",
    "import snowflake.snowpark.functions as F\n",
    "\n",
    "# Import our modules\n",
    "from snowflake_feature_store.connection import SnowflakeConnection\n",
    "from snowflake_feature_store.feature_view import (\n",
    "    FeatureViewBuilder, create_feature_view, \n",
    "    FeatureStats, FeatureMonitor\n",
    ")\n",
    "from snowflake_feature_store.transforms import (\n",
    "    Transform, apply_transforms, TransformConfig,\n",
    "    moving_agg, fill_na\n",
    ")\n",
    "from snowflake_feature_store.config import (\n",
    "    FeatureViewConfig, FeatureConfig, \n",
    "    RefreshConfig, FeatureValidationConfig\n",
    ")\n",
    "from snowflake_feature_store.exceptions import (\n",
    "    FeatureStoreException, EntityError, \n",
    "    FeatureViewError, ValidationError\n",
    ")\n",
    "from snowflake_feature_store.logging import logger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class FeatureStoreCallback:\n",
    "    \"\"\"Protocol for feature store callbacks\"\"\"\n",
    "    def on_feature_view_create(\n",
    "        self, name: str, df: DataFrame, stats: Dict[str, FeatureStats]\n",
    "    ) -> None: ...\n",
    "    \n",
    "    def on_entity_create(self, name: str, keys: List[str]) -> None: ...\n",
    "    def on_error(self, error: str) -> None: ...\n",
    "    def on_drift_detected(\n",
    "        self, feature_view: str, feature: str, metrics: Dict[str, float]\n",
    "    ) -> None: ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class MetricsCallback(FeatureStoreCallback):\n",
    "    \"\"\"Callback that logs metrics and statistics\"\"\"\n",
    "    \n",
    "    def __init__(self, metrics_path: Optional[Path] = None):\n",
    "        self.metrics_path = metrics_path\n",
    "        if metrics_path:\n",
    "            metrics_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    def _save_metrics(self, name: str, data: Dict) -> None:\n",
    "        \"\"\"Save metrics to JSON file if path specified\"\"\"\n",
    "        if self.metrics_path:\n",
    "            timestamp = datetime.now(timezone.utc).strftime('%Y%m%d_%H%M%S')\n",
    "            file_path = self.metrics_path / f\"{name}_{timestamp}.json\"\n",
    "            with open(file_path, 'w') as f:\n",
    "                json.dump(data, f, indent=2)\n",
    "    \n",
    "    def on_feature_view_create(\n",
    "        self, name: str, df: DataFrame, stats: Dict[str, FeatureStats]\n",
    "    ) -> None:\n",
    "        \"\"\"Log feature view creation with statistics\"\"\"\n",
    "        logger.info(f\"Created feature view: {name} with {len(df.columns)} features\")\n",
    "        \n",
    "        # Log detailed stats\n",
    "        stats_data = {\n",
    "            'name': name,\n",
    "            'timestamp': datetime.now(timezone.utc).isoformat(),\n",
    "            'feature_stats': {\n",
    "                fname: fstats.model_dump()\n",
    "                for fname, fstats in stats.items()\n",
    "            }\n",
    "        }\n",
    "        self._save_metrics(f\"{name}_creation\", stats_data)\n",
    "        \n",
    "    def on_entity_create(self, name: str, keys: List[str]) -> None:\n",
    "        \"\"\"Log entity creation\"\"\"\n",
    "        logger.info(f\"Created entity: {name} with keys: {keys}\")\n",
    "        \n",
    "    def on_error(self, error: str) -> None:\n",
    "        \"\"\"Log errors\"\"\"\n",
    "        logger.error(f\"Error: {error}\")\n",
    "        \n",
    "    def on_drift_detected(\n",
    "        self, feature_view: str, feature: str, metrics: Dict[str, float]\n",
    "    ) -> None:\n",
    "        \"\"\"Log feature drift detection\"\"\"\n",
    "        logger.warning(\n",
    "            f\"Drift detected in {feature_view}.{feature}: {metrics}\"\n",
    "        )\n",
    "        self._save_metrics(\n",
    "            f\"{feature_view}_{feature}_drift\",\n",
    "            {\n",
    "                'feature_view': feature_view,\n",
    "                'feature': feature,\n",
    "                'timestamp': datetime.now(timezone.utc).isoformat(),\n",
    "                'metrics': metrics\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class FeatureStoreManager:\n",
    "    \"\"\"Manages feature store operations with monitoring and dependency tracking\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        connection: SnowflakeConnection,\n",
    "        callbacks: Optional[List[FeatureStoreCallback]] = None,\n",
    "        metrics_path: Optional[Union[str, Path]] = None,\n",
    "        overwrite: bool = False\n",
    "    ):\n",
    "        \"\"\"Initialize feature store manager\n",
    "        \n",
    "        Args:\n",
    "            connection: Snowflake connection\n",
    "            callbacks: Optional callbacks for monitoring\n",
    "            metrics_path: Optional path to save metrics\n",
    "            overwrite: Whether to overwrite existing features\n",
    "        \"\"\"\n",
    "        self.connection = connection\n",
    "        self.feature_store = FeatureStore(\n",
    "            session=self.connection.session,\n",
    "            database=self.connection.database,\n",
    "            name=self.connection.schema,\n",
    "            default_warehouse=self.connection.warehouse,\n",
    "            creation_mode=CreationMode.CREATE_IF_NOT_EXIST\n",
    "        )\n",
    "        \n",
    "        # Initialize storage\n",
    "        self.entities: Dict[str, Entity] = {}\n",
    "        self.feature_views: Dict[str, FeatureView] = {}\n",
    "        self.feature_configs: Dict[str, FeatureViewConfig] = {}\n",
    "        self.feature_stats: Dict[str, Dict[str, FeatureStats]] = {}\n",
    "        self.feature_transforms: Dict[str, List[Transform]] = {}  # Add this line\n",
    "        self.dependencies = nx.DiGraph()\n",
    "        \n",
    "        # Setup callbacks\n",
    "        self.callbacks = callbacks or []\n",
    "        if metrics_path:\n",
    "            self.callbacks.append(\n",
    "                MetricsCallback(Path(metrics_path))\n",
    "            )\n",
    "        \n",
    "        self.overwrite = overwrite\n",
    "        logger.info(\"FeatureStoreManager initialized\")\n",
    "            \n",
    "    def add_entity(\n",
    "        self, \n",
    "        name: str, \n",
    "        join_keys: List[str], \n",
    "        description: Optional[str] = None,\n",
    "        tags: Optional[Dict[str, str]] = None\n",
    "    ) -> FeatureStoreManager:\n",
    "        \"\"\"Add entity to feature store\n",
    "        \n",
    "        Args:\n",
    "            name: Entity name\n",
    "            join_keys: Keys used for joining\n",
    "            description: Optional description\n",
    "            tags: Optional metadata tags\n",
    "            \n",
    "        Returns:\n",
    "            Self for method chaining\n",
    "        \"\"\"\n",
    "        try:\n",
    "            entity = Entity(\n",
    "                name=name,\n",
    "                join_keys=join_keys,\n",
    "                desc=description or f\"Entity {name}\"\n",
    "            )\n",
    "            \n",
    "            # Register entity\n",
    "            self.feature_store.register_entity(entity)\n",
    "            self.entities[name] = entity\n",
    "            \n",
    "            # Add tags if provided\n",
    "            if tags:\n",
    "                for key, value in tags.items():\n",
    "                    self.feature_store.set_tag(entity, key, value)\n",
    "            \n",
    "            for cb in self.callbacks:\n",
    "                cb.on_entity_create(name, join_keys)\n",
    "                \n",
    "        except Exception as e:\n",
    "            error_msg = f\"Error creating entity {name}: {str(e)}\"\n",
    "            for cb in self.callbacks:\n",
    "                cb.on_error(error_msg)\n",
    "            raise EntityError(error_msg)\n",
    "            \n",
    "        return self\n",
    "\n",
    "    \n",
    "    def add_feature_view(\n",
    "        self,\n",
    "        config: FeatureViewConfig,\n",
    "        df: DataFrame,\n",
    "        entity_name: str,\n",
    "        transforms: Optional[List[Transform]] = None,\n",
    "        collect_stats: bool = True\n",
    "    ) -> FeatureView:\n",
    "        \"\"\"Add feature view to feature store with monitoring\n",
    "        \n",
    "        Args:\n",
    "            config: Feature view configuration\n",
    "            df: Source DataFrame\n",
    "            entity_name: Entity name\n",
    "            transforms: Optional transformations to apply\n",
    "            collect_stats: Whether to collect feature statistics\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Validate schema only (no execution)\n",
    "            self._validate_schema(df)\n",
    "            \n",
    "            # Apply transforms if provided\n",
    "            if transforms:\n",
    "                self.feature_transforms[config.name] = transforms\n",
    "                df = apply_transforms(df, transforms)\n",
    "                \n",
    "            # Get entity\n",
    "            entity = self.entities.get(entity_name)\n",
    "            if not entity:\n",
    "                raise EntityError(f\"Entity {entity_name} not found\")\n",
    "                \n",
    "            # Create feature view\n",
    "            feature_view = create_feature_view(\n",
    "                config=config,\n",
    "                feature_df=df,\n",
    "                entities=entity,\n",
    "                collect_stats=collect_stats\n",
    "            )\n",
    "            \n",
    "            # Register feature view\n",
    "            registered_view = self.feature_store.register_feature_view(\n",
    "                feature_view=feature_view,\n",
    "                version=config.version,\n",
    "                block=True,\n",
    "                overwrite=self.overwrite\n",
    "            )\n",
    "            \n",
    "            # Store view, config, and compute initial stats\n",
    "            self.feature_views[config.name] = registered_view\n",
    "            self.feature_configs[config.name] = config\n",
    "\n",
    "            # Update dependency graph\n",
    "            self._update_dependencies(config)\n",
    "            \n",
    "            # Compute and store statistics\n",
    "            builder = FeatureViewBuilder(config, df, entity)\n",
    "            stats = {\n",
    "                name: monitor.compute_stats(df, name)\n",
    "                for name, monitor in builder.monitors.items()\n",
    "            }\n",
    "            self.feature_stats[config.name] = stats\n",
    "            \n",
    "            # Notify callbacks\n",
    "            for cb in self.callbacks:\n",
    "                cb.on_feature_view_create(config.name, df, stats)\n",
    "                \n",
    "            return registered_view\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = f\"Error creating feature view {config.name}: {str(e)}\"\n",
    "            for cb in self.callbacks:\n",
    "                cb.on_error(error_msg)\n",
    "            raise FeatureViewError(error_msg)\n",
    "    \n",
    "    def check_feature_drift(\n",
    "        self,\n",
    "        feature_view_name: str,\n",
    "        new_data: DataFrame\n",
    "    ) -> Dict[str, Dict[str, float]]:\n",
    "        \"\"\"Check for feature drift in new data\"\"\"\n",
    "        drift_results = {}\n",
    "        \n",
    "        try:\n",
    "            # Get stored stats\n",
    "            stored_stats = self.feature_stats.get(feature_view_name)\n",
    "            if not stored_stats:\n",
    "                raise FeatureViewError(\n",
    "                    f\"No baseline stats for feature view {feature_view_name}\"\n",
    "                )\n",
    "            \n",
    "            # Get feature view configuration\n",
    "            config = self.feature_configs.get(feature_view_name)\n",
    "            if not config:\n",
    "                raise FeatureViewError(f\"Feature view config {feature_view_name} not found\")\n",
    "                \n",
    "            # Apply stored transforms to new data\n",
    "            transforms = self.feature_transforms.get(feature_view_name, [])\n",
    "            if transforms:\n",
    "                logger.info(f\"Applying {len(transforms)} transforms to new data\")\n",
    "                new_data_with_features = apply_transforms(new_data, transforms)\n",
    "            else:\n",
    "                logger.info(\"No transforms to apply\")\n",
    "                new_data_with_features = new_data\n",
    "            \n",
    "            # Check drift for each feature\n",
    "            for feature_name, baseline_stats in stored_stats.items():\n",
    "                try:\n",
    "                    # Get the original feature config if it exists\n",
    "                    feature_config = config.features.get(feature_name)\n",
    "                    \n",
    "                    # Create monitor with existing config or default\n",
    "                    monitor = FeatureMonitor(\n",
    "                        feature_config or FeatureConfig(\n",
    "                            name=feature_name,\n",
    "                            description=f\"Temporary monitor for {feature_name}\"\n",
    "                        ),\n",
    "                        collect_detailed_stats=True\n",
    "                    )\n",
    "                    \n",
    "                    # Compute current stats and detect drift\n",
    "                    current_stats = monitor.compute_stats(new_data_with_features, feature_name)\n",
    "                    monitor.set_baseline(baseline_stats)\n",
    "                    drift_metrics = monitor.detect_drift(current_stats)\n",
    "                    \n",
    "                    # Check if drift is significant\n",
    "                    if any(abs(v) > 0.1 for v in drift_metrics.values()):\n",
    "                        drift_results[feature_name] = drift_metrics\n",
    "                        # Notify callbacks\n",
    "                        for cb in self.callbacks:\n",
    "                            cb.on_drift_detected(\n",
    "                                feature_view_name, feature_name, drift_metrics\n",
    "                            )\n",
    "                            \n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Skipping drift detection for {feature_name}: {str(e)}\")\n",
    "                    continue\n",
    "            \n",
    "            return drift_results\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = f\"Error checking drift for {feature_view_name}: {str(e)}\"\n",
    "            for cb in self.callbacks:\n",
    "                cb.on_error(error_msg)\n",
    "            raise FeatureViewError(error_msg)\n",
    "        \n",
    "    def get_feature_dependencies(self, feature_view_name: str) -> Set[str]:\n",
    "        \"\"\"Get dependencies for a feature view\"\"\"\n",
    "        try:\n",
    "            return nx.descendants(self.dependencies, feature_view_name)\n",
    "        except Exception as e:\n",
    "            raise FeatureViewError(\n",
    "                f\"Error getting dependencies for {feature_view_name}: {str(e)}\"\n",
    "            )\n",
    "    \n",
    "    def _update_dependencies(self, config: FeatureViewConfig) -> None:\n",
    "        \"\"\"Update dependency graph with new feature view\"\"\"\n",
    "        try:\n",
    "            # Add the feature view as a node\n",
    "            self.dependencies.add_node(config.name)\n",
    "            \n",
    "            # Track dependencies from transforms\n",
    "            for feature_name, feature_config in config.features.items():\n",
    "                # Add each feature as a node\n",
    "                feature_node = f\"{config.name}.{feature_name}\"\n",
    "                self.dependencies.add_node(feature_node)\n",
    "                \n",
    "                # Add edge from feature view to feature\n",
    "                self.dependencies.add_edge(config.name, feature_node)\n",
    "                \n",
    "                # Add dependencies between features\n",
    "                if feature_config.dependencies:\n",
    "                    for dep in feature_config.dependencies:\n",
    "                        self.dependencies.add_edge(feature_node, dep)\n",
    "                        \n",
    "            logger.debug(\n",
    "                f\"Updated dependencies for {config.name}: \"\n",
    "                f\"{list(self.dependencies.edges)}\"\n",
    "            )\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error updating dependencies: {str(e)}\")\n",
    "\n",
    "    def get_feature_dependencies(self, feature_view_name: str) -> Set[str]:\n",
    "        \"\"\"Get dependencies for a feature view\n",
    "        \n",
    "        Args:\n",
    "            feature_view_name: Name of the feature view\n",
    "            \n",
    "        Returns:\n",
    "            Set of dependent feature names\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Get all descendants (dependencies) from the graph\n",
    "            deps = nx.descendants(self.dependencies, feature_view_name)\n",
    "            \n",
    "            # Filter out internal feature nodes\n",
    "            feature_deps = {\n",
    "                dep.split('.')[0] for dep in deps \n",
    "                if '.' in dep  # Only include actual feature views\n",
    "            }\n",
    "            \n",
    "            logger.info(\n",
    "                f\"Dependencies for {feature_view_name}: {feature_deps}\"\n",
    "            )\n",
    "            return feature_deps\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise FeatureViewError(\n",
    "                f\"Error getting dependencies for {feature_view_name}: {str(e)}\"\n",
    "            )\n",
    "\n",
    "    \n",
    "    def _validate_schema(self, df: DataFrame) -> None:\n",
    "        \"\"\"Validate DataFrame schema without execution\"\"\"\n",
    "        if not df.schema:\n",
    "            raise ValidationError(\"DataFrame has no schema\")\n",
    "            \n",
    "    def get_features(\n",
    "        self,\n",
    "        spine_df: DataFrame,\n",
    "        feature_views: List[Union[str, FeatureView, FeatureViewConfig]],\n",
    "        label_cols: Optional[List[str]] = None,\n",
    "        dataset_name: Optional[str] = None,\n",
    "        spine_timestamp_col: Optional[str] = None,\n",
    "        **kwargs\n",
    "    ) -> DataFrame:\n",
    "        \"\"\"Get features for training or inference\"\"\"\n",
    "        try:\n",
    "            # Debug information\n",
    "            logger.info(f\"Spine DataFrame columns: {spine_df.columns}\")\n",
    "            logger.info(f\"Spine DataFrame schema: {spine_df.schema}\")\n",
    "\n",
    "            views = []\n",
    "            for fv in feature_views:\n",
    "                if isinstance(fv, FeatureView):\n",
    "                    # Direct FeatureView object\n",
    "                    views.append(fv)\n",
    "                elif isinstance(fv, FeatureViewConfig):\n",
    "                    # Config object - get view by name/version\n",
    "                    view = self.feature_store.get_feature_view(\n",
    "                        fv.name, version=fv.version\n",
    "                    )\n",
    "                    views.append(view)\n",
    "                elif isinstance(fv, str):\n",
    "                    # String reference \"name/version\"\n",
    "                    name, version = fv.split('/')\n",
    "                    view = self.feature_store.get_feature_view(name, version)\n",
    "                    views.append(view)\n",
    "                else:\n",
    "                    raise ValueError(f\"Unsupported feature view type: {type(fv)}\")\n",
    "            \n",
    "            if dataset_name is None:\n",
    "                timestamp = datetime.now(timezone.utc).strftime('%Y%m%d_%H%M%S')\n",
    "                unique_id = str(uuid.uuid4())[:8]\n",
    "                dataset_name = f\"DATASET_{timestamp}_{unique_id}\"\n",
    "\n",
    "            # If label_cols are provided, ensure they're properly quoted\n",
    "            if label_cols:\n",
    "                label_cols = [f'\"{col}\"' for col in label_cols]\n",
    "                \n",
    "            # Ensure timestamp col is quoted\n",
    "            if spine_timestamp_col:\n",
    "                spine_timestamp_col = f'\"{spine_timestamp_col}\"'\n",
    "\n",
    "            logger.info(f\"Generating dataset with name: {dataset_name}\")\n",
    "            logger.info(f\"Label columns: {label_cols}\")\n",
    "            logger.info(f\"Timestamp column: {spine_timestamp_col}\")\n",
    "                \n",
    "            dataset = self.feature_store.generate_dataset(\n",
    "                name=dataset_name,\n",
    "                spine_df=spine_df,\n",
    "                features=views,\n",
    "                spine_label_cols=label_cols,\n",
    "                spine_timestamp_col=spine_timestamp_col,\n",
    "                **kwargs\n",
    "            )\n",
    "            \n",
    "            return dataset.read.to_snowpark_dataframe()\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = f\"Error generating dataset: {str(e)}\"\n",
    "            for cb in self.callbacks:\n",
    "                cb.on_error(error_msg)\n",
    "            raise FeatureStoreException(error_msg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@contextmanager\n",
    "def feature_store_session(\n",
    "    connection: SnowflakeConnection, \n",
    "    *,  # Force keyword arguments\n",
    "    schema_name: Optional[str] = None,\n",
    "    metrics_path: Optional[Union[str, Path]] = None,  # Changed type hint\n",
    "    cleanup: bool = True\n",
    "):\n",
    "    \"\"\"Context manager for feature store operations\n",
    "    \n",
    "    Args:\n",
    "        connection: Snowflake connection\n",
    "        schema_name: Optional schema name (keyword only)\n",
    "        metrics_path: Optional path to save metrics (keyword only)\n",
    "        cleanup: Whether to cleanup schema after use (keyword only)\n",
    "    \"\"\"\n",
    "    schema = schema_name or (\n",
    "        f\"FEATURE_STORE_{datetime.now(timezone.utc).strftime('%Y%m%d_%H%M%S')}\"\n",
    "        f\"_{uuid.uuid4().hex[:8]}\"\n",
    "    )\n",
    "    original_schema = connection.schema\n",
    "    \n",
    "    try:\n",
    "        # Create schema\n",
    "        connection.session.sql(\n",
    "            f\"CREATE SCHEMA IF NOT EXISTS {connection.database}.{schema}\"\n",
    "        ).collect()\n",
    "        \n",
    "        # Set schema as current\n",
    "        connection.session.sql(\n",
    "            f\"USE SCHEMA {connection.database}.{schema}\"\n",
    "        ).collect()\n",
    "        connection.schema = schema\n",
    "        \n",
    "        # Create and yield manager with metrics path\n",
    "        manager = FeatureStoreManager(\n",
    "            connection=connection,\n",
    "            metrics_path=metrics_path,\n",
    "            overwrite=True\n",
    "        )\n",
    "        yield manager\n",
    "        \n",
    "    finally:\n",
    "        if cleanup:\n",
    "            try:\n",
    "                # Cleanup schema and all objects\n",
    "                connection.session.sql(\n",
    "                    f\"DROP SCHEMA IF EXISTS {connection.database}.{schema} CASCADE\"\n",
    "                ).collect()\n",
    "                logger.info(f\"Cleaned up schema {schema}\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Cleanup failed: {str(e)}\")\n",
    "            \n",
    "            # Restore original schema\n",
    "            try:\n",
    "                connection.session.sql(\n",
    "                    f\"USE SCHEMA {connection.database}.{original_schema}\"\n",
    "                ).collect()\n",
    "                connection.schema = original_schema\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Failed to restore original schema: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-17 18:13:57,583 - snowflake_feature_store - INFO - No active session found, creating new connection from environment\n",
      "2025-02-17 18:13:58,557 - snowflake_feature_store - INFO - Initialized connection to \"DATASCIENCE\".\"FEATURE_STORE_DEMO\"\n",
      "2025-02-17 18:14:00,880 - snowflake_feature_store - INFO - FeatureStoreManager initialized\n",
      "\n",
      "Initial DataFrame Schema:\n",
      "CUSTOMER_ID: StringType()\n",
      "DATE: DateType()\n",
      "AMOUNT: DoubleType()\n",
      "TRANSACTIONS: LongType()\n",
      "SESSION_LENGTH: DoubleType()\n",
      "\n",
      "Sample Data:\n",
      "-----------------------------------------------------------------------------\n",
      "|\"CUSTOMER_ID\"  |\"DATE\"      |\"AMOUNT\"  |\"TRANSACTIONS\"  |\"SESSION_LENGTH\"  |\n",
      "-----------------------------------------------------------------------------\n",
      "|C1             |2024-01-01  |100.0     |2               |NULL              |\n",
      "|C1             |2024-01-02  |150.0     |3               |30.5              |\n",
      "|C2             |2024-01-01  |75.0      |1               |NULL              |\n",
      "|C2             |2024-01-02  |200.0     |4               |45.2              |\n",
      "-----------------------------------------------------------------------------\n",
      "\n",
      "2025-02-17 18:14:04,434 - snowflake_feature_store - INFO - Created entity: CUSTOMER with keys: ['CUSTOMER_ID']\n",
      "2025-02-17 18:14:07,927 - snowflake_feature_store - INFO - Validated feature AMOUNT (stats: {'timestamp': '2025-02-18T02:14:06.972199', 'row_count': 4, 'null_count': 0, 'null_ratio': 0.0, 'unique_count': 4, 'min_value': 75.0, 'max_value': 200.0, 'mean_value': 131.25, 'std_value': 55.433894565208625})\n",
      "2025-02-17 18:14:09,493 - snowflake_feature_store - INFO - Validated feature TRANSACTIONS (stats: {'timestamp': '2025-02-18T02:14:08.530751', 'row_count': 4, 'null_count': 0, 'null_ratio': 0.0, 'unique_count': 4, 'min_value': 1.0, 'max_value': 4.0, 'mean_value': 2.5, 'std_value': 1.290994577835244})\n",
      "2025-02-17 18:14:10,965 - snowflake_feature_store - INFO - Validated feature SESSION_LENGTH (stats: {'timestamp': '2025-02-18T02:14:09.888006', 'row_count': 4, 'null_count': 0, 'null_ratio': 0.0, 'unique_count': 3, 'min_value': 0.0, 'max_value': 45.2, 'mean_value': 18.925, 'std_value': 22.661770304487096})\n",
      "2025-02-17 18:14:12,333 - snowflake_feature_store - INFO - Validated feature SUM_AMOUNT_2 (stats: {'timestamp': '2025-02-18T02:14:11.341565', 'row_count': 4, 'null_count': 0, 'null_ratio': 0.0, 'unique_count': 4, 'min_value': 75.0, 'max_value': 275.0, 'mean_value': 175.0, 'std_value': 102.06207261596575})\n",
      "2025-02-17 18:14:13,732 - snowflake_feature_store - INFO - Validated feature AVG_AMOUNT_2 (stats: {'timestamp': '2025-02-18T02:14:12.745605', 'row_count': 4, 'null_count': 0, 'null_ratio': 0.0, 'unique_count': 4, 'min_value': 75.0, 'max_value': 137.5, 'mean_value': 109.375, 'std_value': 27.716947282604313})\n",
      "2025-02-17 18:14:27,401 - snowflake_feature_store - INFO - Created feature view: customer_behavior with 7 features\n",
      "2025-02-17 18:14:28,234 - snowflake_feature_store - INFO - Applying 2 transforms to new data\n",
      "2025-02-17 18:14:32,008 - snowflake_feature_store - WARNING - Drift detected in customer_behavior.AMOUNT: {'null_ratio_change': 0.0, 'mean_shift': 58.75, 'std_ratio': 2.806288338230435}\n",
      "2025-02-17 18:14:33,748 - snowflake_feature_store - WARNING - Drift detected in customer_behavior.TRANSACTIONS: {'null_ratio_change': 0.0, 'mean_shift': 0.5, 'std_ratio': 2.1908900109316742}\n",
      "2025-02-17 18:14:35,371 - snowflake_feature_store - WARNING - Drift detected in customer_behavior.SESSION_LENGTH: {'null_ratio_change': 0.0, 'mean_shift': 18.825, 'std_ratio': 1.3885169313789645}\n",
      "2025-02-17 18:14:37,070 - snowflake_feature_store - WARNING - Drift detected in customer_behavior.SUM_AMOUNT_2: {'null_ratio_change': 0.0, 'mean_shift': 15.0, 'std_ratio': 1.5242047106606122}\n",
      "2025-02-17 18:14:38,623 - snowflake_feature_store - WARNING - Drift detected in customer_behavior.AVG_AMOUNT_2: {'null_ratio_change': 0.0, 'mean_shift': 80.625, 'std_ratio': 5.61257667646087}\n",
      "\n",
      "Drift Detection Results:\n",
      "\n",
      "AMOUNT:\n",
      "  null_ratio_change: 0.000\n",
      "  mean_shift: 58.750\n",
      "  std_ratio: 2.806\n",
      "\n",
      "TRANSACTIONS:\n",
      "  null_ratio_change: 0.000\n",
      "  mean_shift: 0.500\n",
      "  std_ratio: 2.191\n",
      "\n",
      "SESSION_LENGTH:\n",
      "  null_ratio_change: 0.000\n",
      "  mean_shift: 18.825\n",
      "  std_ratio: 1.389\n",
      "\n",
      "SUM_AMOUNT_2:\n",
      "  null_ratio_change: 0.000\n",
      "  mean_shift: 15.000\n",
      "  std_ratio: 1.524\n",
      "\n",
      "AVG_AMOUNT_2:\n",
      "  null_ratio_change: 0.000\n",
      "  mean_shift: 80.625\n",
      "  std_ratio: 5.613\n",
      "2025-02-17 18:14:38,636 - snowflake_feature_store - INFO - Dependencies for customer_behavior: {'customer_behavior'}\n",
      "\n",
      "Feature Dependencies: {'customer_behavior'}\n",
      "\n",
      "Entity join keys:\n",
      "Entity CUSTOMER: ['CUSTOMER_ID']\n",
      "\n",
      "Spine DataFrame columns:\n",
      "['CUSTOMER_ID', 'DATE']\n",
      "\n",
      "Spine DataFrame schema:\n",
      "CUSTOMER_ID: StringType()\n",
      "DATE: DateType()\n",
      "2025-02-17 18:14:38,805 - snowflake_feature_store - INFO - Spine DataFrame columns: ['CUSTOMER_ID', 'DATE']\n",
      "2025-02-17 18:14:38,806 - snowflake_feature_store - INFO - Spine DataFrame schema: StructType([StructField('CUSTOMER_ID', StringType(), nullable=True), StructField('DATE', DateType(), nullable=True)])\n",
      "2025-02-17 18:14:38,976 - snowflake_feature_store - INFO - Modified spine DataFrame columns: ['CUSTOMER_ID', 'DATE']\n",
      "2025-02-17 18:14:41,056 - snowflake_feature_store - INFO - Feature view customer_behavior entities: ['CUSTOMER']\n",
      "2025-02-17 18:14:41,057 - snowflake_feature_store - INFO - Feature view customer_behavior join keys: [['CUSTOMER_ID']]\n",
      "2025-02-17 18:14:41,058 - snowflake_feature_store - INFO - Generating dataset with name: DATASET_20250218_021441_49154030\n",
      "2025-02-17 18:14:41,059 - snowflake_feature_store - INFO - Label columns: ['\"TRANSACTIONS\"']\n",
      "2025-02-17 18:14:41,059 - snowflake_feature_store - INFO - Timestamp column: \"DATE\"\n",
      "\n",
      "Training Data Sample:\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "|\"CUSTOMER_ID\"  |\"DATE\"      |\"AMOUNT\"  |\"TRANSACTIONS\"  |\"SESSION_LENGTH\"   |\"SUM_AMOUNT_2\"  |\"AVG_AMOUNT_2\"  |\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "|C2             |2024-01-01  |75.0      |1               |0.0                |75.0            |75.0            |\n",
      "|C2             |2024-01-02  |200.0     |4               |45.20000076293945  |275.0           |137.5           |\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Feature Statistics:\n",
      "\n",
      "AMOUNT:\n",
      "Timestamp: 2025-02-18T02:14:19.515701\n",
      "Row count: 4\n",
      "Null count: 0 (0.0%)\n",
      "Unique values: 4\n",
      "Min value: 75.00\n",
      "Max value: 200.00\n",
      "Mean value: 131.25\n",
      "Std dev: 55.43\n",
      "\n",
      "TRANSACTIONS:\n",
      "Timestamp: 2025-02-18T02:14:21.132739\n",
      "Row count: 4\n",
      "Null count: 0 (0.0%)\n",
      "Unique values: 4\n",
      "Min value: 1.00\n",
      "Max value: 4.00\n",
      "Mean value: 2.50\n",
      "Std dev: 1.29\n",
      "\n",
      "SESSION_LENGTH:\n",
      "Timestamp: 2025-02-18T02:14:22.579705\n",
      "Row count: 4\n",
      "Null count: 0 (0.0%)\n",
      "Unique values: 3\n",
      "Min value: 0.00\n",
      "Max value: 45.20\n",
      "Mean value: 18.93\n",
      "Std dev: 22.66\n",
      "\n",
      "SUM_AMOUNT_2:\n",
      "Timestamp: 2025-02-18T02:14:24.172372\n",
      "Row count: 4\n",
      "Null count: 0 (0.0%)\n",
      "Unique values: 4\n",
      "Min value: 75.00\n",
      "Max value: 275.00\n",
      "Mean value: 175.00\n",
      "Std dev: 102.06\n",
      "\n",
      "AVG_AMOUNT_2:\n",
      "Timestamp: 2025-02-18T02:14:26.202853\n",
      "Row count: 4\n",
      "Null count: 0 (0.0%)\n",
      "Unique values: 4\n",
      "Min value: 75.00\n",
      "Max value: 137.50\n",
      "Mean value: 109.38\n",
      "Std dev: 27.72\n",
      "\n",
      "Training Data Schema:\n",
      "CUSTOMER_ID: StringType()\n",
      "DATE: DateType()\n",
      "AMOUNT: DoubleType()\n",
      "TRANSACTIONS: LongType()\n",
      "SESSION_LENGTH: DoubleType()\n",
      "SUM_AMOUNT_2: DoubleType()\n",
      "AVG_AMOUNT_2: DoubleType()\n",
      "2025-02-17 18:14:46,286 - snowflake_feature_store - INFO - Cleaned up schema FEATURE_STORE_20250218_021358_3ca92fd7\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "from snowflake_feature_store.connection import get_connection\n",
    "from snowflake_feature_store.config import (\n",
    "    FeatureViewConfig, FeatureConfig, FeatureValidationConfig, RefreshConfig\n",
    ")\n",
    "from snowflake_feature_store.transforms import TransformConfig, moving_agg, fill_na\n",
    "from snowflake_feature_store.manager import feature_store_session\n",
    "import snowflake.snowpark.functions as F\n",
    "from datetime import datetime\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "\n",
    "# Create a temporary directory for metrics\n",
    "metrics_dir = Path(tempfile.mkdtemp()) / \"feature_store_metrics\"\n",
    "\n",
    "# Get connection\n",
    "conn = get_connection()\n",
    "\n",
    "# Use the feature store session context manager\n",
    "with feature_store_session(conn, metrics_path=str(metrics_dir)) as manager:\n",
    "    # Create sample data\n",
    "    # First, create a regular table to store our data\n",
    "    conn.session.sql(\"\"\"\n",
    "        CREATE OR REPLACE TABLE TEMP_CUSTOMER_DATA (\n",
    "            CUSTOMER_ID STRING,\n",
    "            DATE DATE,\n",
    "            AMOUNT FLOAT,\n",
    "            TRANSACTIONS INT,\n",
    "            SESSION_LENGTH FLOAT\n",
    "        )\n",
    "    \"\"\").collect()\n",
    "    \n",
    "    # Insert the data using SQL\n",
    "    conn.session.sql(\"\"\"\n",
    "        INSERT INTO TEMP_CUSTOMER_DATA VALUES\n",
    "        ('C1', '2024-01-01', 100.0, 2, NULL),\n",
    "        ('C1', '2024-01-02', 150.0, 3, 30.5),\n",
    "        ('C2', '2024-01-01', 75.0, 1, NULL),\n",
    "        ('C2', '2024-01-02', 200.0, 4, 45.2)\n",
    "    \"\"\").collect()\n",
    "    \n",
    "    # Create feature DataFrame from the table\n",
    "    df = conn.session.table(\"TEMP_CUSTOMER_DATA\")\n",
    "    \n",
    "    print(\"\\nInitial DataFrame Schema:\")\n",
    "    for field in df.schema.fields:\n",
    "        print(f\"{field.name}: {field.datatype}\")\n",
    "    \n",
    "    print(\"\\nSample Data:\")\n",
    "    df.show()\n",
    "    \n",
    "    # 1. Add Customer Entity\n",
    "    manager.add_entity(\n",
    "        name=\"CUSTOMER\",\n",
    "        join_keys=[\"CUSTOMER_ID\"],\n",
    "        description=\"Customer entity for retail domain\"\n",
    "    )\n",
    "    \n",
    "    # 2. Create Feature Configurations with dependencies\n",
    "    feature_configs = {\n",
    "        \"AMOUNT\": FeatureConfig(\n",
    "            name=\"AMOUNT\",\n",
    "            description=\"Transaction amount\",\n",
    "            validation=FeatureValidationConfig(\n",
    "                null_threshold=0.1,\n",
    "                range_check=True,\n",
    "                min_value=0\n",
    "            ),\n",
    "            dependencies=[]  # Base feature, no dependencies\n",
    "        ),\n",
    "        \"TRANSACTIONS\": FeatureConfig(\n",
    "            name=\"TRANSACTIONS\",\n",
    "            description=\"Number of transactions\",\n",
    "            validation=FeatureValidationConfig(\n",
    "                null_threshold=0.05,\n",
    "                range_check=True,\n",
    "                min_value=0\n",
    "            ),\n",
    "            dependencies=[]  # Base feature, no dependencies\n",
    "        ),\n",
    "        \"SESSION_LENGTH\": FeatureConfig(\n",
    "            name=\"SESSION_LENGTH\",\n",
    "            description=\"Session length in minutes\",\n",
    "            validation=FeatureValidationConfig(\n",
    "                null_threshold=0.3,\n",
    "                range_check=True,\n",
    "                min_value=0\n",
    "            ),\n",
    "            dependencies=[]  # Base feature, no dependencies\n",
    "        ),\n",
    "        \"SUM_AMOUNT_2\": FeatureConfig(\n",
    "            name=\"SUM_AMOUNT_2\",\n",
    "            description=\"2-day rolling sum of amount\",\n",
    "            validation=FeatureValidationConfig(\n",
    "                null_threshold=0.05,\n",
    "                range_check=True,\n",
    "                min_value=0\n",
    "            ),\n",
    "            dependencies=[\"AMOUNT\"]  # Depends on AMOUNT\n",
    "        ),\n",
    "        \"AVG_AMOUNT_2\": FeatureConfig(\n",
    "            name=\"AVG_AMOUNT_2\",\n",
    "            description=\"2-day rolling average of amount\",\n",
    "            validation=FeatureValidationConfig(\n",
    "                null_threshold=0.05,\n",
    "                range_check=True,\n",
    "                min_value=0\n",
    "            ),\n",
    "            dependencies=[\"AMOUNT\"]  # Depends on AMOUNT\n",
    "        )\n",
    "    }\n",
    "\n",
    "    \n",
    "    # 3. Create Feature View Config\n",
    "    config = FeatureViewConfig(\n",
    "        name=\"customer_behavior\",\n",
    "        domain=\"RETAIL\",\n",
    "        entity=\"CUSTOMER\",\n",
    "        feature_type=\"BEHAVIOR\",\n",
    "        refresh=RefreshConfig(frequency=\"1 day\"),\n",
    "        features=feature_configs,\n",
    "        description=\"Customer behavior features\",\n",
    "        timestamp_col=\"DATE\"\n",
    "    )\n",
    "    \n",
    "    # 4. Create transforms\n",
    "    transform_config = TransformConfig(\n",
    "        name=\"amount_metrics\",\n",
    "        null_threshold=0.05,\n",
    "        expected_types=['DECIMAL', 'DOUBLE', 'NUMBER']\n",
    "    )\n",
    "    \n",
    "    transforms = [\n",
    "        # Fill NA values in session length\n",
    "        fill_na(['SESSION_LENGTH'], fill_value=0),\n",
    "        \n",
    "        # Calculate moving aggregations for amount\n",
    "        moving_agg(\n",
    "            cols='AMOUNT',\n",
    "            window_sizes=[2],  # 2-day window\n",
    "            agg_funcs=['SUM', 'AVG'],\n",
    "            partition_by=['CUSTOMER_ID'],\n",
    "            order_by=['DATE'],\n",
    "            config=transform_config\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # 5. Create Feature View\n",
    "    feature_view = manager.add_feature_view(\n",
    "        config=config,\n",
    "        df=df,\n",
    "        entity_name=\"CUSTOMER\",\n",
    "        transforms=transforms,\n",
    "        collect_stats=True\n",
    "    )\n",
    "    \n",
    "    # 6. Check for feature drift with new data\n",
    "    # Create new table for drift detection\n",
    "    conn.session.sql(\"\"\"\n",
    "        CREATE OR REPLACE TABLE TEMP_NEW_DATA (\n",
    "            CUSTOMER_ID STRING,\n",
    "            DATE DATE,\n",
    "            AMOUNT FLOAT,\n",
    "            TRANSACTIONS INT,\n",
    "            SESSION_LENGTH FLOAT\n",
    "        )\n",
    "    \"\"\").collect()\n",
    "    \n",
    "    conn.session.sql(\"\"\"\n",
    "        INSERT INTO TEMP_NEW_DATA VALUES\n",
    "        ('C1', '2024-01-03', 300.0, 5, 60.0),\n",
    "        ('C2', '2024-01-03', 80.0, 1, 15.5)\n",
    "    \"\"\").collect()\n",
    "    \n",
    "    new_df = conn.session.table(\"TEMP_NEW_DATA\")\n",
    "    \n",
    "    drift_results = manager.check_feature_drift(config.name, new_df)\n",
    "    print(\"\\nDrift Detection Results:\")\n",
    "    for feature, metrics in drift_results.items():\n",
    "        print(f\"\\n{feature}:\")\n",
    "        for metric, value in metrics.items():\n",
    "            print(f\"  {metric}: {value:.3f}\")\n",
    "    \n",
    "    # 7. Get Feature Dependencies\n",
    "    deps = manager.get_feature_dependencies(config.name)\n",
    "    print(\"\\nFeature Dependencies:\", deps)\n",
    "    \n",
    "    # 8. Generate Training Dataset\n",
    "    print(\"\\nEntity join keys:\")\n",
    "    for entity in feature_view.entities:\n",
    "        print(f\"Entity {entity.name}: {entity.join_keys}\")\n",
    "\n",
    "    # Create spine DataFrame with explicit quoting\n",
    "    spine_df = df.select([\n",
    "        F.col('CUSTOMER_ID').alias('\"CUSTOMER_ID\"'),\n",
    "        F.col('DATE').alias('\"DATE\"')\n",
    "    ])\n",
    "\n",
    "    print(\"\\nSpine DataFrame columns:\")\n",
    "    print(spine_df.columns)\n",
    "    print(\"\\nSpine DataFrame schema:\")\n",
    "    for field in spine_df.schema.fields:\n",
    "        print(f\"{field.name}: {field.datatype}\")\n",
    "\n",
    "    training_data = manager.get_features(\n",
    "        spine_df=spine_df,\n",
    "        feature_views=[config],\n",
    "        label_cols=[\"TRANSACTIONS\"],\n",
    "        spine_timestamp_col=\"DATE\"\n",
    "    )\n",
    "    print(\"\\nTraining Data Sample:\")\n",
    "    training_data.show(2)\n",
    "\n",
    "    # After creating the feature view:\n",
    "    print(\"\\nFeature Statistics:\")\n",
    "    for feature_name, stats in manager.feature_stats[config.name].items():\n",
    "        print(f\"\\n{feature_name}:\")\n",
    "        print(stats)\n",
    "    \n",
    "    print(\"\\nTraining Data Schema:\")\n",
    "    for field in training_data.schema.fields:\n",
    "        print(f\"{field.name}: {field.datatype}\")\n",
    "    \n",
    "    # Cleanup temporary tables\n",
    "    conn.session.sql(\"DROP TABLE IF EXISTS TEMP_CUSTOMER_DATA\").collect()\n",
    "    conn.session.sql(\"DROP TABLE IF EXISTS TEMP_NEW_DATA\").collect()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "feature-store",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
