{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End-to-End Feature Store Example\n",
    "\n",
    "> A comprehensive guide to setting up and using the Snowflake Feature Store for ML workflows.\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook demonstrates a complete ML feature engineering workflow using Snowflake's Feature Store. We'll create a permanent feature store that:\n",
    "1. Persists across sessions\n",
    "2. Is accessible through Snowsight UI\n",
    "3. Includes feature monitoring\n",
    "4. Demonstrates best practices\n",
    "\n",
    "### Why Use a Feature Store?\n",
    "\n",
    "Feature stores solve several critical ML challenges:\n",
    "\n",
    "1. **Feature Consistency**\n",
    "   - Ensure features are computed identically in training and production\n",
    "   - Version control for feature definitions\n",
    "   - Single source of truth for feature transformations\n",
    "\n",
    "2. **Feature Reuse**\n",
    "   - Share features across teams and projects\n",
    "   - Reduce duplicate computation\n",
    "   - Standardize common transformations\n",
    "\n",
    "3. **Feature Monitoring**\n",
    "   - Track feature drift over time\n",
    "   - Monitor feature quality\n",
    "   - Alert on data quality issues\n",
    "\n",
    "4. **Feature Discovery**\n",
    "   - Make features searchable and discoverable\n",
    "   - Document feature definitions and usage\n",
    "   - Track feature dependencies\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before running this example, you need:\n",
    "\n",
    "1. Appropriate permissions (see below)\n",
    "\n",
    "2. Python environment with required packages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generation\n",
    "\n",
    "First, we'll create some example customer data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Optional\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import snowflake.snowpark.functions as F\n",
    "\n",
    "from snowflake_feature_store.connection import get_connection\n",
    "from snowflake_feature_store.manager import FeatureStoreManager\n",
    "from snowflake_feature_store.config import (\n",
    "    FeatureViewConfig, FeatureConfig, RefreshConfig, \n",
    "    FeatureValidationConfig\n",
    ")\n",
    "from snowflake_feature_store.transforms import (\n",
    "    Transform, TransformConfig, moving_agg, \n",
    "    fill_na, date_diff, CustomTransform\n",
    ")\n",
    "from snowflake_feature_store.examples import (\n",
    "    get_example_data, create_feature_configs\n",
    ")\n",
    "from snowflake_feature_store.logging import logger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setting Up the Feature Store\n",
    "\n",
    "First, we'll create a permanent feature store in Snowflake. This differs from our previous examples which used temporary schemas.\n",
    "\n",
    "### Required Permissions\n",
    "\n",
    "To create a permanent feature store, you need:\n",
    "- `CREATE DATABASE` if creating a new database\n",
    "- `CREATE SCHEMA` in the target database\n",
    "- `USAGE` on the warehouse\n",
    "- `CREATE TABLE` in the target schema\n",
    "\n",
    "### Why Permanent vs Temporary?\n",
    "\n",
    "Permanent feature stores offer several advantages:\n",
    "1. Persistence across sessions\n",
    "2. Accessibility through Snowsight UI\n",
    "3. Ability to share with other users/roles\n",
    "4. Integration with other Snowflake tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-26 18:57:01,857 - snowflake_feature_store - INFO - Using active Snowflake session\n",
      "2025-02-26 18:57:01,858 - snowflake_feature_store - INFO - Initialized connection to \"DATASCIENCE\".\"FEATURE_STORE_DEMO\"\n",
      "2025-02-26 18:57:04,045 - snowflake_feature_store - INFO - Using role: \"DATA_SCIENTIST\", warehouse: \"DS_WH_XS\", database: DATASCIENCE, schema: FEATURE_STORE_DEMO\n",
      "2025-02-26 18:57:04,047 - snowflake_feature_store - INFO - Connected to DATASCIENCE.FEATURE_STORE_DEMO as DATA_SCIENTIST\n"
     ]
    }
   ],
   "source": [
    "# Specify all connection parameters directly\n",
    "database = \"DATASCIENCE\"\n",
    "schema = \"FEATURE_STORE_DEMO\"\n",
    "warehouse = \"DS_WH_XS\"\n",
    "role = \"DATA_SCIENTIST\"\n",
    "\n",
    "# Get connection with all custom parameters\n",
    "conn = get_connection(\n",
    "    database=database, \n",
    "    schema=schema, \n",
    "    warehouse=warehouse,\n",
    "    role=role\n",
    ")\n",
    "\n",
    "# Now you can use the connection with the proper context\n",
    "logger.info(f\"Connected to {database}.{schema} as {role}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Data Generation and Loading\n",
    "\n",
    "In this section, we'll create example customer data. In a real scenario, you'd load your own data, but this example shows:\n",
    "1. Proper data typing for Snowflake\n",
    "2. Handling temporal data correctly\n",
    "3. Setting up data quality checks\n",
    "4. Creating realistic patterns in the data\n",
    "\n",
    "### Why This Structure?\n",
    "\n",
    "This structure demonstrates common ML feature engineering challenges:\n",
    "1. **Time-based Features**: Rolling averages, time windows\n",
    "2. **Missing Data**: Handling sparse observations\n",
    "3. **Multiple Metrics**: Combining different data types\n",
    "4. **Entity Resolution**: Linking data to customers\n",
    "\n",
    "### Why LTV Prediction?\n",
    "\n",
    "LTV prediction is a common ML use case that demonstrates key feature store benefits:\n",
    "1. **Time-based Features**: Customer spending patterns over time\n",
    "2. **Multiple Data Sources**: Combining transactions, web analytics, and customer data\n",
    "3. **Feature Freshness**: Regular updates as new transactions occur\n",
    "4. **Point-in-Time Correctness**: Avoiding data leakage in training\n",
    "\n",
    "### Data Structure\n",
    "Our LTV example includes:\n",
    "- `LIFE_TIME_VALUE`: Current customer value (target)\n",
    "- `SESSION_LENGTH`: Customer engagement metric\n",
    "- `TRANSACTIONS`: Number of transactions\n",
    "- `TIME_ON_APP/WEBSITE`: Engagement channels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-26 18:57:20,976 - snowflake_feature_store - INFO - Generated 2400 rows of demo data in \"DATASCIENCE\".FEATURE_STORE_DEMO.CUSTOMER_ACTIVITY\n",
      "2025-02-26 18:57:20,979 - snowflake_feature_store - INFO - \n",
      "Sample Data:\n",
      "---------------------------------------------------------------------------------------------------------------------------------\n",
      "|\"CUSTOMER_ID\"  |\"DATE\"      |\"LIFE_TIME_VALUE\"  |\"SESSION_LENGTH\"   |\"TIME_ON_APP\"       |\"TIME_ON_WEBSITE\"   |\"TRANSACTIONS\"  |\n",
      "---------------------------------------------------------------------------------------------------------------------------------\n",
      "|C26            |2025-01-27  |564.6091706355498  |7.616530664264483  |12.142466259190991  |10.138495747448552  |5               |\n",
      "|C85            |2025-02-25  |59.41299820191271  |4.727356127459224  |5.495725181799517   |3.859344227079125   |1               |\n",
      "|C72            |2025-02-25  |646.3111447043822  |8.908367451703572  |9.728204471331994   |13.191869376320893  |6               |\n",
      "|C60            |2025-02-25  |447.0367329096978  |8.704973243200099  |10.936684313415565  |11.222118890927462  |4               |\n",
      "|C48            |2025-02-25  |294.3029664683961  |NULL               |5.2542111627227275  |7.048625379945507   |2               |\n",
      "---------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "2025-02-26 18:57:22,023 - snowflake_feature_store - INFO - \n",
      "Schema:\n",
      "2025-02-26 18:57:22,252 - snowflake_feature_store - INFO - CUSTOMER_ID: StringType()\n",
      "2025-02-26 18:57:22,253 - snowflake_feature_store - INFO - DATE: DateType()\n",
      "2025-02-26 18:57:22,254 - snowflake_feature_store - INFO - LIFE_TIME_VALUE: DoubleType()\n",
      "2025-02-26 18:57:22,255 - snowflake_feature_store - INFO - SESSION_LENGTH: DoubleType()\n",
      "2025-02-26 18:57:22,255 - snowflake_feature_store - INFO - TIME_ON_APP: DoubleType()\n",
      "2025-02-26 18:57:22,256 - snowflake_feature_store - INFO - TIME_ON_WEBSITE: DoubleType()\n",
      "2025-02-26 18:57:22,257 - snowflake_feature_store - INFO - TRANSACTIONS: LongType()\n",
      "2025-02-26 18:57:22,257 - snowflake_feature_store - INFO - \n",
      "Data Profile:\n",
      "2025-02-26 18:57:23,040 - snowflake_feature_store - INFO - CUSTOMER_ID: 0.0% null\n",
      "2025-02-26 18:57:23,442 - snowflake_feature_store - INFO - DATE: 0.0% null\n",
      "2025-02-26 18:57:23,879 - snowflake_feature_store - INFO - LIFE_TIME_VALUE: 0.0% null\n",
      "2025-02-26 18:57:24,264 - snowflake_feature_store - INFO - SESSION_LENGTH: 20.5% null\n",
      "2025-02-26 18:57:24,797 - snowflake_feature_store - INFO - TIME_ON_APP: 0.0% null\n",
      "2025-02-26 18:57:25,175 - snowflake_feature_store - INFO - TIME_ON_WEBSITE: 0.0% null\n",
      "2025-02-26 18:57:25,571 - snowflake_feature_store - INFO - TRANSACTIONS: 0.0% null\n"
     ]
    }
   ],
   "source": [
    "# Get start date\n",
    "num_customers =  100\n",
    "\n",
    "# Generate data with patterns\n",
    "df = get_example_data(\n",
    "    conn.session,\n",
    "    schema,\n",
    "    num_customers,\n",
    ")\n",
    "\n",
    "# Show data profile\n",
    "logger.info(\"\\nData Profile:\")\n",
    "for col in df.columns:\n",
    "    null_count = df.filter(F.col(col).is_null()).count()\n",
    "    null_pct = null_count / df.count() * 100\n",
    "    logger.info(f\"{col}: {null_pct:.1f}% null\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Entity Creation\n",
    "\n",
    "Entities are the foundation of your feature store. They represent the objects you're collecting features about (e.g., customers, products, transactions).\n",
    "\n",
    "### Why Entities Matter\n",
    "\n",
    "Proper entity design is crucial because:\n",
    "1. Entities determine how features can be joined\n",
    "2. Entities define the granularity of your features\n",
    "3. Entities enable point-in-time correct feature retrieval\n",
    "4. Entities help organize and discover features\n",
    "\n",
    "### Entity Best Practices\n",
    "\n",
    "1. **Unique Keys**: Choose stable, unique identifiers\n",
    "2. **Granularity**: Pick the right level (e.g., customer vs. session)\n",
    "3. **Documentation**: Clearly describe what the entity represents\n",
    "4. **Consistency**: Use the same keys across feature views\n",
    "\n",
    "\n",
    "### Entity Design for LTV\n",
    "\n",
    "For LTV prediction, we need a customer entity that:\n",
    "1. Has a stable identifier\n",
    "2. Links to all customer interactions\n",
    "3. Supports time-based feature aggregation\n",
    "\n",
    "Key considerations for our customer entity:\n",
    "1. **Identifier**: Use `CUSTOMER_ID` as stable key\n",
    "2. **Temporal Aspect**: Track customer since first transaction\n",
    "3. **Granularity**: Customer-level for LTV prediction\n",
    "4. **Documentation**: Clear description for feature discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-26 18:57:35,615 - snowflake_feature_store - INFO - FeatureStoreManager initialized\n",
      "2025-02-26 18:57:37,238 - snowflake_feature_store - INFO - Created entity: CUSTOMER with keys: ['CUSTOMER_ID']\n",
      "2025-02-26 18:57:37,240 - snowflake_feature_store - INFO - Created CUSTOMER entity for LTV prediction\n",
      "2025-02-26 18:57:38,054 - snowflake_feature_store - INFO - \n",
      "Entity Details:\n",
      "2025-02-26 18:57:38,055 - snowflake_feature_store - INFO - Name: CUSTOMER\n",
      "2025-02-26 18:57:38,056 - snowflake_feature_store - INFO - Join Keys: ['CUSTOMER_ID']\n"
     ]
    }
   ],
   "source": [
    "# Create detailed documentation\n",
    "description = \"\"\"\n",
    "Customer Entity for LTV Prediction\n",
    "\n",
    "This entity represents individual customers and their behavior over time.\n",
    "It serves as the primary entity for customer lifetime value prediction.\n",
    "\n",
    "Key Information:\n",
    "- Primary Key: CUSTOMER_ID (stable identifier)\n",
    "- Temporal Key: DATE (for point-in-time correct features)\n",
    "- Granularity: One record per customer per day\n",
    "\n",
    "Usage:\n",
    "1. Base entity for customer-level features\n",
    "2. Join key for transaction and session data\n",
    "3. Temporal alignment for time-based features\n",
    "\n",
    "Best Practices:\n",
    "- Always join using CUSTOMER_ID\n",
    "- Use DATE for point-in-time correctness\n",
    "- Aggregate features to customer-day level\n",
    "\n",
    "Example:\n",
    "```sql\n",
    "SELECT CUSTOMER_ID, DATE, COUNT(*) as daily_transactions\n",
    "FROM transactions\n",
    "GROUP BY CUSTOMER_ID, DATE\n",
    "```\n",
    "\"\"\".strip()\n",
    "\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "\n",
    "metrics_dir = Path(tempfile.mkdtemp()) / \"feature_store_metrics\"\n",
    "\n",
    "manager = FeatureStoreManager(\n",
    "    connection=conn,\n",
    "    metrics_path=metrics_dir,\n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "# Create entity\n",
    "manager.add_entity(\n",
    "    name=\"CUSTOMER\",\n",
    "    join_keys=[\"CUSTOMER_ID\"],\n",
    "    description=description,\n",
    ")\n",
    "\n",
    "logger.info(\"Created CUSTOMER entity for LTV prediction\")\n",
    "# Verify entity creation\n",
    "entity = manager.feature_store.get_entity(\"CUSTOMER\")\n",
    "logger.info(\"\\nEntity Details:\")\n",
    "logger.info(f\"Name: {entity.name}\")\n",
    "logger.info(f\"Join Keys: {entity.join_keys}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Points About This Implementation\n",
    "\n",
    "1. **Documentation**\n",
    "   - Clear description of entity purpose\n",
    "   - Usage examples included\n",
    "   - Best practices documented\n",
    "   - SQL example provided\n",
    "\n",
    "\n",
    "2. **Validation**\n",
    "   - Entity creation is verified\n",
    "   - Join keys are explicitly defined\n",
    "   - Logging provides creation confirmation\n",
    "   - Error handling is included\n",
    "\n",
    "3. **LTV Specific**\n",
    "   - Designed for customer-level predictions\n",
    "   - Supports temporal feature creation\n",
    "   - Enables point-in-time correct joins\n",
    "   - Facilitates customer behavior tracking\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Feature Configuration\n",
    "\n",
    "Feature configuration is where we define what features we want to create and how they should behave.\n",
    "\n",
    "### Feature Configuration Concepts\n",
    "\n",
    "A feature configuration defines:\n",
    "1. **Validation Rules**: Data quality checks and thresholds\n",
    "2. **Dependencies**: What other features this feature needs\n",
    "3. **Metadata**: Description, tags, and ownership\n",
    "4. **Refresh Settings**: How often to update the feature\n",
    "\n",
    "### Why Configuration Matters\n",
    "\n",
    "Good feature configuration ensures:\n",
    "1. Data quality is maintained\n",
    "2. Features are well-documented\n",
    "3. Dependencies are tracked\n",
    "4. Feature freshness is appropriate\n",
    "\n",
    "### LTV Feature Configuration\n",
    "\n",
    "For LTV prediction, we need several types of features:\n",
    "1. **Behavioral Features**\n",
    "   - Session metrics\n",
    "   - Engagement patterns\n",
    "   - Transaction history\n",
    "\n",
    "2. **Temporal Features**\n",
    "   - Time since first purchase\n",
    "   - Weekly/monthly patterns\n",
    "   - Rolling aggregations\n",
    "\n",
    "3. **Derived Features**\n",
    "   - Average transaction value\n",
    "   - Engagement ratios\n",
    "   - Time-based metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'LIFE_TIME_VALUE': FeatureConfig(name='LIFE_TIME_VALUE', description='Current customer lifetime value', validation=FeatureValidationConfig(null_check=True, null_threshold=0.0, range_check=True, min_value=0.0, max_value=None, unique_check=False, unique_threshold=0.9), dependencies=[]),\n",
       " 'SESSION_LENGTH': FeatureConfig(name='SESSION_LENGTH', description='Session length in minutes', validation=FeatureValidationConfig(null_check=True, null_threshold=0.2, range_check=True, min_value=0.0, max_value=None, unique_check=False, unique_threshold=0.9), dependencies=[]),\n",
       " 'TRANSACTIONS': FeatureConfig(name='TRANSACTIONS', description='Number of transactions', validation=FeatureValidationConfig(null_check=True, null_threshold=0.0, range_check=True, min_value=0.0, max_value=None, unique_check=False, unique_threshold=0.9), dependencies=[]),\n",
       " 'SUM_TRANSACTIONS_7': FeatureConfig(name='SUM_TRANSACTIONS_7', description='sum of transactions over 7 days', validation=FeatureValidationConfig(null_check=True, null_threshold=0.1, range_check=True, min_value=0.0, max_value=None, unique_check=False, unique_threshold=0.9), dependencies=['TRANSACTIONS']),\n",
       " 'AVG_TRANSACTIONS_7': FeatureConfig(name='AVG_TRANSACTIONS_7', description='avg of transactions over 7 days', validation=FeatureValidationConfig(null_check=True, null_threshold=0.1, range_check=True, min_value=0.0, max_value=None, unique_check=False, unique_threshold=0.9), dependencies=['TRANSACTIONS']),\n",
       " 'SUM_LIFE_TIME_VALUE_7': FeatureConfig(name='SUM_LIFE_TIME_VALUE_7', description='sum of life_time_value over 7 days', validation=FeatureValidationConfig(null_check=True, null_threshold=0.1, range_check=True, min_value=0.0, max_value=None, unique_check=False, unique_threshold=0.9), dependencies=['LIFE_TIME_VALUE']),\n",
       " 'AVG_LIFE_TIME_VALUE_7': FeatureConfig(name='AVG_LIFE_TIME_VALUE_7', description='avg of life_time_value over 7 days', validation=FeatureValidationConfig(null_check=True, null_threshold=0.1, range_check=True, min_value=0.0, max_value=None, unique_check=False, unique_threshold=0.9), dependencies=['LIFE_TIME_VALUE']),\n",
       " 'SUM_TRANSACTIONS_30': FeatureConfig(name='SUM_TRANSACTIONS_30', description='sum of transactions over 30 days', validation=FeatureValidationConfig(null_check=True, null_threshold=0.1, range_check=True, min_value=0.0, max_value=None, unique_check=False, unique_threshold=0.9), dependencies=['TRANSACTIONS']),\n",
       " 'AVG_TRANSACTIONS_30': FeatureConfig(name='AVG_TRANSACTIONS_30', description='avg of transactions over 30 days', validation=FeatureValidationConfig(null_check=True, null_threshold=0.1, range_check=True, min_value=0.0, max_value=None, unique_check=False, unique_threshold=0.9), dependencies=['TRANSACTIONS']),\n",
       " 'SUM_LIFE_TIME_VALUE_30': FeatureConfig(name='SUM_LIFE_TIME_VALUE_30', description='sum of life_time_value over 30 days', validation=FeatureValidationConfig(null_check=True, null_threshold=0.1, range_check=True, min_value=0.0, max_value=None, unique_check=False, unique_threshold=0.9), dependencies=['LIFE_TIME_VALUE']),\n",
       " 'AVG_LIFE_TIME_VALUE_30': FeatureConfig(name='AVG_LIFE_TIME_VALUE_30', description='avg of life_time_value over 30 days', validation=FeatureValidationConfig(null_check=True, null_threshold=0.1, range_check=True, min_value=0.0, max_value=None, unique_check=False, unique_threshold=0.9), dependencies=['LIFE_TIME_VALUE']),\n",
       " 'ENGAGEMENT_SCORE': FeatureConfig(name='ENGAGEMENT_SCORE', description='Combined engagement metric', validation=FeatureValidationConfig(null_check=True, null_threshold=0.1, range_check=True, min_value=0.0, max_value=None, unique_check=False, unique_threshold=0.9), dependencies=['SESSION_LENGTH', 'TIME_ON_APP', 'TIME_ON_WEBSITE']),\n",
       " 'AVG_TRANSACTION_VALUE': FeatureConfig(name='AVG_TRANSACTION_VALUE', description='Average value per transaction', validation=FeatureValidationConfig(null_check=True, null_threshold=0.1, range_check=True, min_value=0.0, max_value=None, unique_check=False, unique_threshold=0.9), dependencies=['LIFE_TIME_VALUE', 'TRANSACTIONS'])}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Base features (from source data)\n",
    "feature_configs = {\n",
    "    \"LIFE_TIME_VALUE\": FeatureConfig(\n",
    "        name=\"LIFE_TIME_VALUE\",\n",
    "        description=\"Current customer lifetime value\",\n",
    "        validation=FeatureValidationConfig(\n",
    "            null_threshold=0.0,\n",
    "            range_check=True,\n",
    "            min_value=0\n",
    "        )\n",
    "    ),\n",
    "    \"SESSION_LENGTH\": FeatureConfig(\n",
    "        name=\"SESSION_LENGTH\",\n",
    "        description=\"Session length in minutes\",\n",
    "        validation=FeatureValidationConfig(\n",
    "            null_threshold=0.2,\n",
    "            range_check=True,\n",
    "            min_value=0\n",
    "        )\n",
    "    ),\n",
    "    \"TRANSACTIONS\": FeatureConfig(\n",
    "        name=\"TRANSACTIONS\",\n",
    "        description=\"Number of transactions\",\n",
    "        validation=FeatureValidationConfig(\n",
    "            null_threshold=0.0,\n",
    "            range_check=True,\n",
    "            min_value=0\n",
    "        )\n",
    "    )\n",
    "}\n",
    "\n",
    "# Time window features (match moving_agg output names)\n",
    "for window in [7, 30]:\n",
    "    for metric in ['TRANSACTIONS', 'LIFE_TIME_VALUE']:\n",
    "        for agg in ['SUM', 'AVG']:\n",
    "            feature_name = f\"{agg}_{metric}_{window}\"\n",
    "            feature_configs[feature_name] = FeatureConfig(\n",
    "                name=feature_name,\n",
    "                description=f\"{agg.lower()} of {metric.lower()} over {window} days\",\n",
    "                validation=FeatureValidationConfig(\n",
    "                    null_threshold=0.1,\n",
    "                    range_check=True,\n",
    "                    min_value=0\n",
    "                ),\n",
    "                dependencies=[metric]\n",
    "            )\n",
    "\n",
    "# Derived features\n",
    "feature_configs.update({\n",
    "    \"ENGAGEMENT_SCORE\": FeatureConfig(\n",
    "        name=\"ENGAGEMENT_SCORE\",\n",
    "        description=\"Combined engagement metric\",\n",
    "        validation=FeatureValidationConfig(\n",
    "            null_threshold=0.1,\n",
    "            range_check=True,\n",
    "            min_value=0\n",
    "        ),\n",
    "        dependencies=[\"SESSION_LENGTH\", \"TIME_ON_APP\", \"TIME_ON_WEBSITE\"]\n",
    "    ),\n",
    "    \"AVG_TRANSACTION_VALUE\": FeatureConfig(\n",
    "        name=\"AVG_TRANSACTION_VALUE\",\n",
    "        description=\"Average value per transaction\",\n",
    "        validation=FeatureValidationConfig(\n",
    "            null_threshold=0.1,\n",
    "            range_check=True,\n",
    "            min_value=0\n",
    "        ),\n",
    "        dependencies=[\"LIFE_TIME_VALUE\", \"TRANSACTIONS\"]\n",
    "    )\n",
    "})\n",
    "\n",
    "feature_configs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Feature Transformations\n",
    "\n",
    "Feature transformations convert raw data into ML-ready features. This is a critical step in the ML pipeline.\n",
    "\n",
    "### Why Transformations Matter\n",
    "\n",
    "Transformations serve multiple purposes:\n",
    "1. **Data Quality**: Handle missing values and outliers\n",
    "2. **Feature Engineering**: Create more predictive features\n",
    "3. **ML Requirements**: Format data for model consumption\n",
    "4. **Business Logic**: Encode domain knowledge\n",
    "\n",
    "### Types of Transformations\n",
    "\n",
    "1. **Basic Transformations**\n",
    "   - Missing value imputation\n",
    "   - Type conversion\n",
    "   - Scaling/normalization\n",
    "\n",
    "2. **Time-Based Transformations**\n",
    "   - Rolling windows\n",
    "   - Time since event\n",
    "   - Seasonal patterns\n",
    "\n",
    "3. **Business Transformations**\n",
    "   - Derived metrics\n",
    "   - Domain-specific calculations\n",
    "   - Feature combinations\n",
    "\n",
    "### LTV-Specific Transformations\n",
    "\n",
    "For LTV prediction, we need several key transformations:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.snowpark import DataFrame\n",
    "from typing import Callable\n",
    "\n",
    "from snowflake_feature_store.transforms import ValidationMixin\n",
    "\n",
    "class CustomTransform(ValidationMixin):\n",
    "    \"\"\"Wrapper for custom transformations\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        transform_func: Callable[[DataFrame], DataFrame],\n",
    "        config: TransformConfig\n",
    "    ):\n",
    "        self._transform = transform_func\n",
    "        self._config = config\n",
    "        \n",
    "    @property\n",
    "    def config(self) -> TransformConfig:\n",
    "        return self._config\n",
    "        \n",
    "    def __call__(self, df: DataFrame) -> DataFrame:\n",
    "        return self._transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default config if none provided\n",
    "transform_config = TransformConfig(\n",
    "    name=\"ltv_transforms\",\n",
    "    null_threshold=0.1,\n",
    "    expected_types=['DECIMAL', 'DOUBLE', 'NUMBER']\n",
    ")\n",
    "\n",
    "transforms = [\n",
    "    # 1. Handle Missing Values\n",
    "    fill_na(\n",
    "        ['SESSION_LENGTH', 'TIME_ON_APP', 'TIME_ON_WEBSITE'],\n",
    "        fill_value=0,\n",
    "        config=TransformConfig(\n",
    "            name=\"engagement_imputation\",\n",
    "            description=\"Fill missing engagement metrics with 0\"\n",
    "        )\n",
    "    ),\n",
    "    \n",
    "    # 2. Time-Based Features\n",
    "    moving_agg(\n",
    "        cols=['TRANSACTIONS', 'LIFE_TIME_VALUE'],\n",
    "        window_sizes=[7, 30],  # 7 and 30 day windows\n",
    "        agg_funcs=['SUM', 'AVG'],\n",
    "        partition_by=['CUSTOMER_ID'],\n",
    "        order_by=['DATE'],\n",
    "        config=TransformConfig(\n",
    "            name=\"time_windows\",\n",
    "            description=\"Rolling window aggregations\"\n",
    "        )\n",
    "    ),\n",
    "    # 3. Engagement Metrics\n",
    "    CustomTransform(\n",
    "        transform_func=lambda df: df.with_column(\n",
    "            'ENGAGEMENT_SCORE',\n",
    "            (F.col('SESSION_LENGTH') + \n",
    "                F.col('TIME_ON_APP') + \n",
    "                F.col('TIME_ON_WEBSITE')) / 3.0\n",
    "        ),\n",
    "        config=TransformConfig(\n",
    "            name=\"engagement_score\",\n",
    "            description=\"Combined engagement metric\",\n",
    "            expected_types=['DOUBLE']\n",
    "        )\n",
    "    ),\n",
    "    \n",
    "    # 4. Transaction Metrics\n",
    "    CustomTransform(\n",
    "        transform_func=lambda df: df.with_column(\n",
    "            'AVG_TRANSACTION_VALUE',\n",
    "            F.col('LIFE_TIME_VALUE') / \n",
    "            F.when(F.col('TRANSACTIONS') > 0, F.col('TRANSACTIONS'))\n",
    "            .otherwise(1)\n",
    "        ),\n",
    "        config=TransformConfig(\n",
    "            name=\"avg_transaction_value\",\n",
    "            description=\"Average value per transaction\",\n",
    "            expected_types=['DOUBLE']\n",
    "        )\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Practices for Transformations\n",
    "\n",
    "1. **Validation**\n",
    "   - Check input data types\n",
    "   - Validate output ranges\n",
    "   - Monitor null ratios\n",
    "\n",
    "2. **Performance**\n",
    "   - Use vectorized operations\n",
    "   - Minimize data movement\n",
    "   - Leverage Snowflake optimizations\n",
    "\n",
    "3. **Documentation**\n",
    "   - Document business logic\n",
    "   - Explain transformation choices\n",
    "   - Track dependencies\n",
    "\n",
    "### Example Usage\n",
    "\n",
    "Let's apply our transformations and examine the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake_feature_store.transforms import apply_transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|\"CUSTOMER_ID\"  |\"DATE\"      |\"LIFE_TIME_VALUE\"   |\"SESSION_LENGTH\"  |\"TIME_ON_APP\"       |\"TIME_ON_WEBSITE\"   |\"TRANSACTIONS\"  |\"SUM_TRANSACTIONS_7\"  |\"AVG_TRANSACTIONS_7\"  |\"SUM_TRANSACTIONS_30\"  |\"AVG_TRANSACTIONS_30\"  |\"SUM_LIFE_TIME_VALUE_7\"  |\"AVG_LIFE_TIME_VALUE_7\"  |\"SUM_LIFE_TIME_VALUE_30\"  |\"AVG_LIFE_TIME_VALUE_30\"  |\"ENGAGEMENT_SCORE\"  |\"AVG_TRANSACTION_VALUE\"  |\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|C17            |2025-01-27  |358.75645239371033  |0.0               |5.8309034937188144  |9.487293041019019   |3               |3.0                   |3.0                   |3.0                    |3.0                    |358.75645239371033       |358.75645239371033       |358.75645239371033        |358.75645239371033        |5.106065511579278   |119.58548413123678       |\n",
      "|C17            |2025-01-28  |518.2970633604735   |0.0               |11.55870770225566   |10.304817307763335  |5               |8.0                   |4.0                   |8.0                    |4.0                    |877.0535157541839        |438.52675787709194       |877.0535157541839         |438.52675787709194        |7.287841670006332   |103.6594126720947        |\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Apply transforms\n",
    "transformed_df = apply_transforms(df, transforms)\n",
    "print(transformed_df.show(2))\n",
    "\n",
    "show = False\n",
    "if show:\n",
    "    # Show new features\n",
    "    print(\"\\nNew Features Created:\")\n",
    "    new_cols = set(transformed_df.columns) - set(df.columns)\n",
    "    for col in sorted(new_cols):\n",
    "        print(f\"\\n{col}:\")\n",
    "        transformed_df.select([\n",
    "            F.min(col).alias('min'),\n",
    "            F.max(col).alias('max'),\n",
    "            F.avg(col).alias('mean')\n",
    "        ]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Feature View Creation\n",
    "\n",
    "Feature views combine configurations, transformations, and source data into production-ready features.\n",
    "\n",
    "### What is a Feature View?\n",
    "\n",
    "A feature view is:\n",
    "1. **Source Data**: Raw data input\n",
    "2. **Transformations**: Feature engineering logic\n",
    "3. **Configurations**: Validation and refresh rules\n",
    "4. **Metadata**: Documentation and lineage\n",
    "\n",
    "### Why Feature Views Matter\n",
    "\n",
    "Feature views provide:\n",
    "1. **Reproducibility**: Consistent feature computation\n",
    "2. **Monitoring**: Track feature health\n",
    "3. **Discovery**: Make features findable\n",
    "4. **Governance**: Control access and updates\n",
    "\n",
    "### LTV Feature View Design\n",
    "\n",
    "For LTV prediction, our feature view needs to:\n",
    "1. Combine engagement and transaction data\n",
    "2. Apply time-based transformations\n",
    "3. Maintain point-in-time correctness\n",
    "4. Enable regular refreshes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-26 18:58:47,395 - snowflake_feature_store - INFO - Validated feature LIFE_TIME_VALUE (stats: {'timestamp': '2025-02-27T02:58:46.549388', 'row_count': 2400, 'null_count': 0, 'null_ratio': 0.0, 'unique_count': 2400, 'min_value': 2.239592053174099, 'max_value': 749.1277222421559, 'mean_value': 371.0923268443173, 'std_value': 217.24439799534923})\n",
      "2025-02-26 18:58:49,708 - snowflake_feature_store - INFO - Validated feature SESSION_LENGTH (stats: {'timestamp': '2025-02-27T02:58:47.764298', 'row_count': 2400, 'null_count': 0, 'null_ratio': 0.0, 'unique_count': 1908, 'min_value': 0.0, 'max_value': 12.219298377916665, 'mean_value': 4.924390829623488, 'std_value': 3.4026523269547897})\n",
      "2025-02-26 18:58:51,019 - snowflake_feature_store - INFO - Validated feature TRANSACTIONS (stats: {'timestamp': '2025-02-27T02:58:50.110091', 'row_count': 2400, 'null_count': 0, 'null_ratio': 0.0, 'unique_count': 7, 'min_value': 1.0, 'max_value': 7.0, 'mean_value': 3.366667, 'std_value': 2.0001389951700856})\n",
      "2025-02-26 18:58:52,525 - snowflake_feature_store - INFO - Validated feature SUM_TRANSACTIONS_7 (stats: {'timestamp': '2025-02-27T02:58:51.405251', 'row_count': 2400, 'null_count': 0, 'null_ratio': 0.0, 'unique_count': 39, 'min_value': 1.0, 'max_value': 39.0, 'mean_value': 20.647083333333335, 'std_value': 7.7325036694767615})\n",
      "2025-02-26 18:58:54,016 - snowflake_feature_store - INFO - Validated feature AVG_TRANSACTIONS_7 (stats: {'timestamp': '2025-02-27T02:58:52.964874', 'row_count': 2400, 'null_count': 0, 'null_ratio': 0.0, 'unique_count': 82, 'min_value': 1.0, 'max_value': 7.0, 'mean_value': 3.3584570833333336, 'std_value': 0.9081127243491726})\n",
      "2025-02-26 18:58:55,361 - snowflake_feature_store - INFO - Validated feature SUM_LIFE_TIME_VALUE_7 (stats: {'timestamp': '2025-02-27T02:58:54.404302', 'row_count': 2400, 'null_count': 0, 'null_ratio': 0.0, 'unique_count': 2400, 'min_value': 13.077530619670986, 'max_value': 4236.6778134791375, 'mean_value': 2274.693079593752, 'std_value': 855.943387600666})\n",
      "2025-02-26 18:58:56,861 - snowflake_feature_store - INFO - Validated feature AVG_LIFE_TIME_VALUE_7 (stats: {'timestamp': '2025-02-27T02:58:55.772679', 'row_count': 2400, 'null_count': 0, 'null_ratio': 0.0, 'unique_count': 2400, 'min_value': 13.077530619670986, 'max_value': 746.5940585674848, 'mean_value': 370.0081979547277, 'std_value': 101.06575000463832})\n",
      "2025-02-26 18:58:58,333 - snowflake_feature_store - INFO - Validated feature SUM_TRANSACTIONS_30 (stats: {'timestamp': '2025-02-27T02:58:57.353567', 'row_count': 2400, 'null_count': 0, 'null_ratio': 0.0, 'unique_count': 106, 'min_value': 1.0, 'max_value': 112.0, 'mean_value': 42.33708333333333, 'std_value': 24.726804909385617})\n",
      "2025-02-26 18:58:59,571 - snowflake_feature_store - INFO - Validated feature AVG_TRANSACTIONS_30 (stats: {'timestamp': '2025-02-27T02:58:58.695444', 'row_count': 2400, 'null_count': 0, 'null_ratio': 0.0, 'unique_count': 409, 'min_value': 1.0, 'max_value': 7.0, 'mean_value': 3.3446108333333333, 'std_value': 0.7701141182408852})\n",
      "2025-02-26 18:59:00,875 - snowflake_feature_store - INFO - Validated feature SUM_LIFE_TIME_VALUE_30 (stats: {'timestamp': '2025-02-27T02:58:59.948270', 'row_count': 2400, 'null_count': 0, 'null_ratio': 0.0, 'unique_count': 2400, 'min_value': 13.077530619670986, 'max_value': 12223.462666733421, 'mean_value': 4667.152806923684, 'std_value': 2730.693558396509})\n",
      "2025-02-26 18:59:02,881 - snowflake_feature_store - INFO - Validated feature AVG_LIFE_TIME_VALUE_30 (stats: {'timestamp': '2025-02-27T02:59:01.831308', 'row_count': 2400, 'null_count': 0, 'null_ratio': 0.0, 'unique_count': 2400, 'min_value': 13.077530619670986, 'max_value': 746.5940585674848, 'mean_value': 368.6911018427686, 'std_value': 85.8407784959337})\n",
      "2025-02-26 18:59:04,494 - snowflake_feature_store - INFO - Validated feature ENGAGEMENT_SCORE (stats: {'timestamp': '2025-02-27T02:59:03.545975', 'row_count': 2400, 'null_count': 0, 'null_ratio': 0.0, 'unique_count': 2400, 'min_value': 1.8904928435296238, 'max_value': 13.00033587035531, 'mean_value': 7.115458021279218, 'std_value': 2.360729920090712})\n",
      "2025-02-26 18:59:06,030 - snowflake_feature_store - INFO - Validated feature AVG_TRANSACTION_VALUE (stats: {'timestamp': '2025-02-27T02:59:04.850674', 'row_count': 2400, 'null_count': 0, 'null_ratio': 0.0, 'unique_count': 2400, 'min_value': 2.239592053174099, 'max_value': 199.91701587497403, 'mean_value': 109.86392365044138, 'std_value': 31.398625475995402})\n",
      "2025-02-26 18:59:32,339 - snowflake_feature_store - INFO - Created feature view: customer_ltv_features with 17 features\n",
      "2025-02-26 18:59:32,352 - snowflake_feature_store - INFO - \n",
      "Created feature view: customer_ltv_features\n",
      "2025-02-26 18:59:32,353 - snowflake_feature_store - INFO - Features created: 13\n",
      "2025-02-26 18:59:32,354 - snowflake_feature_store - INFO - Transformations applied: 4\n",
      "2025-02-26 18:59:32,354 - snowflake_feature_store - INFO - \n",
      "Feature Statistics:\n",
      "2025-02-26 18:59:32,355 - snowflake_feature_store - INFO - \n",
      "LIFE_TIME_VALUE:\n",
      "2025-02-26 18:59:32,356 - snowflake_feature_store - INFO - Timestamp: 2025-02-27T02:59:11.568626\n",
      "Row count: 2400\n",
      "Null count: 0 (0.0%)\n",
      "Unique values: 2400\n",
      "Min value: 2.24\n",
      "Max value: 749.13\n",
      "Mean value: 371.09\n",
      "Std dev: 217.24\n",
      "2025-02-26 18:59:32,357 - snowflake_feature_store - INFO - \n",
      "SESSION_LENGTH:\n",
      "2025-02-26 18:59:32,357 - snowflake_feature_store - INFO - Timestamp: 2025-02-27T02:59:12.977411\n",
      "Row count: 2400\n",
      "Null count: 0 (0.0%)\n",
      "Unique values: 1908\n",
      "Min value: 0.00\n",
      "Max value: 12.22\n",
      "Mean value: 4.92\n",
      "Std dev: 3.40\n",
      "2025-02-26 18:59:32,357 - snowflake_feature_store - INFO - \n",
      "TRANSACTIONS:\n",
      "2025-02-26 18:59:32,358 - snowflake_feature_store - INFO - Timestamp: 2025-02-27T02:59:14.503363\n",
      "Row count: 2400\n",
      "Null count: 0 (0.0%)\n",
      "Unique values: 7\n",
      "Min value: 1.00\n",
      "Max value: 7.00\n",
      "Mean value: 3.37\n",
      "Std dev: 2.00\n",
      "2025-02-26 18:59:32,358 - snowflake_feature_store - INFO - \n",
      "SUM_TRANSACTIONS_7:\n",
      "2025-02-26 18:59:32,359 - snowflake_feature_store - INFO - Timestamp: 2025-02-27T02:59:16.021427\n",
      "Row count: 2400\n",
      "Null count: 0 (0.0%)\n",
      "Unique values: 39\n",
      "Min value: 1.00\n",
      "Max value: 39.00\n",
      "Mean value: 20.65\n",
      "Std dev: 7.73\n",
      "2025-02-26 18:59:32,359 - snowflake_feature_store - INFO - \n",
      "AVG_TRANSACTIONS_7:\n",
      "2025-02-26 18:59:32,360 - snowflake_feature_store - INFO - Timestamp: 2025-02-27T02:59:17.477578\n",
      "Row count: 2400\n",
      "Null count: 0 (0.0%)\n",
      "Unique values: 82\n",
      "Min value: 1.00\n",
      "Max value: 7.00\n",
      "Mean value: 3.36\n",
      "Std dev: 0.91\n",
      "2025-02-26 18:59:32,360 - snowflake_feature_store - INFO - \n",
      "SUM_LIFE_TIME_VALUE_7:\n",
      "2025-02-26 18:59:32,360 - snowflake_feature_store - INFO - Timestamp: 2025-02-27T02:59:19.190279\n",
      "Row count: 2400\n",
      "Null count: 0 (0.0%)\n",
      "Unique values: 2400\n",
      "Min value: 13.08\n",
      "Max value: 4236.68\n",
      "Mean value: 2274.69\n",
      "Std dev: 855.94\n",
      "2025-02-26 18:59:32,361 - snowflake_feature_store - INFO - \n",
      "AVG_LIFE_TIME_VALUE_7:\n",
      "2025-02-26 18:59:32,361 - snowflake_feature_store - INFO - Timestamp: 2025-02-27T02:59:20.605574\n",
      "Row count: 2400\n",
      "Null count: 0 (0.0%)\n",
      "Unique values: 2400\n",
      "Min value: 13.08\n",
      "Max value: 746.59\n",
      "Mean value: 370.01\n",
      "Std dev: 101.07\n",
      "2025-02-26 18:59:32,361 - snowflake_feature_store - INFO - \n",
      "SUM_TRANSACTIONS_30:\n",
      "2025-02-26 18:59:32,362 - snowflake_feature_store - INFO - Timestamp: 2025-02-27T02:59:22.053484\n",
      "Row count: 2400\n",
      "Null count: 0 (0.0%)\n",
      "Unique values: 106\n",
      "Min value: 1.00\n",
      "Max value: 112.00\n",
      "Mean value: 42.34\n",
      "Std dev: 24.73\n",
      "2025-02-26 18:59:32,362 - snowflake_feature_store - INFO - \n",
      "AVG_TRANSACTIONS_30:\n",
      "2025-02-26 18:59:32,362 - snowflake_feature_store - INFO - Timestamp: 2025-02-27T02:59:23.531577\n",
      "Row count: 2400\n",
      "Null count: 0 (0.0%)\n",
      "Unique values: 409\n",
      "Min value: 1.00\n",
      "Max value: 7.00\n",
      "Mean value: 3.34\n",
      "Std dev: 0.77\n",
      "2025-02-26 18:59:32,362 - snowflake_feature_store - INFO - \n",
      "SUM_LIFE_TIME_VALUE_30:\n",
      "2025-02-26 18:59:32,363 - snowflake_feature_store - INFO - Timestamp: 2025-02-27T02:59:25.307538\n",
      "Row count: 2400\n",
      "Null count: 0 (0.0%)\n",
      "Unique values: 2400\n",
      "Min value: 13.08\n",
      "Max value: 12223.46\n",
      "Mean value: 4667.15\n",
      "Std dev: 2730.69\n",
      "2025-02-26 18:59:32,363 - snowflake_feature_store - INFO - \n",
      "AVG_LIFE_TIME_VALUE_30:\n",
      "2025-02-26 18:59:32,363 - snowflake_feature_store - INFO - Timestamp: 2025-02-27T02:59:27.239169\n",
      "Row count: 2400\n",
      "Null count: 0 (0.0%)\n",
      "Unique values: 2400\n",
      "Min value: 13.08\n",
      "Max value: 746.59\n",
      "Mean value: 368.69\n",
      "Std dev: 85.84\n",
      "2025-02-26 18:59:32,364 - snowflake_feature_store - INFO - \n",
      "ENGAGEMENT_SCORE:\n",
      "2025-02-26 18:59:32,364 - snowflake_feature_store - INFO - Timestamp: 2025-02-27T02:59:29.019741\n",
      "Row count: 2400\n",
      "Null count: 0 (0.0%)\n",
      "Unique values: 2400\n",
      "Min value: 1.89\n",
      "Max value: 13.00\n",
      "Mean value: 7.12\n",
      "Std dev: 2.36\n",
      "2025-02-26 18:59:32,364 - snowflake_feature_store - INFO - \n",
      "AVG_TRANSACTION_VALUE:\n",
      "2025-02-26 18:59:32,364 - snowflake_feature_store - INFO - Timestamp: 2025-02-27T02:59:31.280304\n",
      "Row count: 2400\n",
      "Null count: 0 (0.0%)\n",
      "Unique values: 2400\n",
      "Min value: 2.24\n",
      "Max value: 199.92\n",
      "Mean value: 109.86\n",
      "Std dev: 31.40\n"
     ]
    }
   ],
   "source": [
    "# 1. Create feature view config\n",
    "entity_name = \"CUSTOMER\"\n",
    "feature_view_name = \"customer_ltv_features\"\n",
    "\n",
    "# 2. Create feature view config\n",
    "config = FeatureViewConfig(\n",
    "    name=feature_view_name,\n",
    "    domain=\"RETAIL\",\n",
    "    entity=entity_name,\n",
    "    feature_type=\"BEHAVIOR\",\n",
    "    refresh=RefreshConfig(\n",
    "        frequency=\"1 day\",\n",
    "        mode=\"INCREMENTAL\"\n",
    "    ),\n",
    "    features=feature_configs,  #  Created Above Pass the dictionary of FeatureConfigs\n",
    "    description=\"\"\"\n",
    "    Customer LTV prediction features combining:\n",
    "    - Transaction history\n",
    "    - Engagement metrics\n",
    "    - Time-based patterns\n",
    "    \n",
    "    Updated daily with incremental processing.\n",
    "    Use for LTV prediction and customer segmentation.\n",
    "    \"\"\".strip(),\n",
    "    timestamp_col=\"DATE\"\n",
    ")\n",
    "\n",
    "# 3. Create transformations\n",
    "# Created above\n",
    "\n",
    "# 4. Create and register feature view\n",
    "feature_view = manager.add_feature_view(\n",
    "    config=config,\n",
    "    df=df, # Original DataFrame above\n",
    "    entity_name=entity_name,\n",
    "    transforms=transforms,\n",
    "    collect_stats=True  # Enable monitoring\n",
    ")\n",
    "\n",
    "# 5. Log feature view details\n",
    "logger.info(f\"\\nCreated feature view: {feature_view_name}\")\n",
    "logger.info(f\"Features created: {len(feature_configs)}\")\n",
    "logger.info(f\"Transformations applied: {len(transforms)}\")\n",
    "\n",
    "# 6. Show feature statistics\n",
    "logger.info(\"\\nFeature Statistics:\")\n",
    "for feature_name, stats in manager.feature_stats[config.name].items():\n",
    "    logger.info(f\"\\n{feature_name}:\")\n",
    "    logger.info(str(stats))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Practices for Feature Views\n",
    "\n",
    "1. **Documentation**\n",
    "   - Clear descriptions\n",
    "   - Usage examples\n",
    "   - Update frequency\n",
    "   - Dependencies\n",
    "\n",
    "2. **Monitoring**\n",
    "   - Feature statistics\n",
    "   - Data quality metrics\n",
    "   - Refresh status\n",
    "   - Drift detection\n",
    "\n",
    "3. **Performance**\n",
    "   - Incremental updates\n",
    "   - Efficient transformations\n",
    "   - Appropriate refresh schedule\n",
    "\n",
    "4. **Governance**\n",
    "   - Access controls\n",
    "   - Version control\n",
    "   - Audit logging\n",
    "   - Data lineage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Feature Monitoring\n",
    "\n",
    "Monitoring is crucial for maintaining feature quality and detecting issues early.\n",
    "\n",
    "### Why Monitor Features?\n",
    "\n",
    "Feature monitoring helps:\n",
    "1. **Detect Data Quality Issues**: Missing values, outliers, type mismatches\n",
    "2. **Track Feature Drift**: Changes in feature distributions\n",
    "3. **Ensure Freshness**: Verify timely updates\n",
    "4. **Validate Business Rules**: Check domain-specific constraints\n",
    "\n",
    "### Types of Monitoring\n",
    "\n",
    "1. **Data Quality**\n",
    "   - Null ratios\n",
    "   - Type consistency\n",
    "   - Value ranges\n",
    "   - Cardinality\n",
    "\n",
    "2. **Statistical Monitoring**\n",
    "   - Distribution shifts\n",
    "   - Correlation changes\n",
    "   - Seasonality patterns\n",
    "   - Outlier detection\n",
    "\n",
    "3. **Operational Monitoring**\n",
    "   - Refresh status\n",
    "   - Computation time\n",
    "   - Resource usage\n",
    "   - Error rates\n",
    "\n",
    "### LTV-Specific Monitoring\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from typing import Union, List, Callable, Optional, Protocol, Dict, Any\n",
    "\n",
    "import decimal\n",
    "from typing import Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LTVMonitor:\n",
    "    \"\"\"Monitor for LTV feature quality and drift\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        manager: FeatureStoreManager,\n",
    "        feature_view_name: str,\n",
    "        metrics_path: Optional[str] = None\n",
    "    ):\n",
    "        self.manager = manager\n",
    "        self.feature_view_name = feature_view_name\n",
    "        self.metrics_path = metrics_path\n",
    "        self.baseline_stats = {}\n",
    "        \n",
    "    def _convert_decimal(self, obj: Any) -> Any:\n",
    "        \"\"\"Convert Decimal objects to float for JSON serialization\"\"\"\n",
    "        if isinstance(obj, decimal.Decimal):\n",
    "            return float(obj)\n",
    "        return obj\n",
    "    \n",
    "    def _process_metrics(self, metrics: Dict) -> Dict:\n",
    "        \"\"\"Process metrics dictionary to ensure JSON serializable values\"\"\"\n",
    "        return {\n",
    "            k: {\n",
    "                'timestamp': v['timestamp'],\n",
    "                'metrics': {\n",
    "                    mk: self._convert_decimal(mv)\n",
    "                    for mk, mv in v['metrics'].items()\n",
    "                }\n",
    "            }\n",
    "            for k, v in metrics.items()\n",
    "        }\n",
    "    \n",
    "    def compute_feature_metrics(\n",
    "        self,\n",
    "        df: DataFrame,\n",
    "        timestamp: Optional[datetime] = None\n",
    "    ) -> Dict[str, Dict]:\n",
    "        \"\"\"Compute comprehensive feature metrics\"\"\"\n",
    "        metrics = {}\n",
    "        timestamp = timestamp or datetime.now()\n",
    "        \n",
    "        for col in df.columns:\n",
    "            # Skip identifier columns\n",
    "            if col in ['CUSTOMER_ID', 'DATE']:\n",
    "                continue\n",
    "                \n",
    "            # Basic stats\n",
    "            stats = df.select([\n",
    "                F.count(col).alias('count'),\n",
    "                F.count_distinct(col).alias('unique'),\n",
    "                F.sum(F.when(F.col(col).is_null(), 1).otherwise(0)).alias('nulls')\n",
    "            ]).collect()[0].asDict()\n",
    "            \n",
    "            # Convert Decimal to float\n",
    "            stats = {k: self._convert_decimal(v) for k, v in stats.items()}\n",
    "            \n",
    "            # Numeric stats for appropriate columns\n",
    "            if col in ['LIFE_TIME_VALUE', 'SESSION_LENGTH', 'TRANSACTIONS']:\n",
    "                numeric_stats = df.select([\n",
    "                    F.min(col).alias('min'),\n",
    "                    F.max(col).alias('max'),\n",
    "                    F.avg(col).alias('mean'),\n",
    "                    F.stddev(col).alias('std')\n",
    "                ]).collect()[0].asDict()\n",
    "                \n",
    "                # Convert Decimal to float\n",
    "                numeric_stats = {k: self._convert_decimal(v) for k, v in numeric_stats.items()}\n",
    "                stats.update(numeric_stats)\n",
    "            \n",
    "            metrics[col] = {\n",
    "                'timestamp': timestamp.isoformat(),\n",
    "                'metrics': stats\n",
    "            }\n",
    "            \n",
    "        return metrics\n",
    "    \n",
    "    def set_baseline(self, df: DataFrame) -> None:\n",
    "        \"\"\"Set baseline statistics for drift detection\"\"\"\n",
    "        self.baseline_stats = self.compute_feature_metrics(df)\n",
    "        logger.info(\"Set baseline statistics\")\n",
    "        \n",
    "        # Save baseline if metrics path provided\n",
    "        if self.metrics_path:\n",
    "            baseline_file = Path(self.metrics_path) / \"baseline_stats.json\"\n",
    "            processed_stats = self._process_metrics(self.baseline_stats)\n",
    "            with open(baseline_file, 'w') as f:\n",
    "                json.dump(processed_stats, f, indent=2)\n",
    "    \n",
    "    def check_feature_health(\n",
    "        self,\n",
    "        df: DataFrame,\n",
    "        drift_threshold: float = 0.1\n",
    "    ) -> None:\n",
    "        \"\"\"Check overall feature health\"\"\"\n",
    "        try:\n",
    "            # Compute current metrics\n",
    "            current_metrics = self.compute_feature_metrics(df)\n",
    "            \n",
    "            # Detect drift if baseline exists\n",
    "            if self.baseline_stats:\n",
    "                drift_alerts = self.detect_drift(\n",
    "                    current_metrics,\n",
    "                    drift_threshold\n",
    "                )\n",
    "                \n",
    "                if drift_alerts:\n",
    "                    logger.warning(\"\\nFeature Drift Detected:\")\n",
    "                    for feature, alerts in drift_alerts.items():\n",
    "                        logger.warning(f\"\\n{feature}:\")\n",
    "                        for alert in alerts:\n",
    "                            logger.warning(f\"- {alert}\")\n",
    "            \n",
    "            # Log current metrics\n",
    "            logger.info(\"\\nCurrent Feature Metrics:\")\n",
    "            for feature, metrics in current_metrics.items():\n",
    "                logger.info(f\"\\n{feature}:\")\n",
    "                for metric, value in metrics['metrics'].items():\n",
    "                    logger.info(f\"  {metric}: {value}\")\n",
    "                    \n",
    "            # Save metrics if path provided\n",
    "            if self.metrics_path:\n",
    "                metrics_file = Path(self.metrics_path) / f\"metrics_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "                processed_metrics = self._process_metrics(current_metrics)\n",
    "                with open(metrics_file, 'w') as f:\n",
    "                    json.dump(processed_metrics, f, indent=2)\n",
    "                    \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error checking feature health: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def detect_drift(\n",
    "        self,\n",
    "        current_metrics: Dict,\n",
    "        drift_threshold: float = 0.1\n",
    "    ) -> Dict[str, List[str]]:\n",
    "        \"\"\"Detect significant changes in feature distributions\"\"\"\n",
    "        drift_alerts = {}\n",
    "        \n",
    "        for feature, metrics in current_metrics.items():\n",
    "            if feature not in self.baseline_stats:\n",
    "                continue\n",
    "                \n",
    "            alerts = []\n",
    "            baseline = self.baseline_stats[feature]['metrics']\n",
    "            current = metrics['metrics']\n",
    "            \n",
    "            # Check for distribution changes\n",
    "            for metric in ['mean', 'std']:\n",
    "                if metric not in current or metric not in baseline:\n",
    "                    continue\n",
    "                    \n",
    "                change = abs(current[metric] - baseline[metric]) / baseline[metric]\n",
    "                if change > drift_threshold:\n",
    "                    alerts.append(\n",
    "                        f\"{metric.upper()} changed by {change:.1%}\"\n",
    "                    )\n",
    "            \n",
    "            # Check for data quality changes\n",
    "            null_ratio = current['NULLS'] / current['COUNT']\n",
    "            baseline_null_ratio = baseline['NULLS'] / baseline['COUNT']\n",
    "            if abs(null_ratio - baseline_null_ratio) > drift_threshold:\n",
    "                alerts.append(\n",
    "                    f\"NULL ratio changed from {baseline_null_ratio:.1%} to {null_ratio:.1%}\"\n",
    "                )\n",
    "            \n",
    "            if alerts:\n",
    "                drift_alerts[feature] = alerts\n",
    "                \n",
    "        return drift_alerts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Usage\n",
    "\n",
    "Let's set up monitoring for our LTV features:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up metrics directory\n",
    "metrics_dir = Path(\"feature_metrics\")\n",
    "metrics_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up monitoring\n",
    "monitor = LTVMonitor(\n",
    "    manager=manager,\n",
    "    feature_view_name=feature_view.name,\n",
    "    metrics_path=str(metrics_dir)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake_feature_store.examples import get_example_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-26 19:00:15,844 - snowflake_feature_store - INFO - Set baseline statistics\n",
      "2025-02-26 19:00:22,871 - snowflake_feature_store - INFO - Generated 6000 rows of demo data in \"DATASCIENCE\".FEATURE_STORE_DEMO.CUSTOMER_ACTIVITY_TEST\n",
      "2025-02-26 19:00:22,874 - snowflake_feature_store - INFO - \n",
      "Sample Data:\n",
      "---------------------------------------------------------------------------------------------------------------------------------\n",
      "|\"CUSTOMER_ID\"  |\"DATE\"      |\"LIFE_TIME_VALUE\"  |\"SESSION_LENGTH\"   |\"TIME_ON_APP\"       |\"TIME_ON_WEBSITE\"   |\"TRANSACTIONS\"  |\n",
      "---------------------------------------------------------------------------------------------------------------------------------\n",
      "|C26            |2025-01-27  |564.6091706355498  |7.616530664264483  |12.142466259190991  |10.138495747448552  |5               |\n",
      "|C85            |2025-02-25  |59.41299820191271  |4.727356127459224  |5.495725181799517   |3.859344227079125   |1               |\n",
      "|C72            |2025-02-25  |646.3111447043822  |8.908367451703572  |9.728204471331994   |13.191869376320893  |6               |\n",
      "|C60            |2025-02-25  |447.0367329096978  |8.704973243200099  |10.936684313415565  |11.222118890927462  |4               |\n",
      "|C48            |2025-02-25  |294.3029664683961  |NULL               |5.2542111627227275  |7.048625379945507   |2               |\n",
      "---------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "2025-02-26 19:00:23,220 - snowflake_feature_store - INFO - \n",
      "Schema:\n",
      "2025-02-26 19:00:23,346 - snowflake_feature_store - INFO - CUSTOMER_ID: StringType()\n",
      "2025-02-26 19:00:23,348 - snowflake_feature_store - INFO - DATE: DateType()\n",
      "2025-02-26 19:00:23,349 - snowflake_feature_store - INFO - LIFE_TIME_VALUE: DoubleType()\n",
      "2025-02-26 19:00:23,350 - snowflake_feature_store - INFO - SESSION_LENGTH: DoubleType()\n",
      "2025-02-26 19:00:23,351 - snowflake_feature_store - INFO - TIME_ON_APP: DoubleType()\n",
      "2025-02-26 19:00:23,352 - snowflake_feature_store - INFO - TIME_ON_WEBSITE: DoubleType()\n",
      "2025-02-26 19:00:23,353 - snowflake_feature_store - INFO - TRANSACTIONS: LongType()\n",
      "2025-02-26 19:00:25,574 - snowflake_feature_store - WARNING - \n",
      "Feature Drift Detected:\n",
      "2025-02-26 19:00:25,576 - snowflake_feature_store - WARNING - \n",
      "SESSION_LENGTH:\n",
      "2025-02-26 19:00:25,577 - snowflake_feature_store - WARNING - - NULL ratio changed from 0.0% to 25.9%\n",
      "2025-02-26 19:00:25,577 - snowflake_feature_store - INFO - \n",
      "Current Feature Metrics:\n",
      "2025-02-26 19:00:25,578 - snowflake_feature_store - INFO - \n",
      "LIFE_TIME_VALUE:\n",
      "2025-02-26 19:00:25,579 - snowflake_feature_store - INFO -   COUNT: 2400\n",
      "2025-02-26 19:00:25,580 - snowflake_feature_store - INFO -   UNIQUE: 2400\n",
      "2025-02-26 19:00:25,580 - snowflake_feature_store - INFO -   NULLS: 0\n",
      "2025-02-26 19:00:25,581 - snowflake_feature_store - INFO -   MIN: 2.239592053174099\n",
      "2025-02-26 19:00:25,581 - snowflake_feature_store - INFO -   MAX: 749.1277222421559\n",
      "2025-02-26 19:00:25,582 - snowflake_feature_store - INFO -   MEAN: 371.0923268443173\n",
      "2025-02-26 19:00:25,583 - snowflake_feature_store - INFO -   STD: 217.24439799534923\n",
      "2025-02-26 19:00:25,583 - snowflake_feature_store - INFO - \n",
      "SESSION_LENGTH:\n",
      "2025-02-26 19:00:25,584 - snowflake_feature_store - INFO -   COUNT: 1907\n",
      "2025-02-26 19:00:25,585 - snowflake_feature_store - INFO -   UNIQUE: 1907\n",
      "2025-02-26 19:00:25,585 - snowflake_feature_store - INFO -   NULLS: 493\n",
      "2025-02-26 19:00:25,586 - snowflake_feature_store - INFO -   MIN: 0.08102027782085003\n",
      "2025-02-26 19:00:25,587 - snowflake_feature_store - INFO -   MAX: 12.219298377916665\n",
      "2025-02-26 19:00:25,587 - snowflake_feature_store - INFO -   MEAN: 6.197450441057352\n",
      "2025-02-26 19:00:25,588 - snowflake_feature_store - INFO -   STD: 2.584360647215551\n",
      "2025-02-26 19:00:25,588 - snowflake_feature_store - INFO - \n",
      "TIME_ON_APP:\n",
      "2025-02-26 19:00:25,589 - snowflake_feature_store - INFO -   COUNT: 2400\n",
      "2025-02-26 19:00:25,589 - snowflake_feature_store - INFO -   UNIQUE: 2400\n",
      "2025-02-26 19:00:25,589 - snowflake_feature_store - INFO -   NULLS: 0\n",
      "2025-02-26 19:00:25,590 - snowflake_feature_store - INFO - \n",
      "TIME_ON_WEBSITE:\n",
      "2025-02-26 19:00:25,591 - snowflake_feature_store - INFO -   COUNT: 2400\n",
      "2025-02-26 19:00:25,592 - snowflake_feature_store - INFO -   UNIQUE: 2400\n",
      "2025-02-26 19:00:25,592 - snowflake_feature_store - INFO -   NULLS: 0\n",
      "2025-02-26 19:00:25,593 - snowflake_feature_store - INFO - \n",
      "TRANSACTIONS:\n",
      "2025-02-26 19:00:25,593 - snowflake_feature_store - INFO -   COUNT: 2400\n",
      "2025-02-26 19:00:25,593 - snowflake_feature_store - INFO -   UNIQUE: 7\n",
      "2025-02-26 19:00:25,594 - snowflake_feature_store - INFO -   NULLS: 0\n",
      "2025-02-26 19:00:25,594 - snowflake_feature_store - INFO -   MIN: 1\n",
      "2025-02-26 19:00:25,595 - snowflake_feature_store - INFO -   MAX: 7\n",
      "2025-02-26 19:00:25,595 - snowflake_feature_store - INFO -   MEAN: 3.366667\n",
      "2025-02-26 19:00:25,596 - snowflake_feature_store - INFO -   STD: 2.0001389951700856\n"
     ]
    }
   ],
   "source": [
    "# Set baseline\n",
    "monitor.set_baseline(feature_view.feature_df)\n",
    "# Generate some drift\n",
    "drift_df = get_example_data(\n",
    "    conn.session,\n",
    "    schema,\n",
    "    num_customers=250,\n",
    "    ltv_multiplier=4.5,  # Increase values to simulate drift\n",
    "    table_type = 'TEST'\n",
    ")\n",
    "# Check for drift\n",
    "monitor.check_feature_health(drift_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Training Data Generation\n",
    "\n",
    "Generating training data from a feature store requires special consideration to avoid data leakage and ensure point-in-time correctness.\n",
    "\n",
    "### Why Training Data Generation Matters\n",
    "\n",
    "Proper training data generation:\n",
    "1. **Prevents Data Leakage**: Ensures future data doesn't leak into training\n",
    "2. **Maintains Consistency**: Uses same feature computations as production\n",
    "3. **Enables Reproducibility**: Training sets can be recreated exactly\n",
    "4. **Supports Experimentation**: Easy to create different feature combinations\n",
    "\n",
    "### LTV Training Data Requirements\n",
    "\n",
    "For LTV prediction, we need to:\n",
    "1. Use historical data to predict future LTV\n",
    "2. Include time-based features correctly\n",
    "3. Handle missing values consistently\n",
    "4. Maintain customer context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-26 19:01:11,456 - snowflake_feature_store - INFO - FeatureStoreManager initialized\n"
     ]
    }
   ],
   "source": [
    "import tempfile\n",
    "from pathlib import Path\n",
    "\n",
    "metrics_dir = Path(tempfile.mkdtemp()) / \"feature_store_metrics\"\n",
    "\n",
    "training_start_date='2025-01-01'\n",
    "training_end_date='2025-03-01'\n",
    "prediction_window=90  # Predict 90-day LTV\n",
    "save_table='DATASCIENCE.FEATURE_STORE_DEMO.LTV_TRAINING_DATA'\n",
    "\n",
    "manager = FeatureStoreManager(\n",
    "    connection=conn,\n",
    "    metrics_path=metrics_dir,\n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "# Get existing feature view\n",
    "feature_view = manager.feature_store.get_feature_view(\n",
    "    name=\"customer_ltv_features\",\n",
    "    version=\"V1_0\"  # Use the version from earlier\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------\n",
      "|\"CUSTOMER_ID\"  |\"DATE\"      |\"TARGET_LTV\"       |\"LABEL_DATE\"  |\n",
      "-----------------------------------------------------------------\n",
      "|C26            |2025-01-27  |740.132663156064   |2025-02-25    |\n",
      "|C85            |2025-02-25  |59.41299820191271  |2025-02-25    |\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "2025-02-26 19:01:24,224 - snowflake_feature_store - INFO - Created spine with 2400 rows\n"
     ]
    }
   ],
   "source": [
    "# Get the fully qualified table name\n",
    "table_name = (\n",
    "    f\"{manager.connection.database}.\"\n",
    "    f\"{manager.connection.schema}.\"\n",
    "    f\"CUSTOMER_ACTIVITY\"\n",
    ")\n",
    "\n",
    "# 1. Create spine query for point-in-time correct features\n",
    "spine_df = manager.connection.session.sql(f\"\"\"\n",
    "    WITH customer_dates AS (\n",
    "        -- Get all customer-date combinations\n",
    "    SELECT DISTINCT\n",
    "            CUSTOMER_ID,\n",
    "            DATE\n",
    "        FROM {table_name}\n",
    "        WHERE DATE BETWEEN '{training_start_date}' AND '{training_end_date}'\n",
    "    ),\n",
    "    future_ltv AS (\n",
    "        -- Calculate future LTV for each customer-date\n",
    "        SELECT \n",
    "            cd.CUSTOMER_ID,\n",
    "            cd.DATE as FEATURE_DATE,\n",
    "            MAX(f.DATE) as LABEL_DATE,\n",
    "            MAX(f.LIFE_TIME_VALUE) as FUTURE_LTV\n",
    "        FROM customer_dates cd\n",
    "        LEFT JOIN {table_name} f\n",
    "            ON cd.CUSTOMER_ID = f.CUSTOMER_ID\n",
    "            AND f.DATE BETWEEN cd.DATE \n",
    "                AND DATEADD(days, {prediction_window}, cd.DATE)\n",
    "        GROUP BY 1, 2\n",
    ")\n",
    "    -- Final spine query\n",
    "    SELECT \n",
    "        CUSTOMER_ID,\n",
    "        FEATURE_DATE as \"DATE\",\n",
    "        FUTURE_LTV as \"TARGET_LTV\",\n",
    "        LABEL_DATE as \"LABEL_DATE\"\n",
    "    FROM future_ltv\n",
    "\"\"\")\n",
    "spine_df.show(2)\n",
    "\n",
    "logger.info(f\"Created spine with {spine_df.count()} rows\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-26 19:01:30,389 - snowflake_feature_store - INFO - Spine DataFrame columns: ['CUSTOMER_ID', 'DATE', 'TARGET_LTV', 'LABEL_DATE']\n",
      "2025-02-26 19:01:30,390 - snowflake_feature_store - INFO - Spine DataFrame schema: StructType([StructField('CUSTOMER_ID', StringType(), nullable=True), StructField('DATE', DateType(), nullable=True), StructField('TARGET_LTV', DoubleType(), nullable=True), StructField('LABEL_DATE', DateType(), nullable=True)])\n",
      "2025-02-26 19:01:30,390 - snowflake_feature_store - INFO - Generating dataset with name: DATASET_20250227_030130_63cf45e6\n",
      "2025-02-26 19:01:30,390 - snowflake_feature_store - INFO - Label columns: ['\"TARGET_LTV\"', '\"LABEL_DATE\"']\n",
      "2025-02-26 19:01:30,390 - snowflake_feature_store - INFO - Timestamp column: \"DATE\"\n",
      "2025-02-26 19:01:34,597 - snowflake_feature_store - INFO - \n",
      "Sample Data:\n",
      "---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|\"CUSTOMER_ID\"  |\"DATE\"      |\"TARGET_LTV\"       |\"LABEL_DATE\"  |\"LIFE_TIME_VALUE\"   |\"SESSION_LENGTH\"   |\"TIME_ON_APP\"      |\"TIME_ON_WEBSITE\"   |\"TRANSACTIONS\"  |\"SUM_TRANSACTIONS_7\"  |\"AVG_TRANSACTIONS_7\"  |\"SUM_TRANSACTIONS_30\"  |\"AVG_TRANSACTIONS_30\"  |\"SUM_LIFE_TIME_VALUE_7\"  |\"AVG_LIFE_TIME_VALUE_7\"  |\"SUM_LIFE_TIME_VALUE_30\"  |\"AVG_LIFE_TIME_VALUE_30\"  |\"ENGAGEMENT_SCORE\"  |\"AVG_TRANSACTION_VALUE\"  |\n",
      "---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|C54            |2025-01-27  |748.4986572265625  |2025-02-25    |473.930419921875    |8.07875919342041   |8.7145357131958    |11.581034660339355  |4               |4.0                   |4.0                   |4.0                    |4.0                    |473.930419921875         |473.930419921875         |473.930419921875          |473.930419921875          |9.458109855651855   |118.48260498046875       |\n",
      "|C54            |2025-01-28  |748.4986572265625  |2025-02-25    |258.25628662109375  |5.443594455718994  |6.002151012420654  |5.871957778930664   |2               |6.0                   |3.0                   |6.0                    |3.0                    |732.1867065429688        |366.0933532714844        |732.1867065429688         |366.0933532714844         |5.7725677490234375  |129.12814331054688       |\n",
      "---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "2025-02-26 19:01:35,803 - snowflake_feature_store - INFO - \n",
      "Schema:\n",
      "2025-02-26 19:01:35,805 - snowflake_feature_store - INFO - CUSTOMER_ID: StringType()\n",
      "2025-02-26 19:01:35,805 - snowflake_feature_store - INFO - DATE: DateType()\n",
      "2025-02-26 19:01:35,806 - snowflake_feature_store - INFO - TARGET_LTV: DoubleType()\n",
      "2025-02-26 19:01:35,807 - snowflake_feature_store - INFO - LABEL_DATE: DateType()\n",
      "2025-02-26 19:01:35,807 - snowflake_feature_store - INFO - LIFE_TIME_VALUE: DoubleType()\n",
      "2025-02-26 19:01:35,808 - snowflake_feature_store - INFO - SESSION_LENGTH: DoubleType()\n",
      "2025-02-26 19:01:35,809 - snowflake_feature_store - INFO - TIME_ON_APP: DoubleType()\n",
      "2025-02-26 19:01:35,809 - snowflake_feature_store - INFO - TIME_ON_WEBSITE: DoubleType()\n",
      "2025-02-26 19:01:35,810 - snowflake_feature_store - INFO - TRANSACTIONS: LongType()\n",
      "2025-02-26 19:01:35,811 - snowflake_feature_store - INFO - SUM_TRANSACTIONS_7: DoubleType()\n",
      "2025-02-26 19:01:35,811 - snowflake_feature_store - INFO - AVG_TRANSACTIONS_7: DoubleType()\n",
      "2025-02-26 19:01:35,812 - snowflake_feature_store - INFO - SUM_TRANSACTIONS_30: DoubleType()\n",
      "2025-02-26 19:01:35,812 - snowflake_feature_store - INFO - AVG_TRANSACTIONS_30: DoubleType()\n",
      "2025-02-26 19:01:35,813 - snowflake_feature_store - INFO - SUM_LIFE_TIME_VALUE_7: DoubleType()\n",
      "2025-02-26 19:01:35,813 - snowflake_feature_store - INFO - AVG_LIFE_TIME_VALUE_7: DoubleType()\n",
      "2025-02-26 19:01:35,814 - snowflake_feature_store - INFO - SUM_LIFE_TIME_VALUE_30: DoubleType()\n",
      "2025-02-26 19:01:35,814 - snowflake_feature_store - INFO - AVG_LIFE_TIME_VALUE_30: DoubleType()\n",
      "2025-02-26 19:01:35,815 - snowflake_feature_store - INFO - ENGAGEMENT_SCORE: DoubleType()\n",
      "2025-02-26 19:01:35,815 - snowflake_feature_store - INFO - AVG_TRANSACTION_VALUE: DoubleType()\n"
     ]
    }
   ],
   "source": [
    "# 2. Get features using point-in-time correct joins\n",
    "training_data = manager.get_features(\n",
    "    spine_df=spine_df,\n",
    "    feature_views=[feature_view],\n",
    "    spine_timestamp_col=\"DATE\",\n",
    "    label_cols=[\"TARGET_LTV\", \"LABEL_DATE\"]\n",
    ")\n",
    "logger.info(\"\\nSample Data:\")\n",
    "training_data.show(2)\n",
    "logger.info(\"\\nSchema:\")\n",
    "for field in training_data.schema.fields:\n",
    "    logger.info(f\"{field.name}: {field.datatype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-26 19:01:40,964 - snowflake_feature_store - INFO - Saved training data to DATASCIENCE.FEATURE_STORE_DEMO.LTV_TRAINING_DATA\n",
      "2025-02-26 19:01:40,965 - snowflake_feature_store - INFO - \n",
      "Training Data Statistics:\n",
      "2025-02-26 19:01:41,855 - snowflake_feature_store - INFO - Total rows: 2400\n",
      "2025-02-26 19:01:41,857 - snowflake_feature_store - INFO - Date range: 2025-01-01 to 2025-03-01\n",
      "2025-02-26 19:01:41,858 - snowflake_feature_store - INFO - Prediction window: 90 days\n"
     ]
    }
   ],
   "source": [
    "# 3. Add metadata columns\n",
    "training_data = training_data.select(\n",
    "    \"*\",  # Keep all existing columns\n",
    "    F.lit(training_start_date).alias(\"TRAINING_START_DATE\"),\n",
    "    F.lit(training_end_date).alias(\"TRAINING_END_DATE\"),\n",
    "    F.lit(prediction_window).alias(\"PREDICTION_WINDOW_DAYS\"),\n",
    "    F.current_timestamp().alias(\"GENERATED_AT\")\n",
    ")\n",
    "\n",
    "# 4. Save if table name provided\n",
    "if save_table:\n",
    "    training_data.write.mode(\"overwrite\").save_as_table(save_table)\n",
    "    logger.info(f\"Saved training data to {save_table}\")\n",
    "\n",
    "# 5. Log data generation stats\n",
    "logger.info(\"\\nTraining Data Statistics:\")\n",
    "logger.info(f\"Total rows: {training_data.count()}\")\n",
    "logger.info(f\"Date range: {training_start_date} to {training_end_date}\")\n",
    "logger.info(f\"Prediction window: {prediction_window} days\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -rf feature_metrics/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Practices for Training Data\n",
    "\n",
    "1. **Time Windows**\n",
    "   - Use appropriate training/validation splits\n",
    "   - Consider seasonal patterns\n",
    "   - Match prediction window to business needs\n",
    "\n",
    "2. **Feature Selection**\n",
    "   - Include all relevant features\n",
    "   - Document feature importance\n",
    "   - Track feature dependencies\n",
    "\n",
    "3. **Data Quality**\n",
    "   - Handle missing values consistently\n",
    "   - Check for data leakage\n",
    "   - Validate label quality\n",
    "\n",
    "4. **Documentation**\n",
    "   - Record generation parameters\n",
    "   - Track data lineage\n",
    "   - Document assumptions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion and Next Steps\n",
    "\n",
    "### What We've Built\n",
    "\n",
    "We've created a comprehensive example of using Snowflake's Feature Store for LTV prediction that demonstrates:\n",
    "1. **Feature Store Setup**: Creating and managing a feature store\n",
    "2. **Entity Management**: Defining and documenting customer entities\n",
    "3. **Feature Engineering**: Creating and transforming features\n",
    "4. **Feature Views**: Organizing and versioning features\n",
    "5. **Monitoring**: Tracking feature quality and drift\n",
    "6. **Training Data**: Generating point-in-time correct datasets\n",
    "\n",
    "### Potential Enhancements\n",
    "\n",
    "Future versions could include:\n",
    "1. **Advanced Monitoring**\n",
    "   - Automated drift detection alerts\n",
    "   - Custom validation rules\n",
    "   - Feature quality dashboards\n",
    "   - Historical metrics tracking\n",
    "\n",
    "2. **Feature Discovery**\n",
    "   - Feature search capabilities\n",
    "   - Metadata management\n",
    "   - Usage tracking\n",
    "   - Documentation generation\n",
    "\n",
    "3. **Production Integration**\n",
    "   - CI/CD pipeline integration\n",
    "   - Automated testing\n",
    "   - Deployment workflows\n",
    "   - Model registry integration\n",
    "\n",
    "4. **Performance Optimization**\n",
    "   - Incremental updates\n",
    "   - Caching strategies\n",
    "   - Query optimization\n",
    "   - Resource management\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "When using this template:\n",
    "1. **Documentation**\n",
    "   - Document feature definitions\n",
    "   - Explain business logic\n",
    "   - Track dependencies\n",
    "   - Maintain version history\n",
    "\n",
    "2. **Testing**\n",
    "   - Validate feature logic\n",
    "   - Check data quality\n",
    "   - Test transformations\n",
    "   - Verify point-in-time correctness\n",
    "\n",
    "3. **Monitoring**\n",
    "   - Set up drift detection\n",
    "   - Track feature freshness\n",
    "   - Monitor data quality\n",
    "   - Alert on issues\n",
    "\n",
    "4. **Governance**\n",
    "   - Manage access controls\n",
    "   - Track lineage\n",
    "   - Enforce standards\n",
    "   - Maintain audit logs\n",
    "\n",
    "### Using This Template\n",
    "\n",
    "To adapt this example:\n",
    "1. Replace LTV-specific logic with your use case\n",
    "2. Adjust feature definitions and transformations\n",
    "3. Customize monitoring thresholds\n",
    "4. Add domain-specific validation\n",
    "\n",
    "### Resources\n",
    "\n",
    "For more information:\n",
    "1. [Snowflake Feature Store Documentation](https://docs.snowflake.com/en/user-guide/feature-store)\n",
    "2. [Feature Store Best Practices](https://docs.snowflake.com/en/user-guide/feature-store-use)\n",
    "3. [Snowpark ML Documentation](https://docs.snowflake.com/en/developer-guide/snowpark-ml/index)\n",
    "4. [Feature Store Examples](https://github.com/Snowflake-Labs/snowpark-python-examples)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "feature-store",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
