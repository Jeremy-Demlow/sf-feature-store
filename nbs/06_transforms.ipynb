{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Transforms\n",
    "\n",
    "> This module defines a comprehensive feature transformation framework for data preprocessing in Snowflake, providing configurable and composable data transformations with built-in validation. It implements various transform types (window operations, null filling, date calculations, moving and cumulative aggregations) all following a consistent Transform protocol, and supports data quality checks through a ValidationMixin that enforces constraints on null ratios, cardinality, and data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from __future__ import annotations\n",
    "from typing import Union, List, Callable, Optional, Protocol, Dict\n",
    "from dataclasses import dataclass\n",
    "from fastcore.basics import listify\n",
    "import snowflake.snowpark.functions as F\n",
    "from snowflake.snowpark import DataFrame, Window\n",
    "from pydantic import BaseModel, Field, field_validator\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Import our new modules\n",
    "from snowflake_feature_store.exceptions import ValidationError\n",
    "from snowflake_feature_store.logging import logger\n",
    "from snowflake_feature_store.config import BaseModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class TransformConfig(BaseModel):\n",
    "    \"\"\"Configuration for data transformations\"\"\"\n",
    "    name: str\n",
    "    description: Optional[str] = None\n",
    "    validate_output: bool = Field(True, description=\"Whether to validate transform output\")\n",
    "    \n",
    "    # Validation settings\n",
    "    null_threshold: float = Field(0.1, ge=0, le=1, description=\"Maximum allowed null ratio\")\n",
    "    cardinality_threshold: Optional[int] = Field(None, description=\"Maximum distinct values\")\n",
    "    expected_types: Optional[List[str]] = Field(\n",
    "        None, \n",
    "        description=\"List of acceptable types (e.g., ['DECIMAL', 'DOUBLE', 'NUMBER'] for numeric)\"\n",
    "    )\n",
    "\n",
    "    @field_validator('expected_types')\n",
    "    @classmethod\n",
    "    def validate_types(cls, v):\n",
    "        if v is not None:\n",
    "            valid_types = {'DECIMAL', 'DOUBLE', 'NUMBER', 'INT', 'LONG', 'STRING', 'BOOLEAN', 'DATE', 'TIMESTAMP'}\n",
    "            for t in v:\n",
    "                if t.upper() not in valid_types:\n",
    "                    raise ValueError(f\"Invalid type: {t}. Must be one of {valid_types}\")\n",
    "        return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Transform(Protocol):\n",
    "    \"\"\"Protocol for feature transformations\"\"\"\n",
    "    def __call__(self, df: DataFrame) -> DataFrame: ...\n",
    "    \n",
    "    @property\n",
    "    def config(self) -> TransformConfig: ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ValidationMixin:\n",
    "    \"\"\"Mixin class providing validation methods for transforms\"\"\"\n",
    "    def validate_dataframe(self, df: DataFrame, columns: List[str]) -> None:\n",
    "        \"\"\"Validate DataFrame columns against configuration\"\"\"\n",
    "        if not hasattr(self, 'config'):\n",
    "            return\n",
    "            \n",
    "        try:\n",
    "            # Check for nulls\n",
    "            if self.config.null_threshold < 1.0:\n",
    "                for col in columns:\n",
    "                    null_count = df.filter(F.col(col).is_null()).count()\n",
    "                    total_count = df.count()\n",
    "                    null_ratio = null_count / total_count if total_count > 0 else 0\n",
    "                    \n",
    "                    if null_ratio > self.config.null_threshold:\n",
    "                        raise ValidationError(\n",
    "                            f\"Column {col} has {null_ratio:.1%} null values, \"\n",
    "                            f\"exceeding threshold of {self.config.null_threshold:.1%}\"\n",
    "                        )\n",
    "                        \n",
    "            # Check cardinality\n",
    "            if self.config.cardinality_threshold:\n",
    "                for col in columns:\n",
    "                    distinct_count = df.select(col).distinct().count()\n",
    "                    if distinct_count > self.config.cardinality_threshold:\n",
    "                        raise ValidationError(\n",
    "                            f\"Column {col} has {distinct_count} distinct values, \"\n",
    "                            f\"exceeding threshold of {self.config.cardinality_threshold}\"\n",
    "                        )\n",
    "                        \n",
    "            # Check data types if specified\n",
    "            if self.config.expected_types:\n",
    "                for col in columns:\n",
    "                    col_type = str(df.schema[col].datatype)\n",
    "                    valid_type = any(\n",
    "                        col_type.upper().startswith(exp_type.upper()) \n",
    "                        for exp_type in self.config.expected_types\n",
    "                    )\n",
    "                    if not valid_type:\n",
    "                        raise ValidationError(\n",
    "                            f\"Column {col} has type {col_type}, \"\n",
    "                            f\"expected one of {self.config.expected_types}\"\n",
    "                        )\n",
    "                        \n",
    "            logger.debug(f\"Validation passed for columns: {columns}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            if not isinstance(e, ValidationError):\n",
    "                raise ValidationError(f\"Validation failed: {str(e)}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class CustomTransform(ValidationMixin):\n",
    "    \"\"\"Wrapper for custom transformations\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        transform_func: Callable[[DataFrame], DataFrame],\n",
    "        config: TransformConfig\n",
    "    ):\n",
    "        self._transform = transform_func\n",
    "        self._config = config\n",
    "        \n",
    "    @property\n",
    "    def config(self) -> TransformConfig:\n",
    "        return self._config\n",
    "        \n",
    "    def __call__(self, df: DataFrame) -> DataFrame:\n",
    "        return self._transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@dataclass\n",
    "class WindowSpec:\n",
    "    \"\"\"Configuration for window-based transformations\"\"\"\n",
    "    partition_by: Optional[Union[str, List[str]]] = None\n",
    "    order_by: Optional[Union[str, List[str]]] = None\n",
    "    window_size: Optional[int] = None\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        \"\"\"Convert string inputs to lists\"\"\"\n",
    "        if isinstance(self.partition_by, str):\n",
    "            self.partition_by = [self.partition_by]\n",
    "        if isinstance(self.order_by, str):\n",
    "            self.order_by = [self.order_by]\n",
    "            \n",
    "    def to_window(self) -> Window:\n",
    "        \"\"\"Convert to Snowpark Window specification\"\"\"\n",
    "        window = Window.partition_by(self.partition_by or []) \\\n",
    "                      .order_by(self.order_by or [])\n",
    "        \n",
    "        if self.window_size:\n",
    "            window = window.rows_between(\n",
    "                -(self.window_size-1),\n",
    "                Window.CURRENT_ROW\n",
    "            )\n",
    "        return window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class WindowTransform(ValidationMixin):\n",
    "    \"\"\"Window-based aggregation transform\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        agg_cols: Dict[str, List[str]],\n",
    "        window_spec: WindowSpec,\n",
    "        config: Optional[TransformConfig] = None\n",
    "    ):\n",
    "        self._agg_cols = agg_cols\n",
    "        self._window_spec = window_spec\n",
    "        self._config = config or TransformConfig(\n",
    "            name=\"window_transform\",\n",
    "            description=\"Window-based aggregation\"\n",
    "        )\n",
    "        \n",
    "    @property\n",
    "    def config(self) -> TransformConfig:\n",
    "        return self._config\n",
    "        \n",
    "    def __call__(self, df: DataFrame) -> DataFrame:\n",
    "        \"\"\"Apply window-based aggregations\"\"\"\n",
    "        try:\n",
    "            window = self._window_spec.to_window()\n",
    "            new_cols = []\n",
    "            \n",
    "            for col, aggs in self._agg_cols.items():\n",
    "                for agg in aggs:\n",
    "                    agg_func = getattr(F, agg.lower())\n",
    "                    new_col = f\"{agg.upper()}_{col.upper()}\"\n",
    "                    new_cols.append(new_col)\n",
    "                    df = df.with_column(\n",
    "                        new_col,\n",
    "                        agg_func(F.col(col)).over(window)\n",
    "                    )\n",
    "                    \n",
    "            if self.config.validate_output:\n",
    "                self.validate_dataframe(df, new_cols)\n",
    "                \n",
    "            return df\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Window transform failed: {str(e)}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def window_agg(\n",
    "    agg_cols: Dict[str, List[str]],\n",
    "    window_spec: WindowSpec,\n",
    "    config: Optional[TransformConfig] = None\n",
    ") -> Transform:\n",
    "    \"\"\"Create window-based aggregation transform\n",
    "    \n",
    "    Args:\n",
    "        agg_cols: Dictionary mapping columns to aggregation functions\n",
    "        window_spec: Window specification for the aggregation\n",
    "        config: Optional transform configuration\n",
    "        \n",
    "    Returns:\n",
    "        Transform function\n",
    "        \n",
    "    Example:\n",
    "        >>> config = TransformConfig(\n",
    "        ...     name=\"customer_metrics\",\n",
    "        ...     null_threshold=0.05,\n",
    "        ...     expected_type=\"DOUBLE\"\n",
    "        ... )\n",
    "        >>> spec = WindowSpec(partition_by='customer_id', order_by='date')\n",
    "        >>> transform = window_agg({'amount': ['SUM', 'AVG']}, spec, config)\n",
    "    \"\"\"\n",
    "    return WindowTransform(agg_cols, window_spec, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class FillNATransform(ValidationMixin):\n",
    "    def __init__(\n",
    "        self, \n",
    "        cols: Union[str, List[str]], \n",
    "        fill_value: Union[int, float, str] = 0,\n",
    "        config: Optional[TransformConfig] = None\n",
    "    ):\n",
    "        self.cols = listify(cols)\n",
    "        self.fill_value = fill_value\n",
    "        self._config = config or TransformConfig(\n",
    "            name=\"fill_na_transform\",\n",
    "            description=f\"Fill NA values with {fill_value}\"\n",
    "        )\n",
    "        \n",
    "    @property\n",
    "    def config(self) -> TransformConfig:\n",
    "        return self._config\n",
    "        \n",
    "    def __call__(self, df: DataFrame) -> DataFrame:\n",
    "        try:\n",
    "            for col in self.cols:\n",
    "                actual_col = next(\n",
    "                    (c for c in df.schema.names if c.upper() == col.upper()),\n",
    "                    None\n",
    "                )\n",
    "                if actual_col is None:\n",
    "                    raise ValueError(f\"Column {col} not found in DataFrame\")\n",
    "                \n",
    "                col_type = df.schema[actual_col].datatype\n",
    "                typed_value = (\n",
    "                    int(self.fill_value) if str(col_type).startswith(('Long', 'Int')) \n",
    "                    else float(self.fill_value) if str(col_type).startswith('Double') \n",
    "                    else str(self.fill_value)\n",
    "                )\n",
    "                \n",
    "                df = df.na.fill({actual_col: typed_value})\n",
    "                \n",
    "            if self.config.validate_output:\n",
    "                self.validate_dataframe(df, self.cols)\n",
    "                \n",
    "            return df\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Fill NA transform failed: {str(e)}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#| export\n",
    "def fill_na(\n",
    "    cols: Union[str, List[str]], \n",
    "    fill_value: Union[int, float, str] = 0,\n",
    "    config: Optional[TransformConfig] = None\n",
    ") -> Transform:\n",
    "    \"\"\"Create transform to fill NA values\n",
    "    \n",
    "    Args:\n",
    "        cols: Column(s) to fill NA values in\n",
    "        fill_value: Value to use for filling NAs\n",
    "        config: Optional transform configuration\n",
    "        \n",
    "    Returns:\n",
    "        Transform function\n",
    "        \n",
    "    Example:\n",
    "        >>> config = TransformConfig(\n",
    "        ...     name=\"fill_scores\",\n",
    "        ...     null_threshold=1.0\n",
    "        ... )\n",
    "        >>> transform = fill_na(['score'], fill_value=0, config=config)\n",
    "    \"\"\"\n",
    "    return FillNATransform(cols, fill_value, config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class DateDiffTransform(ValidationMixin):\n",
    "    \"\"\"Calculate date difference between column and reference\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        col: str,\n",
    "        new_col: str,\n",
    "        reference_date: Optional[str] = None,\n",
    "        date_part: str = 'day',\n",
    "        config: Optional[TransformConfig] = None\n",
    "    ):\n",
    "        self.col = col\n",
    "        self.new_col = new_col.upper()\n",
    "        self.reference_date = reference_date\n",
    "        self.date_part = date_part\n",
    "        self._config = config or TransformConfig(\n",
    "            name=\"date_diff_transform\",\n",
    "            description=f\"Calculate {date_part} difference from {col}\",\n",
    "            expected_type=\"INT\"  # Date diffs return integers\n",
    "        )\n",
    "        \n",
    "    @property\n",
    "    def config(self) -> TransformConfig:\n",
    "        return self._config\n",
    "        \n",
    "    def __call__(self, df: DataFrame) -> DataFrame:\n",
    "        try:\n",
    "            # Find actual column name (case-insensitive)\n",
    "            col_actual = next(\n",
    "                (c for c in df.columns if c.upper() == self.col.upper()),\n",
    "                None\n",
    "            )\n",
    "            if col_actual is None:\n",
    "                raise ValueError(f\"Column {self.col} not found in DataFrame\")\n",
    "            \n",
    "            reference = (F.to_date(F.lit(self.reference_date)) \n",
    "                        if self.reference_date\n",
    "                        else F.current_date())\n",
    "            \n",
    "            df = df.with_column(\n",
    "                self.new_col,\n",
    "                F.datediff(self.date_part, F.col(col_actual), reference)\n",
    "            )\n",
    "            \n",
    "            if self.config.validate_output:\n",
    "                self.validate_dataframe(df, [self.new_col])\n",
    "                \n",
    "            return df\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Date diff transform failed: {str(e)}\")\n",
    "            raise\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def date_diff(\n",
    "    col: str,\n",
    "    new_col: str,\n",
    "    reference_date: Optional[str] = None,\n",
    "    date_part: str = 'day',\n",
    "    config: Optional[TransformConfig] = None\n",
    ") -> Transform:\n",
    "    \"\"\"Create date difference transform\n",
    "    \n",
    "    Args:\n",
    "        col: Date column to calculate difference from\n",
    "        new_col: Name for the new difference column\n",
    "        reference_date: Reference date (default: current_date)\n",
    "        date_part: Part to calculate difference in ('day', 'month', etc.)\n",
    "        config: Optional transform configuration\n",
    "        \n",
    "    Returns:\n",
    "        Transform function\n",
    "        \n",
    "    Example:\n",
    "        >>> config = TransformConfig(\n",
    "        ...     name=\"membership_length\",\n",
    "        ...     null_threshold=0.0  # No nulls allowed\n",
    "        ... )\n",
    "        >>> transform = date_diff('join_date', 'days_member', config=config)\n",
    "    \"\"\"\n",
    "    return DateDiffTransform(col, new_col, reference_date, date_part, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class MovingAggTransform(ValidationMixin):\n",
    "    \"\"\"Calculate moving window aggregations\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        cols: Union[str, List[str]],\n",
    "        window_sizes: List[int],\n",
    "        agg_funcs: List[str] = ['SUM', 'AVG'],\n",
    "        partition_by: Optional[List[str]] = None,\n",
    "        order_by: Optional[List[str]] = None,\n",
    "        config: Optional[TransformConfig] = None\n",
    "    ):\n",
    "        self.cols = listify(cols)\n",
    "        self.window_sizes = window_sizes\n",
    "        self.agg_funcs = agg_funcs\n",
    "        self.partition_by = partition_by\n",
    "        self.order_by = order_by\n",
    "        self._config = config or TransformConfig(\n",
    "            name=\"moving_agg_transform\",\n",
    "            description=f\"Moving {', '.join(agg_funcs)} over {window_sizes} periods\"\n",
    "        )\n",
    "        \n",
    "    @property\n",
    "    def config(self) -> TransformConfig:\n",
    "        return self._config\n",
    "        \n",
    "    def __call__(self, df: DataFrame) -> DataFrame:\n",
    "        try:\n",
    "            new_cols = []\n",
    "            \n",
    "            for col in self.cols:\n",
    "                for size in self.window_sizes:\n",
    "                    spec = WindowSpec(\n",
    "                        partition_by=self.partition_by,\n",
    "                        order_by=self.order_by,\n",
    "                        window_size=size\n",
    "                    )\n",
    "                    \n",
    "                    window = spec.to_window()\n",
    "                    \n",
    "                    for agg in self.agg_funcs:\n",
    "                        agg_func = getattr(F, agg.lower())\n",
    "                        new_col = f\"{agg.upper()}_{col.upper()}_{size}\"\n",
    "                        new_cols.append(new_col)\n",
    "                        \n",
    "                        # Cast result to DOUBLE\n",
    "                        df = df.with_column(\n",
    "                            new_col,\n",
    "                            agg_func(F.col(col)).over(window).cast('DOUBLE')\n",
    "                        )\n",
    "            \n",
    "            if self.config.validate_output:\n",
    "                self.validate_dataframe(df, new_cols)\n",
    "                \n",
    "            return df\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Moving aggregation transform failed: {str(e)}\")\n",
    "            raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def moving_agg(\n",
    "    cols: Union[str, List[str]],\n",
    "    window_sizes: List[int],\n",
    "    agg_funcs: List[str] = ['SUM', 'AVG'],\n",
    "    partition_by: Optional[List[str]] = None,\n",
    "    order_by: Optional[List[str]] = None,\n",
    "    config: Optional[TransformConfig] = None\n",
    ") -> Transform:\n",
    "    \"\"\"Create moving aggregation transform\n",
    "    \n",
    "    Args:\n",
    "        cols: Columns to aggregate\n",
    "        window_sizes: List of window sizes\n",
    "        agg_funcs: List of aggregation functions\n",
    "        partition_by: Columns to partition by\n",
    "        order_by: Columns to order by\n",
    "        config: Optional transform configuration\n",
    "        \n",
    "    Returns:\n",
    "        Transform function\n",
    "        \n",
    "    Example:\n",
    "        >>> config = TransformConfig(\n",
    "        ...     name=\"rolling_metrics\",\n",
    "        ...     expected_type=\"DOUBLE\"\n",
    "        ... )\n",
    "        >>> transform = moving_agg(\n",
    "        ...     'amount', \n",
    "        ...     [7, 30], \n",
    "        ...     ['SUM'], \n",
    "        ...     ['customer_id'], \n",
    "        ...     ['date'],\n",
    "        ...     config\n",
    "        ... )\n",
    "    \"\"\"\n",
    "    return MovingAggTransform(\n",
    "        cols, window_sizes, agg_funcs, \n",
    "        partition_by, order_by, config\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class CumulativeAggTransform(ValidationMixin):\n",
    "    \"\"\"Calculate cumulative aggregations\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        cols: Union[str, List[str]],\n",
    "        agg_funcs: List[str] = ['SUM'],\n",
    "        partition_by: Optional[List[str]] = None,\n",
    "        order_by: Optional[List[str]] = None,\n",
    "        config: Optional[TransformConfig] = None\n",
    "    ):\n",
    "        self.cols = listify(cols)\n",
    "        self.agg_funcs = agg_funcs\n",
    "        self.partition_by = partition_by\n",
    "        self.order_by = order_by\n",
    "        self._config = config or TransformConfig(\n",
    "            name=\"cumulative_agg_transform\",\n",
    "            description=f\"Cumulative {', '.join(agg_funcs)}\"\n",
    "        )\n",
    "        \n",
    "    @property\n",
    "    def config(self) -> TransformConfig:\n",
    "        return self._config\n",
    "        \n",
    "    def __call__(self, df: DataFrame) -> DataFrame:\n",
    "        try:\n",
    "            new_cols = []\n",
    "            \n",
    "            for col in self.cols:\n",
    "                spec = WindowSpec(\n",
    "                    partition_by=self.partition_by,\n",
    "                    order_by=self.order_by\n",
    "                )\n",
    "                window = spec.to_window()\n",
    "                \n",
    "                for agg in self.agg_funcs:\n",
    "                    agg_func = getattr(F, agg.lower())\n",
    "                    new_col = f\"CUM_{agg.upper()}_{col.upper()}\"\n",
    "                    new_cols.append(new_col)\n",
    "                    \n",
    "                    df = df.with_column(\n",
    "                        new_col,\n",
    "                        agg_func(F.col(col)).over(window)\n",
    "                    )\n",
    "            \n",
    "            if self.config.validate_output:\n",
    "                self.validate_dataframe(df, new_cols)\n",
    "                \n",
    "            return df\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Cumulative aggregation transform failed: {str(e)}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def cumulative_agg(\n",
    "    cols: Union[str, List[str]],\n",
    "    agg_funcs: List[str] = ['SUM'],\n",
    "    partition_by: Optional[List[str]] = None,\n",
    "    order_by: Optional[List[str]] = None,\n",
    "    config: Optional[TransformConfig] = None\n",
    ") -> Transform:\n",
    "    \"\"\"Create cumulative aggregation transform\n",
    "    \n",
    "    Args:\n",
    "        cols: Columns to aggregate\n",
    "        agg_funcs: List of aggregation functions\n",
    "        partition_by: Columns to partition by\n",
    "        order_by: Columns to order by\n",
    "        config: Optional transform configuration\n",
    "        \n",
    "    Returns:\n",
    "        Transform function\n",
    "        \n",
    "    Example:\n",
    "        >>> config = TransformConfig(\n",
    "        ...     name=\"running_totals\",\n",
    "        ...     expected_type=\"DOUBLE\"\n",
    "        ... )\n",
    "        >>> transform = cumulative_agg(\n",
    "        ...     'amount', \n",
    "        ...     ['SUM'], \n",
    "        ...     ['customer_id'], \n",
    "        ...     ['date'],\n",
    "        ...     config\n",
    "        ... )\n",
    "    \"\"\"\n",
    "    return CumulativeAggTransform(\n",
    "        cols, agg_funcs, partition_by, order_by, config\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def apply_transforms(df: DataFrame, transforms: List[Transform]) -> DataFrame:\n",
    "    \"\"\"Apply a list of transformations to a DataFrame\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame\n",
    "        transforms: List of transform functions to apply\n",
    "        \n",
    "    Returns:\n",
    "        Transformed DataFrame\n",
    "        \n",
    "    Example:\n",
    "        >>> transforms = [\n",
    "        ...     fill_na(['score'], config=TransformConfig(name='fill_scores')),\n",
    "        ...     date_diff('date', 'days_ago', config=TransformConfig(name='time_features'))\n",
    "        ... ]\n",
    "        >>> df = apply_transforms(df, transforms)\n",
    "    \"\"\"\n",
    "    for transform in transforms:\n",
    "        try:\n",
    "            logger.debug(f\"Applying transform: {transform.config.name}\")\n",
    "            df = transform(df)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Transform {transform.config.name} failed: {str(e)}\")\n",
    "            raise\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-26 19:34:47,083 - snowflake_feature_store - INFO - No active session found, creating new connection from environment\n",
      "2025-02-26 19:34:47,762 - snowflake_feature_store - INFO - Initialized connection to \"CONTAINER_DEMO_DB\".\"PUBLIC\"\n",
      "2025-02-26 19:34:47,763 - snowflake_feature_store - INFO - Using role: \"ACCOUNTADMIN\", warehouse: \"CONTAINER_DEMO_WH\", database: \"CONTAINER_DEMO_DB\", schema: \"PUBLIC\"\n",
      "\n",
      "Transformed DataFrame:\n",
      "--------------------------------------------------------------------------------------------------\n",
      "|\"CUSTOMER_ID\"  |\"DATE\"      |\"AMOUNT\"  |\"SCORE\"  |\"SUM_AMOUNT_2\"  |\"AVG_AMOUNT_2\"  |\"DAYS_AGO\"  |\n",
      "--------------------------------------------------------------------------------------------------\n",
      "|C2             |2024-01-01  |150       |0.0      |150.0           |150.0           |2           |\n",
      "|C1             |2024-01-01  |100       |0.0      |100.0           |100.0           |2           |\n",
      "|C1             |2024-01-02  |200       |3.5      |300.0           |150.0           |1           |\n",
      "--------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Result Schema:\n",
      "CUSTOMER_ID: StringType()\n",
      "DATE: StringType()\n",
      "AMOUNT: LongType()\n",
      "SCORE: DoubleType()\n",
      "SUM_AMOUNT_2: DoubleType()\n",
      "AVG_AMOUNT_2: DoubleType()\n",
      "DAYS_AGO: LongType()\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "# Example usage\n",
    "from snowflake_feature_store.connection import get_connection, ConnectionConfig\n",
    "from snowflake_feature_store.transforms import *\n",
    "\n",
    "# Get connection automatically\n",
    "conn = get_connection()\n",
    "session = conn.session\n",
    "\n",
    "# Create example DataFrame - Note the UPPERCASE column names for Snowflake\n",
    "df = session.create_dataframe([\n",
    "    ['C1', '2024-01-01', 100, None],\n",
    "    ['C1', '2024-01-02', 200, 3.5],\n",
    "    ['C2', '2024-01-01', 150, None]\n",
    "], ['CUSTOMER_ID', 'DATE', 'AMOUNT', 'SCORE'])\n",
    "\n",
    "# Configuration with appropriate type validation\n",
    "agg_config = TransformConfig(\n",
    "    name=\"customer_metrics\",\n",
    "    null_threshold=0.05,\n",
    "    expected_types=['LONG', 'DOUBLE']  # Accept either type for numeric aggregations\n",
    ")\n",
    "\n",
    "date_config = TransformConfig(\n",
    "    name=\"date_features\",\n",
    "    null_threshold=0.0,\n",
    "    expected_types=['INT', 'LONG']  # Date diffs can be either type\n",
    ")\n",
    "\n",
    "# Create transforms\n",
    "transforms = [\n",
    "    # Fill NA values in score column\n",
    "    fill_na(['SCORE'], fill_value=0),\n",
    "    \n",
    "    # Calculate moving aggregations on amount\n",
    "    moving_agg(\n",
    "        cols='AMOUNT',\n",
    "        window_sizes=[2],\n",
    "        agg_funcs=['SUM', 'AVG'],  # Now we can use both\n",
    "        partition_by=['CUSTOMER_ID'],\n",
    "        order_by=['DATE'],\n",
    "        config=agg_config\n",
    "    ),\n",
    "    \n",
    "    # Calculate date differences\n",
    "    date_diff(\n",
    "        col='DATE',\n",
    "        new_col='DAYS_AGO',\n",
    "        reference_date='2024-01-03',\n",
    "        config=date_config\n",
    "    )\n",
    "]\n",
    "\n",
    "# Apply transforms\n",
    "result_df = apply_transforms(df, transforms)\n",
    "\n",
    "# Show results\n",
    "print(\"\\nTransformed DataFrame:\")\n",
    "result_df.show()\n",
    "\n",
    "# Show schema\n",
    "print(\"\\nResult Schema:\")\n",
    "for field in result_df.schema.fields:\n",
    "    print(f\"{field.name}: {field.datatype}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "feature-store",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
