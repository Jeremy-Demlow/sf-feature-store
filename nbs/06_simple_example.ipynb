{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End-to-End Feature Store Example\n",
    "\n",
    "> A comprehensive guide to setting up and using the Snowflake Feature Store for ML workflows.\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook demonstrates a complete ML feature engineering workflow using Snowflake's Feature Store. We'll create a permanent feature store that:\n",
    "1. Persists across sessions\n",
    "2. Is accessible through Snowsight UI\n",
    "3. Includes feature monitoring\n",
    "4. Demonstrates best practices\n",
    "\n",
    "### Why Use a Feature Store?\n",
    "\n",
    "Feature stores solve several critical ML challenges:\n",
    "\n",
    "1. **Feature Consistency**\n",
    "   - Ensure features are computed identically in training and production\n",
    "   - Version control for feature definitions\n",
    "   - Single source of truth for feature transformations\n",
    "\n",
    "2. **Feature Reuse**\n",
    "   - Share features across teams and projects\n",
    "   - Reduce duplicate computation\n",
    "   - Standardize common transformations\n",
    "\n",
    "3. **Feature Monitoring**\n",
    "   - Track feature drift over time\n",
    "   - Monitor feature quality\n",
    "   - Alert on data quality issues\n",
    "\n",
    "4. **Feature Discovery**\n",
    "   - Make features searchable and discoverable\n",
    "   - Document feature definitions and usage\n",
    "   - Track feature dependencies\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before running this example, you need:\n",
    "\n",
    "1. Appropriate permissions (see below)\n",
    "\n",
    "2. Python environment with required packages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generation\n",
    "\n",
    "First, we'll create some example customer data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jdemlow/miniconda3/envs/feature-store/lib/python3.11/site-packages/pydantic/_internal/_fields.py:192: UserWarning: Field name \"schema\" in \"ConnectionConfig\" shadows an attribute in parent \"BaseModel\"\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import Optional\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import snowflake.snowpark.functions as F\n",
    "\n",
    "from snowflake_feature_store.connection import get_connection\n",
    "from snowflake_feature_store.manager import FeatureStoreManager\n",
    "from snowflake_feature_store.config import (\n",
    "    FeatureViewConfig, FeatureConfig, RefreshConfig, \n",
    "    FeatureValidationConfig\n",
    ")\n",
    "from snowflake_feature_store.transforms import (\n",
    "    Transform, TransformConfig, moving_agg, \n",
    "    fill_na, date_diff\n",
    ")\n",
    "from snowflake_feature_store.examples import (\n",
    "    get_example_data, create_feature_configs\n",
    ")\n",
    "from snowflake_feature_store.logging import logger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setting Up the Feature Store\n",
    "\n",
    "First, we'll create a permanent feature store in Snowflake. This differs from our previous examples which used temporary schemas.\n",
    "\n",
    "### Required Permissions\n",
    "\n",
    "To create a permanent feature store, you need:\n",
    "- `CREATE DATABASE` if creating a new database\n",
    "- `CREATE SCHEMA` in the target database\n",
    "- `USAGE` on the warehouse\n",
    "- `CREATE TABLE` in the target schema\n",
    "\n",
    "### Why Permanent vs Temporary?\n",
    "\n",
    "Permanent feature stores offer several advantages:\n",
    "1. Persistence across sessions\n",
    "2. Accessibility through Snowsight UI\n",
    "3. Ability to share with other users/roles\n",
    "4. Integration with other Snowflake tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-17 20:31:53,306 - snowflake_feature_store - INFO - No active session found, creating new connection from environment\n",
      "2025-02-17 20:31:54,142 - snowflake_feature_store - INFO - Initialized connection to \"DATASCIENCE\".\"FEATURE_STORE_DEMO\"\n",
      "2025-02-17 20:31:54,559 - snowflake_feature_store - INFO - Created and using DATASCIENCE.FEATURE_STORE_DEMO\n"
     ]
    }
   ],
   "source": [
    "# Remove trailing commas from string assignments\n",
    "database: str = \"DATASCIENCE\"\n",
    "schema: str = \"FEATURE_STORE_DEMO\"\n",
    "warehouse: str = \"DS_XS_WH\"\n",
    "\n",
    "conn = get_connection()\n",
    "\n",
    "# Create database and schema\n",
    "conn.session.sql(f\"CREATE DATABASE IF NOT EXISTS {database}\").collect()\n",
    "conn.session.sql(f\"CREATE SCHEMA IF NOT EXISTS {database}.{schema}\").collect()\n",
    "\n",
    "logger.info(f\"Created and using {database}.{schema}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Data Generation and Loading\n",
    "\n",
    "In this section, we'll create example customer data. In a real scenario, you'd load your own data, but this example shows:\n",
    "1. Proper data typing for Snowflake\n",
    "2. Handling temporal data correctly\n",
    "3. Setting up data quality checks\n",
    "4. Creating realistic patterns in the data\n",
    "\n",
    "### Why This Structure?\n",
    "\n",
    "This structure demonstrates common ML feature engineering challenges:\n",
    "1. **Time-based Features**: Rolling averages, time windows\n",
    "2. **Missing Data**: Handling sparse observations\n",
    "3. **Multiple Metrics**: Combining different data types\n",
    "4. **Entity Resolution**: Linking data to customers\n",
    "\n",
    "### Why LTV Prediction?\n",
    "\n",
    "LTV prediction is a common ML use case that demonstrates key feature store benefits:\n",
    "1. **Time-based Features**: Customer spending patterns over time\n",
    "2. **Multiple Data Sources**: Combining transactions, web analytics, and customer data\n",
    "3. **Feature Freshness**: Regular updates as new transactions occur\n",
    "4. **Point-in-Time Correctness**: Avoiding data leakage in training\n",
    "\n",
    "### Data Structure\n",
    "Our LTV example includes:\n",
    "- `LIFE_TIME_VALUE`: Current customer value (target)\n",
    "- `SESSION_LENGTH`: Customer engagement metric\n",
    "- `TRANSACTIONS`: Number of transactions\n",
    "- `TIME_ON_APP/WEBSITE`: Engagement channels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-17 20:31:58,160 - snowflake_feature_store - INFO - Generated 2400 rows of demo data in \"DATASCIENCE\".FEATURE_STORE_DEMO.CUSTOMER_ACTIVITY\n",
      "2025-02-17 20:31:58,162 - snowflake_feature_store - INFO - \n",
      "Sample Data:\n",
      "----------------------------------------------------------------------------------------------------------------------------------\n",
      "|\"CUSTOMER_ID\"  |\"DATE\"      |\"LIFE_TIME_VALUE\"   |\"SESSION_LENGTH\"    |\"TIME_ON_APP\"      |\"TIME_ON_WEBSITE\"   |\"TRANSACTIONS\"  |\n",
      "----------------------------------------------------------------------------------------------------------------------------------\n",
      "|C0             |2025-01-18  |411.9831046823803   |6.101285704798359   |9.912268452627666  |7.269712691809914   |4               |\n",
      "|C93            |2025-02-16  |587.2728207366863   |7.409478170285487   |9.255783785633916  |10.025373746735333  |5               |\n",
      "|C22            |2025-02-16  |266.1622473753326   |7.3149273852223144  |4.897446257228738  |9.4240638710075     |2               |\n",
      "|C51            |2025-02-16  |222.37101022575163  |5.636574120338889   |8.347517400939065  |9.196765681205283   |2               |\n",
      "|C36            |2025-02-16  |144.72168907455833  |NULL                |6.852434333584844  |7.780060192861447   |1               |\n",
      "----------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "2025-02-17 20:31:58,919 - snowflake_feature_store - INFO - \n",
      "Schema:\n",
      "2025-02-17 20:31:59,131 - snowflake_feature_store - INFO - CUSTOMER_ID: StringType()\n",
      "2025-02-17 20:31:59,131 - snowflake_feature_store - INFO - DATE: DateType()\n",
      "2025-02-17 20:31:59,132 - snowflake_feature_store - INFO - LIFE_TIME_VALUE: DoubleType()\n",
      "2025-02-17 20:31:59,133 - snowflake_feature_store - INFO - SESSION_LENGTH: DoubleType()\n",
      "2025-02-17 20:31:59,133 - snowflake_feature_store - INFO - TIME_ON_APP: DoubleType()\n",
      "2025-02-17 20:31:59,134 - snowflake_feature_store - INFO - TIME_ON_WEBSITE: DoubleType()\n",
      "2025-02-17 20:31:59,135 - snowflake_feature_store - INFO - TRANSACTIONS: LongType()\n",
      "2025-02-17 20:31:59,135 - snowflake_feature_store - INFO - \n",
      "Data Profile:\n",
      "2025-02-17 20:31:59,902 - snowflake_feature_store - INFO - CUSTOMER_ID: 0.0% null\n",
      "2025-02-17 20:32:00,364 - snowflake_feature_store - INFO - DATE: 0.0% null\n",
      "2025-02-17 20:32:01,008 - snowflake_feature_store - INFO - LIFE_TIME_VALUE: 0.0% null\n",
      "2025-02-17 20:32:01,419 - snowflake_feature_store - INFO - SESSION_LENGTH: 18.8% null\n",
      "2025-02-17 20:32:01,811 - snowflake_feature_store - INFO - TIME_ON_APP: 0.0% null\n",
      "2025-02-17 20:32:02,172 - snowflake_feature_store - INFO - TIME_ON_WEBSITE: 0.0% null\n",
      "2025-02-17 20:32:02,704 - snowflake_feature_store - INFO - TRANSACTIONS: 0.0% null\n"
     ]
    }
   ],
   "source": [
    "# Get start date\n",
    "num_customers =  100\n",
    "\n",
    "# Generate data with patterns\n",
    "df = get_example_data(\n",
    "    conn.session,\n",
    "    schema,\n",
    "    num_customers,\n",
    ")\n",
    "\n",
    "# Show data profile\n",
    "logger.info(\"\\nData Profile:\")\n",
    "for col in df.columns:\n",
    "    null_count = df.filter(F.col(col).is_null()).count()\n",
    "    null_pct = null_count / df.count() * 100\n",
    "    logger.info(f\"{col}: {null_pct:.1f}% null\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Entity Creation\n",
    "\n",
    "Entities are the foundation of your feature store. They represent the objects you're collecting features about (e.g., customers, products, transactions).\n",
    "\n",
    "### Why Entities Matter\n",
    "\n",
    "Proper entity design is crucial because:\n",
    "1. Entities determine how features can be joined\n",
    "2. Entities define the granularity of your features\n",
    "3. Entities enable point-in-time correct feature retrieval\n",
    "4. Entities help organize and discover features\n",
    "\n",
    "### Entity Best Practices\n",
    "\n",
    "1. **Unique Keys**: Choose stable, unique identifiers\n",
    "2. **Granularity**: Pick the right level (e.g., customer vs. session)\n",
    "3. **Documentation**: Clearly describe what the entity represents\n",
    "4. **Consistency**: Use the same keys across feature views\n",
    "\n",
    "\n",
    "### Entity Design for LTV\n",
    "\n",
    "For LTV prediction, we need a customer entity that:\n",
    "1. Has a stable identifier\n",
    "2. Links to all customer interactions\n",
    "3. Supports time-based feature aggregation\n",
    "\n",
    "Key considerations for our customer entity:\n",
    "1. **Identifier**: Use `CUSTOMER_ID` as stable key\n",
    "2. **Temporal Aspect**: Track customer since first transaction\n",
    "3. **Granularity**: Customer-level for LTV prediction\n",
    "4. **Documentation**: Clear description for feature discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-17 20:32:04,320 - snowflake_feature_store - INFO - FeatureStoreManager initialized\n",
      "2025-02-17 20:32:04,494 - snowflake_feature_store - INFO - Created entity: CUSTOMER with keys: ['CUSTOMER_ID']\n",
      "2025-02-17 20:32:04,496 - snowflake_feature_store - INFO - Created CUSTOMER entity for LTV prediction\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jdemlow/miniconda3/envs/feature-store/lib/python3.11/site-packages/snowflake/ml/feature_store/feature_store.py:197: UserWarning: Entity CUSTOMER already exists. Skip registration.\n",
      "  return f(self, *args, **kargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-17 20:32:05,516 - snowflake_feature_store - INFO - \n",
      "Entity Details:\n",
      "2025-02-17 20:32:05,517 - snowflake_feature_store - INFO - Name: CUSTOMER\n",
      "2025-02-17 20:32:05,518 - snowflake_feature_store - INFO - Join Keys: ['CUSTOMER_ID']\n"
     ]
    }
   ],
   "source": [
    "# Create detailed documentation\n",
    "description = \"\"\"\n",
    "Customer Entity for LTV Prediction\n",
    "\n",
    "This entity represents individual customers and their behavior over time.\n",
    "It serves as the primary entity for customer lifetime value prediction.\n",
    "\n",
    "Key Information:\n",
    "- Primary Key: CUSTOMER_ID (stable identifier)\n",
    "- Temporal Key: DATE (for point-in-time correct features)\n",
    "- Granularity: One record per customer per day\n",
    "\n",
    "Usage:\n",
    "1. Base entity for customer-level features\n",
    "2. Join key for transaction and session data\n",
    "3. Temporal alignment for time-based features\n",
    "\n",
    "Best Practices:\n",
    "- Always join using CUSTOMER_ID\n",
    "- Use DATE for point-in-time correctness\n",
    "- Aggregate features to customer-day level\n",
    "\n",
    "Example:\n",
    "```sql\n",
    "SELECT CUSTOMER_ID, DATE, COUNT(*) as daily_transactions\n",
    "FROM transactions\n",
    "GROUP BY CUSTOMER_ID, DATE\n",
    "```\n",
    "\"\"\".strip()\n",
    "\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "\n",
    "metrics_dir = Path(tempfile.mkdtemp()) / \"feature_store_metrics\"\n",
    "\n",
    "manager = FeatureStoreManager(\n",
    "    connection=conn,\n",
    "    metrics_path=metrics_dir,\n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "# Create entity\n",
    "manager.add_entity(\n",
    "    name=\"CUSTOMER\",\n",
    "    join_keys=[\"CUSTOMER_ID\"],\n",
    "    description=description,\n",
    ")\n",
    "\n",
    "logger.info(\"Created CUSTOMER entity for LTV prediction\")\n",
    "# Verify entity creation\n",
    "entity = manager.feature_store.get_entity(\"CUSTOMER\")\n",
    "logger.info(\"\\nEntity Details:\")\n",
    "logger.info(f\"Name: {entity.name}\")\n",
    "logger.info(f\"Join Keys: {entity.join_keys}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Points About This Implementation\n",
    "\n",
    "1. **Documentation**\n",
    "   - Clear description of entity purpose\n",
    "   - Usage examples included\n",
    "   - Best practices documented\n",
    "   - SQL example provided\n",
    "\n",
    "\n",
    "2. **Validation**\n",
    "   - Entity creation is verified\n",
    "   - Join keys are explicitly defined\n",
    "   - Logging provides creation confirmation\n",
    "   - Error handling is included\n",
    "\n",
    "3. **LTV Specific**\n",
    "   - Designed for customer-level predictions\n",
    "   - Supports temporal feature creation\n",
    "   - Enables point-in-time correct joins\n",
    "   - Facilitates customer behavior tracking\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Feature Configuration\n",
    "\n",
    "Feature configuration is where we define what features we want to create and how they should behave.\n",
    "\n",
    "### Feature Configuration Concepts\n",
    "\n",
    "A feature configuration defines:\n",
    "1. **Validation Rules**: Data quality checks and thresholds\n",
    "2. **Dependencies**: What other features this feature needs\n",
    "3. **Metadata**: Description, tags, and ownership\n",
    "4. **Refresh Settings**: How often to update the feature\n",
    "\n",
    "### Why Configuration Matters\n",
    "\n",
    "Good feature configuration ensures:\n",
    "1. Data quality is maintained\n",
    "2. Features are well-documented\n",
    "3. Dependencies are tracked\n",
    "4. Feature freshness is appropriate\n",
    "\n",
    "### LTV Feature Configuration\n",
    "\n",
    "For LTV prediction, we need several types of features:\n",
    "1. **Behavioral Features**\n",
    "   - Session metrics\n",
    "   - Engagement patterns\n",
    "   - Transaction history\n",
    "\n",
    "2. **Temporal Features**\n",
    "   - Time since first purchase\n",
    "   - Weekly/monthly patterns\n",
    "   - Rolling aggregations\n",
    "\n",
    "3. **Derived Features**\n",
    "   - Average transaction value\n",
    "   - Engagement ratios\n",
    "   - Time-based metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'LIFE_TIME_VALUE': FeatureConfig(name='LIFE_TIME_VALUE', description='Current customer lifetime value', validation=FeatureValidationConfig(null_check=True, null_threshold=0.0, range_check=True, min_value=0.0, max_value=None, unique_check=False, unique_threshold=0.9), dependencies=[]),\n",
       " 'SESSION_LENGTH': FeatureConfig(name='SESSION_LENGTH', description='Session length in minutes', validation=FeatureValidationConfig(null_check=True, null_threshold=0.2, range_check=True, min_value=0.0, max_value=None, unique_check=False, unique_threshold=0.9), dependencies=[]),\n",
       " 'TRANSACTIONS': FeatureConfig(name='TRANSACTIONS', description='Number of transactions', validation=FeatureValidationConfig(null_check=True, null_threshold=0.0, range_check=True, min_value=0.0, max_value=None, unique_check=False, unique_threshold=0.9), dependencies=[]),\n",
       " 'SUM_TRANSACTIONS_7': FeatureConfig(name='SUM_TRANSACTIONS_7', description='sum of transactions over 7 days', validation=FeatureValidationConfig(null_check=True, null_threshold=0.1, range_check=True, min_value=0.0, max_value=None, unique_check=False, unique_threshold=0.9), dependencies=['TRANSACTIONS']),\n",
       " 'AVG_TRANSACTIONS_7': FeatureConfig(name='AVG_TRANSACTIONS_7', description='avg of transactions over 7 days', validation=FeatureValidationConfig(null_check=True, null_threshold=0.1, range_check=True, min_value=0.0, max_value=None, unique_check=False, unique_threshold=0.9), dependencies=['TRANSACTIONS']),\n",
       " 'SUM_LIFE_TIME_VALUE_7': FeatureConfig(name='SUM_LIFE_TIME_VALUE_7', description='sum of life_time_value over 7 days', validation=FeatureValidationConfig(null_check=True, null_threshold=0.1, range_check=True, min_value=0.0, max_value=None, unique_check=False, unique_threshold=0.9), dependencies=['LIFE_TIME_VALUE']),\n",
       " 'AVG_LIFE_TIME_VALUE_7': FeatureConfig(name='AVG_LIFE_TIME_VALUE_7', description='avg of life_time_value over 7 days', validation=FeatureValidationConfig(null_check=True, null_threshold=0.1, range_check=True, min_value=0.0, max_value=None, unique_check=False, unique_threshold=0.9), dependencies=['LIFE_TIME_VALUE']),\n",
       " 'SUM_TRANSACTIONS_30': FeatureConfig(name='SUM_TRANSACTIONS_30', description='sum of transactions over 30 days', validation=FeatureValidationConfig(null_check=True, null_threshold=0.1, range_check=True, min_value=0.0, max_value=None, unique_check=False, unique_threshold=0.9), dependencies=['TRANSACTIONS']),\n",
       " 'AVG_TRANSACTIONS_30': FeatureConfig(name='AVG_TRANSACTIONS_30', description='avg of transactions over 30 days', validation=FeatureValidationConfig(null_check=True, null_threshold=0.1, range_check=True, min_value=0.0, max_value=None, unique_check=False, unique_threshold=0.9), dependencies=['TRANSACTIONS']),\n",
       " 'SUM_LIFE_TIME_VALUE_30': FeatureConfig(name='SUM_LIFE_TIME_VALUE_30', description='sum of life_time_value over 30 days', validation=FeatureValidationConfig(null_check=True, null_threshold=0.1, range_check=True, min_value=0.0, max_value=None, unique_check=False, unique_threshold=0.9), dependencies=['LIFE_TIME_VALUE']),\n",
       " 'AVG_LIFE_TIME_VALUE_30': FeatureConfig(name='AVG_LIFE_TIME_VALUE_30', description='avg of life_time_value over 30 days', validation=FeatureValidationConfig(null_check=True, null_threshold=0.1, range_check=True, min_value=0.0, max_value=None, unique_check=False, unique_threshold=0.9), dependencies=['LIFE_TIME_VALUE']),\n",
       " 'ENGAGEMENT_SCORE': FeatureConfig(name='ENGAGEMENT_SCORE', description='Combined engagement metric', validation=FeatureValidationConfig(null_check=True, null_threshold=0.1, range_check=True, min_value=0.0, max_value=None, unique_check=False, unique_threshold=0.9), dependencies=['SESSION_LENGTH', 'TIME_ON_APP', 'TIME_ON_WEBSITE']),\n",
       " 'AVG_TRANSACTION_VALUE': FeatureConfig(name='AVG_TRANSACTION_VALUE', description='Average value per transaction', validation=FeatureValidationConfig(null_check=True, null_threshold=0.1, range_check=True, min_value=0.0, max_value=None, unique_check=False, unique_threshold=0.9), dependencies=['LIFE_TIME_VALUE', 'TRANSACTIONS'])}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Base features (from source data)\n",
    "feature_configs = {\n",
    "    \"LIFE_TIME_VALUE\": FeatureConfig(\n",
    "        name=\"LIFE_TIME_VALUE\",\n",
    "        description=\"Current customer lifetime value\",\n",
    "        validation=FeatureValidationConfig(\n",
    "            null_threshold=0.0,\n",
    "            range_check=True,\n",
    "            min_value=0\n",
    "        )\n",
    "    ),\n",
    "    \"SESSION_LENGTH\": FeatureConfig(\n",
    "        name=\"SESSION_LENGTH\",\n",
    "        description=\"Session length in minutes\",\n",
    "        validation=FeatureValidationConfig(\n",
    "            null_threshold=0.2,\n",
    "            range_check=True,\n",
    "            min_value=0\n",
    "        )\n",
    "    ),\n",
    "    \"TRANSACTIONS\": FeatureConfig(\n",
    "        name=\"TRANSACTIONS\",\n",
    "        description=\"Number of transactions\",\n",
    "        validation=FeatureValidationConfig(\n",
    "            null_threshold=0.0,\n",
    "            range_check=True,\n",
    "            min_value=0\n",
    "        )\n",
    "    )\n",
    "}\n",
    "\n",
    "# Time window features (match moving_agg output names)\n",
    "for window in [7, 30]:\n",
    "    for metric in ['TRANSACTIONS', 'LIFE_TIME_VALUE']:\n",
    "        for agg in ['SUM', 'AVG']:\n",
    "            feature_name = f\"{agg}_{metric}_{window}\"\n",
    "            feature_configs[feature_name] = FeatureConfig(\n",
    "                name=feature_name,\n",
    "                description=f\"{agg.lower()} of {metric.lower()} over {window} days\",\n",
    "                validation=FeatureValidationConfig(\n",
    "                    null_threshold=0.1,\n",
    "                    range_check=True,\n",
    "                    min_value=0\n",
    "                ),\n",
    "                dependencies=[metric]\n",
    "            )\n",
    "\n",
    "# Derived features\n",
    "feature_configs.update({\n",
    "    \"ENGAGEMENT_SCORE\": FeatureConfig(\n",
    "        name=\"ENGAGEMENT_SCORE\",\n",
    "        description=\"Combined engagement metric\",\n",
    "        validation=FeatureValidationConfig(\n",
    "            null_threshold=0.1,\n",
    "            range_check=True,\n",
    "            min_value=0\n",
    "        ),\n",
    "        dependencies=[\"SESSION_LENGTH\", \"TIME_ON_APP\", \"TIME_ON_WEBSITE\"]\n",
    "    ),\n",
    "    \"AVG_TRANSACTION_VALUE\": FeatureConfig(\n",
    "        name=\"AVG_TRANSACTION_VALUE\",\n",
    "        description=\"Average value per transaction\",\n",
    "        validation=FeatureValidationConfig(\n",
    "            null_threshold=0.1,\n",
    "            range_check=True,\n",
    "            min_value=0\n",
    "        ),\n",
    "        dependencies=[\"LIFE_TIME_VALUE\", \"TRANSACTIONS\"]\n",
    "    )\n",
    "})\n",
    "\n",
    "feature_configs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Feature Transformations\n",
    "\n",
    "Feature transformations convert raw data into ML-ready features. This is a critical step in the ML pipeline.\n",
    "\n",
    "### Why Transformations Matter\n",
    "\n",
    "Transformations serve multiple purposes:\n",
    "1. **Data Quality**: Handle missing values and outliers\n",
    "2. **Feature Engineering**: Create more predictive features\n",
    "3. **ML Requirements**: Format data for model consumption\n",
    "4. **Business Logic**: Encode domain knowledge\n",
    "\n",
    "### Types of Transformations\n",
    "\n",
    "1. **Basic Transformations**\n",
    "   - Missing value imputation\n",
    "   - Type conversion\n",
    "   - Scaling/normalization\n",
    "\n",
    "2. **Time-Based Transformations**\n",
    "   - Rolling windows\n",
    "   - Time since event\n",
    "   - Seasonal patterns\n",
    "\n",
    "3. **Business Transformations**\n",
    "   - Derived metrics\n",
    "   - Domain-specific calculations\n",
    "   - Feature combinations\n",
    "\n",
    "### LTV-Specific Transformations\n",
    "\n",
    "For LTV prediction, we need several key transformations:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.snowpark import DataFrame\n",
    "from typing import Callable\n",
    "\n",
    "from snowflake_feature_store.transforms import ValidationMixin\n",
    "\n",
    "class CustomTransform(ValidationMixin):\n",
    "    \"\"\"Wrapper for custom transformations\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        transform_func: Callable[[DataFrame], DataFrame],\n",
    "        config: TransformConfig\n",
    "    ):\n",
    "        self._transform = transform_func\n",
    "        self._config = config\n",
    "        \n",
    "    @property\n",
    "    def config(self) -> TransformConfig:\n",
    "        return self._config\n",
    "        \n",
    "    def __call__(self, df: DataFrame) -> DataFrame:\n",
    "        return self._transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default config if none provided\n",
    "transform_config = TransformConfig(\n",
    "    name=\"ltv_transforms\",\n",
    "    null_threshold=0.1,\n",
    "    expected_types=['DECIMAL', 'DOUBLE', 'NUMBER']\n",
    ")\n",
    "\n",
    "transforms = [\n",
    "    # 1. Handle Missing Values\n",
    "    fill_na(\n",
    "        ['SESSION_LENGTH', 'TIME_ON_APP', 'TIME_ON_WEBSITE'],\n",
    "        fill_value=0,\n",
    "        config=TransformConfig(\n",
    "            name=\"engagement_imputation\",\n",
    "            description=\"Fill missing engagement metrics with 0\"\n",
    "        )\n",
    "    ),\n",
    "    \n",
    "    # 2. Time-Based Features\n",
    "    moving_agg(\n",
    "        cols=['TRANSACTIONS', 'LIFE_TIME_VALUE'],\n",
    "        window_sizes=[7, 30],  # 7 and 30 day windows\n",
    "        agg_funcs=['SUM', 'AVG'],\n",
    "        partition_by=['CUSTOMER_ID'],\n",
    "        order_by=['DATE'],\n",
    "        config=TransformConfig(\n",
    "            name=\"time_windows\",\n",
    "            description=\"Rolling window aggregations\"\n",
    "        )\n",
    "    ),\n",
    "    # 3. Engagement Metrics\n",
    "    CustomTransform(\n",
    "        transform_func=lambda df: df.with_column(\n",
    "            'ENGAGEMENT_SCORE',\n",
    "            (F.col('SESSION_LENGTH') + \n",
    "                F.col('TIME_ON_APP') + \n",
    "                F.col('TIME_ON_WEBSITE')) / 3.0\n",
    "        ),\n",
    "        config=TransformConfig(\n",
    "            name=\"engagement_score\",\n",
    "            description=\"Combined engagement metric\",\n",
    "            expected_types=['DOUBLE']\n",
    "        )\n",
    "    ),\n",
    "    \n",
    "    # 4. Transaction Metrics\n",
    "    CustomTransform(\n",
    "        transform_func=lambda df: df.with_column(\n",
    "            'AVG_TRANSACTION_VALUE',\n",
    "            F.col('LIFE_TIME_VALUE') / \n",
    "            F.when(F.col('TRANSACTIONS') > 0, F.col('TRANSACTIONS'))\n",
    "            .otherwise(1)\n",
    "        ),\n",
    "        config=TransformConfig(\n",
    "            name=\"avg_transaction_value\",\n",
    "            description=\"Average value per transaction\",\n",
    "            expected_types=['DOUBLE']\n",
    "        )\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Practices for Transformations\n",
    "\n",
    "1. **Validation**\n",
    "   - Check input data types\n",
    "   - Validate output ranges\n",
    "   - Monitor null ratios\n",
    "\n",
    "2. **Performance**\n",
    "   - Use vectorized operations\n",
    "   - Minimize data movement\n",
    "   - Leverage Snowflake optimizations\n",
    "\n",
    "3. **Documentation**\n",
    "   - Document business logic\n",
    "   - Explain transformation choices\n",
    "   - Track dependencies\n",
    "\n",
    "### Example Usage\n",
    "\n",
    "Let's apply our transformations and examine the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake_feature_store.transforms import apply_transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|\"CUSTOMER_ID\"  |\"DATE\"      |\"LIFE_TIME_VALUE\"  |\"SESSION_LENGTH\"    |\"TIME_ON_APP\"      |\"TIME_ON_WEBSITE\"  |\"TRANSACTIONS\"  |\"SUM_TRANSACTIONS_7\"  |\"AVG_TRANSACTIONS_7\"  |\"SUM_TRANSACTIONS_30\"  |\"AVG_TRANSACTIONS_30\"  |\"SUM_LIFE_TIME_VALUE_7\"  |\"AVG_LIFE_TIME_VALUE_7\"  |\"SUM_LIFE_TIME_VALUE_30\"  |\"AVG_LIFE_TIME_VALUE_30\"  |\"ENGAGEMENT_SCORE\"  |\"AVG_TRANSACTION_VALUE\"  |\n",
      "---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|C17            |2025-01-19  |326.2440288924739  |7.2016989831135705  |8.976656464668245  |9.220516522830426  |3               |3.0                   |3.0                   |3.0                    |3.0                    |326.2440288924739        |326.2440288924739        |326.2440288924739         |326.2440288924739         |8.466290656870747   |108.74800963082464       |\n",
      "|C17            |2025-01-21  |54.16033841877866  |0.0                 |6.048969074557204  |6.607916111383904  |1               |4.0                   |2.0                   |4.0                    |2.0                    |380.4043673112526        |190.2021836556263        |380.4043673112526         |190.2021836556263         |4.218961728647035   |54.16033841877866        |\n",
      "---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Apply transforms\n",
    "transformed_df = apply_transforms(df, transforms)\n",
    "print(transformed_df.show(2))\n",
    "\n",
    "show = False\n",
    "if show:\n",
    "    # Show new features\n",
    "    print(\"\\nNew Features Created:\")\n",
    "    new_cols = set(transformed_df.columns) - set(df.columns)\n",
    "    for col in sorted(new_cols):\n",
    "        print(f\"\\n{col}:\")\n",
    "        transformed_df.select([\n",
    "            F.min(col).alias('min'),\n",
    "            F.max(col).alias('max'),\n",
    "            F.avg(col).alias('mean')\n",
    "        ]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Feature View Creation\n",
    "\n",
    "Feature views combine configurations, transformations, and source data into production-ready features.\n",
    "\n",
    "### What is a Feature View?\n",
    "\n",
    "A feature view is:\n",
    "1. **Source Data**: Raw data input\n",
    "2. **Transformations**: Feature engineering logic\n",
    "3. **Configurations**: Validation and refresh rules\n",
    "4. **Metadata**: Documentation and lineage\n",
    "\n",
    "### Why Feature Views Matter\n",
    "\n",
    "Feature views provide:\n",
    "1. **Reproducibility**: Consistent feature computation\n",
    "2. **Monitoring**: Track feature health\n",
    "3. **Discovery**: Make features findable\n",
    "4. **Governance**: Control access and updates\n",
    "\n",
    "### LTV Feature View Design\n",
    "\n",
    "For LTV prediction, our feature view needs to:\n",
    "1. Combine engagement and transaction data\n",
    "2. Apply time-based transformations\n",
    "3. Maintain point-in-time correctness\n",
    "4. Enable regular refreshes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-17 20:32:18,914 - snowflake_feature_store - INFO - Validated feature LIFE_TIME_VALUE (stats: {'timestamp': '2025-02-18T04:32:17.726988', 'row_count': 2400, 'null_count': 0, 'null_ratio': 0.0, 'unique_count': 2400, 'min_value': 1.648862185296137, 'max_value': 749.9894377990938, 'mean_value': 386.7765027344634, 'std_value': 216.32557697363958})\n",
      "2025-02-17 20:32:20,275 - snowflake_feature_store - INFO - Validated feature SESSION_LENGTH (stats: {'timestamp': '2025-02-18T04:32:19.402874', 'row_count': 2400, 'null_count': 0, 'null_ratio': 0.0, 'unique_count': 1950, 'min_value': 0.0, 'max_value': 12.410005497480682, 'mean_value': 5.229708861335616, 'std_value': 3.431862682684851})\n",
      "2025-02-17 20:32:21,854 - snowflake_feature_store - INFO - Validated feature TRANSACTIONS (stats: {'timestamp': '2025-02-18T04:32:20.733519', 'row_count': 2400, 'null_count': 0, 'null_ratio': 0.0, 'unique_count': 7, 'min_value': 1.0, 'max_value': 7.0, 'mean_value': 3.498333, 'std_value': 2.0083186998083744})\n",
      "2025-02-17 20:32:23,186 - snowflake_feature_store - INFO - Validated feature SUM_TRANSACTIONS_7 (stats: {'timestamp': '2025-02-18T04:32:22.239931', 'row_count': 2400, 'null_count': 0, 'null_ratio': 0.0, 'unique_count': 43, 'min_value': 1.0, 'max_value': 43.0, 'mean_value': 21.270416666666666, 'std_value': 7.985906186342453})\n",
      "2025-02-17 20:32:24,808 - snowflake_feature_store - INFO - Validated feature AVG_TRANSACTIONS_7 (stats: {'timestamp': '2025-02-18T04:32:23.607263', 'row_count': 2400, 'null_count': 0, 'null_ratio': 0.0, 'unique_count': 87, 'min_value': 1.0, 'max_value': 7.0, 'mean_value': 3.4522975000000002, 'std_value': 0.9334753459979952})\n",
      "2025-02-17 20:32:26,587 - snowflake_feature_store - INFO - Validated feature SUM_LIFE_TIME_VALUE_7 (stats: {'timestamp': '2025-02-18T04:32:25.249776', 'row_count': 2400, 'null_count': 0, 'null_ratio': 0.0, 'unique_count': 2400, 'min_value': 2.3604486643994282, 'max_value': 4516.837225952391, 'mean_value': 2352.305074179735, 'std_value': 872.6628279170988})\n",
      "2025-02-17 20:32:27,983 - snowflake_feature_store - INFO - Validated feature AVG_LIFE_TIME_VALUE_7 (stats: {'timestamp': '2025-02-18T04:32:26.990794', 'row_count': 2400, 'null_count': 0, 'null_ratio': 0.0, 'unique_count': 2400, 'min_value': 2.3604486643994282, 'max_value': 746.7937644244283, 'mean_value': 381.6178184605153, 'std_value': 100.95620746470256})\n",
      "2025-02-17 20:32:29,512 - snowflake_feature_store - INFO - Validated feature SUM_TRANSACTIONS_30 (stats: {'timestamp': '2025-02-18T04:32:28.401970', 'row_count': 2400, 'null_count': 0, 'null_ratio': 0.0, 'unique_count': 109, 'min_value': 1.0, 'max_value': 112.0, 'mean_value': 43.403333333333336, 'std_value': 25.886947381660594})\n",
      "2025-02-17 20:32:31,156 - snowflake_feature_store - INFO - Validated feature AVG_TRANSACTIONS_30 (stats: {'timestamp': '2025-02-18T04:32:29.912207', 'row_count': 2400, 'null_count': 0, 'null_ratio': 0.0, 'unique_count': 423, 'min_value': 1.0, 'max_value': 7.0, 'mean_value': 3.4125366666666666, 'std_value': 0.81236544639503})\n",
      "2025-02-17 20:32:32,760 - snowflake_feature_store - INFO - Validated feature SUM_LIFE_TIME_VALUE_30 (stats: {'timestamp': '2025-02-18T04:32:31.644694', 'row_count': 2400, 'null_count': 0, 'null_ratio': 0.0, 'unique_count': 2400, 'min_value': 2.3604486643994282, 'max_value': 12245.960289417902, 'mean_value': 4798.932578660564, 'std_value': 2853.5725260633594})\n",
      "2025-02-17 20:32:34,479 - snowflake_feature_store - INFO - Validated feature AVG_LIFE_TIME_VALUE_30 (stats: {'timestamp': '2025-02-18T04:32:33.233919', 'row_count': 2400, 'null_count': 0, 'null_ratio': 0.0, 'unique_count': 2400, 'min_value': 2.3604486643994282, 'max_value': 746.7937644244283, 'mean_value': 377.05517163412384, 'std_value': 88.0311813911325})\n",
      "2025-02-17 20:32:36,111 - snowflake_feature_store - INFO - Validated feature ENGAGEMENT_SCORE (stats: {'timestamp': '2025-02-18T04:32:34.962689', 'row_count': 2400, 'null_count': 0, 'null_ratio': 0.0, 'unique_count': 2400, 'min_value': 1.9506477953781742, 'max_value': 13.661164361145945, 'mean_value': 7.336550503381129, 'std_value': 2.3540242782743106})\n",
      "2025-02-17 20:32:37,424 - snowflake_feature_store - INFO - Validated feature AVG_TRANSACTION_VALUE (stats: {'timestamp': '2025-02-18T04:32:36.479206', 'row_count': 2400, 'null_count': 0, 'null_ratio': 0.0, 'unique_count': 2400, 'min_value': 1.648862185296137, 'max_value': 199.701969611493, 'mean_value': 111.05532318882489, 'std_value': 30.834003614918423})\n"
     ]
    }
   ],
   "source": [
    "# 1. Create feature view config\n",
    "entity_name = \"CUSTOMER\"\n",
    "feature_view_name = \"customer_ltv_features\"\n",
    "\n",
    "# 2. Create feature view config\n",
    "config = FeatureViewConfig(\n",
    "    name=feature_view_name,\n",
    "    domain=\"RETAIL\",\n",
    "    entity=entity_name,\n",
    "    feature_type=\"BEHAVIOR\",\n",
    "    refresh=RefreshConfig(\n",
    "        frequency=\"1 day\",\n",
    "        mode=\"INCREMENTAL\"\n",
    "    ),\n",
    "    features=feature_configs,  #  Created Above Pass the dictionary of FeatureConfigs\n",
    "    description=\"\"\"\n",
    "    Customer LTV prediction features combining:\n",
    "    - Transaction history\n",
    "    - Engagement metrics\n",
    "    - Time-based patterns\n",
    "    \n",
    "    Updated daily with incremental processing.\n",
    "    Use for LTV prediction and customer segmentation.\n",
    "    \"\"\".strip(),\n",
    "    timestamp_col=\"DATE\"\n",
    ")\n",
    "\n",
    "# 3. Create transformations\n",
    "# Created above\n",
    "\n",
    "# 4. Create and register feature view\n",
    "feature_view = manager.add_feature_view(\n",
    "    config=config,\n",
    "    df=df, # Original DataFrame above\n",
    "    entity_name=entity_name,\n",
    "    transforms=transforms,\n",
    "    collect_stats=True  # Enable monitoring\n",
    ")\n",
    "\n",
    "# 5. Log feature view details\n",
    "logger.info(f\"\\nCreated feature view: {feature_view_name}\")\n",
    "logger.info(f\"Features created: {len(feature_configs)}\")\n",
    "logger.info(f\"Transformations applied: {len(transforms)}\")\n",
    "\n",
    "# 6. Show feature statistics\n",
    "logger.info(\"\\nFeature Statistics:\")\n",
    "for feature_name, stats in manager.feature_stats[config.name].items():\n",
    "    logger.info(f\"\\n{feature_name}:\")\n",
    "    logger.info(str(stats))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Practices for Feature Views\n",
    "\n",
    "1. **Documentation**\n",
    "   - Clear descriptions\n",
    "   - Usage examples\n",
    "   - Update frequency\n",
    "   - Dependencies\n",
    "\n",
    "2. **Monitoring**\n",
    "   - Feature statistics\n",
    "   - Data quality metrics\n",
    "   - Refresh status\n",
    "   - Drift detection\n",
    "\n",
    "3. **Performance**\n",
    "   - Incremental updates\n",
    "   - Efficient transformations\n",
    "   - Appropriate refresh schedule\n",
    "\n",
    "4. **Governance**\n",
    "   - Access controls\n",
    "   - Version control\n",
    "   - Audit logging\n",
    "   - Data lineage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Feature Monitoring\n",
    "\n",
    "Monitoring is crucial for maintaining feature quality and detecting issues early.\n",
    "\n",
    "### Why Monitor Features?\n",
    "\n",
    "Feature monitoring helps:\n",
    "1. **Detect Data Quality Issues**: Missing values, outliers, type mismatches\n",
    "2. **Track Feature Drift**: Changes in feature distributions\n",
    "3. **Ensure Freshness**: Verify timely updates\n",
    "4. **Validate Business Rules**: Check domain-specific constraints\n",
    "\n",
    "### Types of Monitoring\n",
    "\n",
    "1. **Data Quality**\n",
    "   - Null ratios\n",
    "   - Type consistency\n",
    "   - Value ranges\n",
    "   - Cardinality\n",
    "\n",
    "2. **Statistical Monitoring**\n",
    "   - Distribution shifts\n",
    "   - Correlation changes\n",
    "   - Seasonality patterns\n",
    "   - Outlier detection\n",
    "\n",
    "3. **Operational Monitoring**\n",
    "   - Refresh status\n",
    "   - Computation time\n",
    "   - Resource usage\n",
    "   - Error rates\n",
    "\n",
    "### LTV-Specific Monitoring\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from typing import Union, List, Callable, Optional, Protocol, Dict, Any\n",
    "\n",
    "import decimal\n",
    "from typing import Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class LTVMonitor:\n",
    "    \"\"\"Monitor for LTV feature quality and drift\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        manager: FeatureStoreManager,\n",
    "        feature_view_name: str,\n",
    "        metrics_path: Optional[str] = None\n",
    "    ):\n",
    "        self.manager = manager\n",
    "        self.feature_view_name = feature_view_name\n",
    "        self.metrics_path = metrics_path\n",
    "        self.baseline_stats = {}\n",
    "        \n",
    "    def _convert_decimal(self, obj: Any) -> Any:\n",
    "        \"\"\"Convert Decimal objects to float for JSON serialization\"\"\"\n",
    "        if isinstance(obj, decimal.Decimal):\n",
    "            return float(obj)\n",
    "        return obj\n",
    "    \n",
    "    def _process_metrics(self, metrics: Dict) -> Dict:\n",
    "        \"\"\"Process metrics dictionary to ensure JSON serializable values\"\"\"\n",
    "        return {\n",
    "            k: {\n",
    "                'timestamp': v['timestamp'],\n",
    "                'metrics': {\n",
    "                    mk: self._convert_decimal(mv)\n",
    "                    for mk, mv in v['metrics'].items()\n",
    "                }\n",
    "            }\n",
    "            for k, v in metrics.items()\n",
    "        }\n",
    "    \n",
    "    def compute_feature_metrics(\n",
    "        self,\n",
    "        df: DataFrame,\n",
    "        timestamp: Optional[datetime] = None\n",
    "    ) -> Dict[str, Dict]:\n",
    "        \"\"\"Compute comprehensive feature metrics\"\"\"\n",
    "        metrics = {}\n",
    "        timestamp = timestamp or datetime.now()\n",
    "        \n",
    "        for col in df.columns:\n",
    "            # Skip identifier columns\n",
    "            if col in ['CUSTOMER_ID', 'DATE']:\n",
    "                continue\n",
    "                \n",
    "            # Basic stats\n",
    "            stats = df.select([\n",
    "                F.count(col).alias('count'),\n",
    "                F.count_distinct(col).alias('unique'),\n",
    "                F.sum(F.when(F.col(col).is_null(), 1).otherwise(0)).alias('nulls')\n",
    "            ]).collect()[0].asDict()\n",
    "            \n",
    "            # Convert Decimal to float\n",
    "            stats = {k: self._convert_decimal(v) for k, v in stats.items()}\n",
    "            \n",
    "            # Numeric stats for appropriate columns\n",
    "            if col in ['LIFE_TIME_VALUE', 'SESSION_LENGTH', 'TRANSACTIONS']:\n",
    "                numeric_stats = df.select([\n",
    "                    F.min(col).alias('min'),\n",
    "                    F.max(col).alias('max'),\n",
    "                    F.avg(col).alias('mean'),\n",
    "                    F.stddev(col).alias('std')\n",
    "                ]).collect()[0].asDict()\n",
    "                \n",
    "                # Convert Decimal to float\n",
    "                numeric_stats = {k: self._convert_decimal(v) for k, v in numeric_stats.items()}\n",
    "                stats.update(numeric_stats)\n",
    "            \n",
    "            metrics[col] = {\n",
    "                'timestamp': timestamp.isoformat(),\n",
    "                'metrics': stats\n",
    "            }\n",
    "            \n",
    "        return metrics\n",
    "    \n",
    "    def set_baseline(self, df: DataFrame) -> None:\n",
    "        \"\"\"Set baseline statistics for drift detection\"\"\"\n",
    "        self.baseline_stats = self.compute_feature_metrics(df)\n",
    "        logger.info(\"Set baseline statistics\")\n",
    "        \n",
    "        # Save baseline if metrics path provided\n",
    "        if self.metrics_path:\n",
    "            baseline_file = Path(self.metrics_path) / \"baseline_stats.json\"\n",
    "            processed_stats = self._process_metrics(self.baseline_stats)\n",
    "            with open(baseline_file, 'w') as f:\n",
    "                json.dump(processed_stats, f, indent=2)\n",
    "    \n",
    "    def check_feature_health(\n",
    "        self,\n",
    "        df: DataFrame,\n",
    "        drift_threshold: float = 0.1\n",
    "    ) -> None:\n",
    "        \"\"\"Check overall feature health\"\"\"\n",
    "        try:\n",
    "            # Compute current metrics\n",
    "            current_metrics = self.compute_feature_metrics(df)\n",
    "            \n",
    "            # Detect drift if baseline exists\n",
    "            if self.baseline_stats:\n",
    "                drift_alerts = self.detect_drift(\n",
    "                    current_metrics,\n",
    "                    drift_threshold\n",
    "                )\n",
    "                \n",
    "                if drift_alerts:\n",
    "                    logger.warning(\"\\nFeature Drift Detected:\")\n",
    "                    for feature, alerts in drift_alerts.items():\n",
    "                        logger.warning(f\"\\n{feature}:\")\n",
    "                        for alert in alerts:\n",
    "                            logger.warning(f\"- {alert}\")\n",
    "            \n",
    "            # Log current metrics\n",
    "            logger.info(\"\\nCurrent Feature Metrics:\")\n",
    "            for feature, metrics in current_metrics.items():\n",
    "                logger.info(f\"\\n{feature}:\")\n",
    "                for metric, value in metrics['metrics'].items():\n",
    "                    logger.info(f\"  {metric}: {value}\")\n",
    "                    \n",
    "            # Save metrics if path provided\n",
    "            if self.metrics_path:\n",
    "                metrics_file = Path(self.metrics_path) / f\"metrics_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "                processed_metrics = self._process_metrics(current_metrics)\n",
    "                with open(metrics_file, 'w') as f:\n",
    "                    json.dump(processed_metrics, f, indent=2)\n",
    "                    \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error checking feature health: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def detect_drift(\n",
    "        self,\n",
    "        current_metrics: Dict,\n",
    "        drift_threshold: float = 0.1\n",
    "    ) -> Dict[str, List[str]]:\n",
    "        \"\"\"Detect significant changes in feature distributions\"\"\"\n",
    "        drift_alerts = {}\n",
    "        \n",
    "        for feature, metrics in current_metrics.items():\n",
    "            if feature not in self.baseline_stats:\n",
    "                continue\n",
    "                \n",
    "            alerts = []\n",
    "            baseline = self.baseline_stats[feature]['metrics']\n",
    "            current = metrics['metrics']\n",
    "            \n",
    "            # Check for distribution changes\n",
    "            for metric in ['mean', 'std']:\n",
    "                if metric not in current or metric not in baseline:\n",
    "                    continue\n",
    "                    \n",
    "                change = abs(current[metric] - baseline[metric]) / baseline[metric]\n",
    "                if change > drift_threshold:\n",
    "                    alerts.append(\n",
    "                        f\"{metric.upper()} changed by {change:.1%}\"\n",
    "                    )\n",
    "            \n",
    "            # Check for data quality changes\n",
    "            null_ratio = current['NULLS'] / current['COUNT']\n",
    "            baseline_null_ratio = baseline['NULLS'] / baseline['COUNT']\n",
    "            if abs(null_ratio - baseline_null_ratio) > drift_threshold:\n",
    "                alerts.append(\n",
    "                    f\"NULL ratio changed from {baseline_null_ratio:.1%} to {null_ratio:.1%}\"\n",
    "                )\n",
    "            \n",
    "            if alerts:\n",
    "                drift_alerts[feature] = alerts\n",
    "                \n",
    "        return drift_alerts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Usage\n",
    "\n",
    "Let's set up monitoring for our LTV features:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up metrics directory\n",
    "metrics_dir = Path(\"feature_metrics\")\n",
    "metrics_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up monitoring\n",
    "monitor = LTVMonitor(\n",
    "    manager=manager,\n",
    "    feature_view_name=feature_view.name,\n",
    "    metrics_path=str(metrics_dir)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake_feature_store.examples import get_example_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-17 19:53:41,460 - snowflake_feature_store - INFO - Set baseline statistics\n",
      "2025-02-17 19:53:49,621 - snowflake_feature_store - INFO - Generated 6000 rows of demo data in \"DATASCIENCE\".FEATURE_STORE_DEMO.CUSTOMER_ACTIVITYT_E_S_T\n",
      "2025-02-17 19:53:49,625 - snowflake_feature_store - INFO - \n",
      "Sample Data:\n",
      "---------------------------------------------------------------------------------------------------------------------------------\n",
      "|\"CUSTOMER_ID\"  |\"DATE\"      |\"LIFE_TIME_VALUE\"   |\"SESSION_LENGTH\"   |\"TIME_ON_APP\"      |\"TIME_ON_WEBSITE\"   |\"TRANSACTIONS\"  |\n",
      "---------------------------------------------------------------------------------------------------------------------------------\n",
      "|C10            |2025-01-18  |92.86384204124737   |2.810757885431538  |5.699398765006296  |4.89662965787127    |1               |\n",
      "|C69            |2025-02-16  |209.27431935211192  |4.725524450691177  |5.17511813932502   |5.568512927658125   |2               |\n",
      "|C90            |2025-02-16  |480.49942116614795  |9.371673523868244  |6.670381127747131  |8.736866177225082   |4               |\n",
      "|C4             |2025-02-16  |663.933648585558    |NULL               |8.544860151515563  |13.223287737198099  |6               |\n",
      "|C45            |2025-02-16  |171.08757872959268  |5.891257507241159  |5.715128528008879  |7.958730645279172   |1               |\n",
      "---------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "2025-02-17 19:53:49,922 - snowflake_feature_store - INFO - \n",
      "Schema:\n",
      "2025-02-17 19:53:50,055 - snowflake_feature_store - INFO - CUSTOMER_ID: StringType()\n",
      "2025-02-17 19:53:50,057 - snowflake_feature_store - INFO - DATE: DateType()\n",
      "2025-02-17 19:53:50,057 - snowflake_feature_store - INFO - LIFE_TIME_VALUE: DoubleType()\n",
      "2025-02-17 19:53:50,058 - snowflake_feature_store - INFO - SESSION_LENGTH: DoubleType()\n",
      "2025-02-17 19:53:50,058 - snowflake_feature_store - INFO - TIME_ON_APP: DoubleType()\n",
      "2025-02-17 19:53:50,059 - snowflake_feature_store - INFO - TIME_ON_WEBSITE: DoubleType()\n",
      "2025-02-17 19:53:50,060 - snowflake_feature_store - INFO - TRANSACTIONS: LongType()\n",
      "2025-02-17 19:53:51,904 - snowflake_feature_store - WARNING - \n",
      "Feature Drift Detected:\n",
      "2025-02-17 19:53:51,904 - snowflake_feature_store - WARNING - \n",
      "SESSION_LENGTH:\n",
      "2025-02-17 19:53:51,904 - snowflake_feature_store - WARNING - - NULL ratio changed from 0.0% to 21.8%\n",
      "2025-02-17 19:53:51,905 - snowflake_feature_store - INFO - \n",
      "Current Feature Metrics:\n",
      "2025-02-17 19:53:51,905 - snowflake_feature_store - INFO - \n",
      "LIFE_TIME_VALUE:\n",
      "2025-02-17 19:53:51,906 - snowflake_feature_store - INFO -   COUNT: 2400\n",
      "2025-02-17 19:53:51,906 - snowflake_feature_store - INFO -   UNIQUE: 2400\n",
      "2025-02-17 19:53:51,906 - snowflake_feature_store - INFO -   NULLS: 0\n",
      "2025-02-17 19:53:51,907 - snowflake_feature_store - INFO -   MIN: 1.1764882580900178\n",
      "2025-02-17 19:53:51,907 - snowflake_feature_store - INFO -   MAX: 749.7281269022976\n",
      "2025-02-17 19:53:51,908 - snowflake_feature_store - INFO -   MEAN: 373.7064093219135\n",
      "2025-02-17 19:53:51,908 - snowflake_feature_store - INFO -   STD: 214.89071380107362\n",
      "2025-02-17 19:53:51,908 - snowflake_feature_store - INFO - \n",
      "SESSION_LENGTH:\n",
      "2025-02-17 19:53:51,909 - snowflake_feature_store - INFO -   COUNT: 1971\n",
      "2025-02-17 19:53:51,909 - snowflake_feature_store - INFO -   UNIQUE: 1971\n",
      "2025-02-17 19:53:51,909 - snowflake_feature_store - INFO -   NULLS: 429\n",
      "2025-02-17 19:53:51,910 - snowflake_feature_store - INFO -   MIN: 0.056783376075392626\n",
      "2025-02-17 19:53:51,910 - snowflake_feature_store - INFO -   MAX: 12.27514690185668\n",
      "2025-02-17 19:53:51,910 - snowflake_feature_store - INFO -   MEAN: 6.196373803480408\n",
      "2025-02-17 19:53:51,910 - snowflake_feature_store - INFO -   STD: 2.646483829793292\n",
      "2025-02-17 19:53:51,911 - snowflake_feature_store - INFO - \n",
      "TIME_ON_APP:\n",
      "2025-02-17 19:53:51,911 - snowflake_feature_store - INFO -   COUNT: 2400\n",
      "2025-02-17 19:53:51,911 - snowflake_feature_store - INFO -   UNIQUE: 2400\n",
      "2025-02-17 19:53:51,911 - snowflake_feature_store - INFO -   NULLS: 0\n",
      "2025-02-17 19:53:51,912 - snowflake_feature_store - INFO - \n",
      "TIME_ON_WEBSITE:\n",
      "2025-02-17 19:53:51,912 - snowflake_feature_store - INFO -   COUNT: 2400\n",
      "2025-02-17 19:53:51,912 - snowflake_feature_store - INFO -   UNIQUE: 2400\n",
      "2025-02-17 19:53:51,913 - snowflake_feature_store - INFO -   NULLS: 0\n",
      "2025-02-17 19:53:51,913 - snowflake_feature_store - INFO - \n",
      "TRANSACTIONS:\n",
      "2025-02-17 19:53:51,913 - snowflake_feature_store - INFO -   COUNT: 2400\n",
      "2025-02-17 19:53:51,914 - snowflake_feature_store - INFO -   UNIQUE: 7\n",
      "2025-02-17 19:53:51,914 - snowflake_feature_store - INFO -   NULLS: 0\n",
      "2025-02-17 19:53:51,914 - snowflake_feature_store - INFO -   MIN: 1\n",
      "2025-02-17 19:53:51,914 - snowflake_feature_store - INFO -   MAX: 7\n",
      "2025-02-17 19:53:51,915 - snowflake_feature_store - INFO -   MEAN: 3.38375\n",
      "2025-02-17 19:53:51,915 - snowflake_feature_store - INFO -   STD: 1.9853115624505893\n"
     ]
    }
   ],
   "source": [
    "# Set baseline\n",
    "monitor.set_baseline(feature_view.feature_df)\n",
    "# Generate some drift\n",
    "drift_df = get_example_data(\n",
    "    conn.session,\n",
    "    schema,\n",
    "    num_customers=250,\n",
    "    ltv_multiplier=4.5,  # Increase values to simulate drift\n",
    "    table_type = 'TEST'\n",
    ")\n",
    "# Check for drift\n",
    "monitor.check_feature_health(drift_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Training Data Generation\n",
    "\n",
    "Generating training data from a feature store requires special consideration to avoid data leakage and ensure point-in-time correctness.\n",
    "\n",
    "### Why Training Data Generation Matters\n",
    "\n",
    "Proper training data generation:\n",
    "1. **Prevents Data Leakage**: Ensures future data doesn't leak into training\n",
    "2. **Maintains Consistency**: Uses same feature computations as production\n",
    "3. **Enables Reproducibility**: Training sets can be recreated exactly\n",
    "4. **Supports Experimentation**: Easy to create different feature combinations\n",
    "\n",
    "### LTV Training Data Requirements\n",
    "\n",
    "For LTV prediction, we need to:\n",
    "1. Use historical data to predict future LTV\n",
    "2. Include time-based features correctly\n",
    "3. Handle missing values consistently\n",
    "4. Maintain customer context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-17 20:26:37,379 - snowflake_feature_store - INFO - FeatureStoreManager initialized\n"
     ]
    }
   ],
   "source": [
    "import tempfile\n",
    "from pathlib import Path\n",
    "\n",
    "metrics_dir = Path(tempfile.mkdtemp()) / \"feature_store_metrics\"\n",
    "\n",
    "training_start_date='2025-01-01'\n",
    "training_end_date='2025-03-01'\n",
    "prediction_window=90  # Predict 90-day LTV\n",
    "save_table='DEMO_DB.FEATURE_STORE_DEMO.LTV_TRAINING_DATA'\n",
    "\n",
    "manager = FeatureStoreManager(\n",
    "    connection=conn,\n",
    "    metrics_path=metrics_dir,\n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "# Get existing feature view\n",
    "feature_view = manager.feature_store.get_feature_view(\n",
    "    name=\"customer_ltv_features\",\n",
    "    version=\"V1_0\"  # Use the version from earlier\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------\n",
      "|\"CUSTOMER_ID\"  |\"DATE\"      |\"TARGET_LTV\"        |\"LABEL_DATE\"  |\n",
      "------------------------------------------------------------------\n",
      "|C10            |2025-01-18  |741.0711860183914   |2025-02-16    |\n",
      "|C69            |2025-02-16  |209.27431935211192  |2025-02-16    |\n",
      "------------------------------------------------------------------\n",
      "\n",
      "2025-02-17 20:26:41,925 - snowflake_feature_store - INFO - Created spine with 2400 rows\n"
     ]
    }
   ],
   "source": [
    "# Get the fully qualified table name\n",
    "table_name = (\n",
    "    f\"{manager.connection.database}.\"\n",
    "    f\"{manager.connection.schema}.\"\n",
    "    f\"CUSTOMER_ACTIVITY\"\n",
    ")\n",
    "\n",
    "# 1. Create spine query for point-in-time correct features\n",
    "spine_df = manager.connection.session.sql(f\"\"\"\n",
    "    WITH customer_dates AS (\n",
    "        -- Get all customer-date combinations\n",
    "    SELECT DISTINCT\n",
    "            CUSTOMER_ID,\n",
    "            DATE\n",
    "        FROM {table_name}\n",
    "        WHERE DATE BETWEEN '{training_start_date}' AND '{training_end_date}'\n",
    "    ),\n",
    "    future_ltv AS (\n",
    "        -- Calculate future LTV for each customer-date\n",
    "        SELECT \n",
    "            cd.CUSTOMER_ID,\n",
    "            cd.DATE as FEATURE_DATE,\n",
    "            MAX(f.DATE) as LABEL_DATE,\n",
    "            MAX(f.LIFE_TIME_VALUE) as FUTURE_LTV\n",
    "        FROM customer_dates cd\n",
    "        LEFT JOIN {table_name} f\n",
    "            ON cd.CUSTOMER_ID = f.CUSTOMER_ID\n",
    "            AND f.DATE BETWEEN cd.DATE \n",
    "                AND DATEADD(days, {prediction_window}, cd.DATE)\n",
    "        GROUP BY 1, 2\n",
    ")\n",
    "    -- Final spine query\n",
    "    SELECT \n",
    "        CUSTOMER_ID,\n",
    "        FEATURE_DATE as \"DATE\",\n",
    "        FUTURE_LTV as \"TARGET_LTV\",\n",
    "        LABEL_DATE as \"LABEL_DATE\"\n",
    "    FROM future_ltv\n",
    "\"\"\")\n",
    "spine_df.show(2)\n",
    "\n",
    "logger.info(f\"Created spine with {spine_df.count()} rows\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-17 20:26:47,193 - snowflake_feature_store - INFO - Spine DataFrame columns: ['CUSTOMER_ID', 'DATE', 'TARGET_LTV', 'LABEL_DATE']\n",
      "2025-02-17 20:26:47,194 - snowflake_feature_store - INFO - Spine DataFrame schema: StructType([StructField('CUSTOMER_ID', StringType(), nullable=True), StructField('DATE', DateType(), nullable=True), StructField('TARGET_LTV', DoubleType(), nullable=True), StructField('LABEL_DATE', DateType(), nullable=True)])\n",
      "2025-02-17 20:26:47,348 - snowflake_feature_store - INFO - Modified spine DataFrame columns: ['CUSTOMER_ID', 'DATE', 'TARGET_LTV', 'LABEL_DATE']\n",
      "2025-02-17 20:26:47,348 - snowflake_feature_store - ERROR - Error: Error generating dataset: 'FeatureView' object has no attribute 'split'\n"
     ]
    },
    {
     "ename": "FeatureStoreException",
     "evalue": "Error generating dataset: 'FeatureView' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/github/sf-feature-store/nbs/snowflake_feature_store/manager.py:456\u001b[0m, in \u001b[0;36mFeatureStoreManager.get_features\u001b[0;34m(self, spine_df, feature_views, label_cols, dataset_name, spine_timestamp_col, **kwargs)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 456\u001b[0m     name, version \u001b[38;5;241m=\u001b[39m \u001b[43mfv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    457\u001b[0m     view \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_store\u001b[38;5;241m.\u001b[39mget_feature_view(name, version)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'FeatureView' object has no attribute 'split'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFeatureStoreException\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[78], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 2. Get features using point-in-time correct joins\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m training_data \u001b[38;5;241m=\u001b[39m \u001b[43mmanager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_features\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspine_df\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspine_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_views\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mfeature_view\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspine_timestamp_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDATE\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel_cols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTARGET_LTV\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLABEL_DATE\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m training_data\u001b[38;5;241m.\u001b[39mshow(\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m~/github/sf-feature-store/nbs/snowflake_feature_store/manager.py:492\u001b[0m, in \u001b[0;36mFeatureStoreManager.get_features\u001b[0;34m(self, spine_df, feature_views, label_cols, dataset_name, spine_timestamp_col, **kwargs)\u001b[0m\n\u001b[1;32m    490\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cb \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks:\n\u001b[1;32m    491\u001b[0m     cb\u001b[38;5;241m.\u001b[39mon_error(error_msg)\n\u001b[0;32m--> 492\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m FeatureStoreException(error_msg)\n",
      "\u001b[0;31mFeatureStoreException\u001b[0m: Error generating dataset: 'FeatureView' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "# 2. Get features using point-in-time correct joins\n",
    "training_data = manager.get_features(\n",
    "    spine_df=spine_df,\n",
    "    feature_views=[feature_view],\n",
    "    spine_timestamp_col=\"DATE\",\n",
    "    label_cols=[\"TARGET_LTV\", \"LABEL_DATE\"]\n",
    ")\n",
    "training_data.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 3. Add metadata columns\n",
    "training_data = training_data.with_columns([\n",
    "    F.lit(training_start_date).alias(\"TRAINING_START_DATE\"),\n",
    "    F.lit(training_end_date).alias(\"TRAINING_END_DATE\"),\n",
    "    F.lit(prediction_window).alias(\"PREDICTION_WINDOW_DAYS\"),\n",
    "    F.current_timestamp().alias(\"GENERATED_AT\")\n",
    "])\n",
    "\n",
    "# 4. Save if table name provided\n",
    "if save_table:\n",
    "    training_data.write.mode(\"overwrite\").save_as_table(save_table)\n",
    "    logger.info(f\"Saved training data to {save_table}\")\n",
    "\n",
    "# 5. Log data generation stats\n",
    "logger.info(\"\\nTraining Data Statistics:\")\n",
    "logger.info(f\"Total rows: {training_data.count()}\")\n",
    "logger.info(f\"Date range: {training_start_date} to {training_end_date}\")\n",
    "logger.info(f\"Prediction window: {prediction_window} days\")\n",
    "\n",
    "# 6. Show sample and schema\n",
    "logger.info(\"\\nSample Data:\")\n",
    "training_data.show(5)\n",
    "\n",
    "logger.info(\"\\nSchema:\")\n",
    "for field in training_data.schema.fields:\n",
    "    logger.info(f\"{field.name}: {field.datatype}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Best Practices for Training Data\n",
    "\n",
    "1. **Time Windows**\n",
    "   - Use appropriate training/validation splits\n",
    "   - Consider seasonal patterns\n",
    "   - Match prediction window to business needs\n",
    "\n",
    "2. **Feature Selection**\n",
    "   - Include all relevant features\n",
    "   - Document feature importance\n",
    "   - Track feature dependencies\n",
    "\n",
    "3. **Data Quality**\n",
    "   - Handle missing values consistently\n",
    "   - Check for data leakage\n",
    "   - Validate label quality\n",
    "\n",
    "4. **Documentation**\n",
    "   - Record generation parameters\n",
    "   - Track data lineage\n",
    "   - Document assumptions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Training data generation\n",
    "Production usage examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "feature-store",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
